--- 
title: "ReCentering Psych Stats"
author: "Lynette H. Bikos, PhD, ABPP"
date: "`r format (Sys.Date(), '%d %b %Y')`" 
site: bookdown::bookdown_site
documentclass: book
fontsize: 11pt
geometry: margin=1in
highlight: tango
urlcolor: blue #without this the links in the PDF do not show
citation-style: apa-single-spaced.csl #if you want APA style
bibliography: STATSnMETH.bib #This is my own bibtex file from my Zotero account. If this file is not in the project folder there will be an error and the book will not build.
link-citations: yes
#DID NOT WORK, BUT DID NOT HURT EITHER I THINK IT NEEDS TO BE CONNECTED TO A PKG
#line-wrapping-in-code: true #wrap overflowing lines in code blocks? 
url: https://lhbikos.github.io/ReCenterPsychStats/ #a link to the GitHub pages where it is rendered
cover-image: images/ReCenterPsychStats-bookcover2.jpg #link to the image for the book which will show up in any previews
description: |
  ReCentering Psych Stats is an open education resource for teaching statistics with the open-source program, R, in a socially and culturally responsive manner. The series provides workflows and worked examples in R and each statistic is accompanied by an example APA style presentation of results. A core focus of the ReCentering series is simulated data from published articles that focus on issues of social justice and are, themselves, conducted in a socially responsive manner. 
github-repo: lhbikos/ReCenterPsychStats

---

# BOOK COVER {-}

Placeholder


## Copyright with Open Access {-}

<!--chapter:end:index.Rmd-->


# Introduction {#ReCintro}

Placeholder


## What to expect in each chapter
## Strategies for Accessing and Using this OER
## If You are New to R

<!--chapter:end:01-Introduction.Rmd-->


# Ready_Set_R {#Ready}

Placeholder


## Navigating this Lesson
### Learning Objectives
## downloading and installing R
### So many paRts and pieces
### oRienting to R Studio (focusing only on the things we will be using first and most often)
## best pRactices
### Everything is documented in the .rmd file
### Setting up the file
### Script in chunks and everything else in the "inline text" sections
### Managing packages
### Upload the data 
#### To and from .csv files
#### To and from .rds files
#### From SPSS files
## quick demonstRation
## the knitted file
## tRoubleshooting in R maRkdown
## just *why* have we tRansitioned to R?
## stRategies for success
## Resources for getting staRted

<!--chapter:end:02-ReadySetR.Rmd-->


# Preliminary Results {#preliminaries}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
## Research Vignette
## Variable Types (Scale of Measurement) 
### Measurement Scale
### Corresponding Variable Structure in R
## Descriptive Statistics
### Measures of Central Tendency
#### Mean
#### Median
#### Mode
#### Relationship between mean, median, and mode
## Variability
### Range
### Percentiles, Quantiles, Interquartile Range
### Deviations around the Mean
### Variance
### Standard Deviation
## Are the Variables Normally Distributed?
## Relations between Variables
## Shortcuts to Preliminary Analyses
### SPLOM
### apaTables
## An APA Style Writeup
## Practice Problems
### Problem #1: Change the Random Seed
### Problem #2: Swap Variables in the Simulation
### Problem #3: Use (or Simulate) Your Own Data
### Grading Rubric

<!--chapter:end:03-Preliminaries.Rmd-->

# *t*-tests {-}

The lessons offered in the *t*-tests section introduce *inferential statistics*. In the prior chapters, our use of measures of central tendency (i.e., mean, median, mode) and variance (i.e., range, variance, standard deviation) were merely about to *describe* a sample.

As we move into *inferential* statistics we evaluate data from a sample and try to determine whether or not we can use it to draw conclusions (i.e, predict or make inferences) about a larger, defined, population.at

The *t*-test lessons begin with an explanation of the *z*-score and progress through one sample, independent samples, and paired samples *t*-tests. Each lesson is centered around a research vignette that was focused on physicians' communication with patients who were critically and terminally ill and in the intensive care unit at a hospial [@elliott_differences_2016].

In addition to a conceptual presentation of of each statistic, each lesson includes:

* a workflow that guides researchers through decision-points in each statistic,
* the presentation of formulas and R code for "hand-calculating" each component of the formula,
* script for efficiently computing the statistic with R packages,
* an "recipe" for an APA style presentation of the results,
* a discussion of *power* in that particular statistic with R script for calculating sample sizes sufficient to reject the null hypothesis, if in fact, it is appropriate to do so, and
* suggestions for practice that vary in degree of challenge.

 

# One Sample *t*-tests {#tOneSample}

[Screencasted Lecture Link](link here) 
 
```{r  tidy=TRUE, tidy.opts=list(width.cutoff=70), include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
options(scipen=999)#eliminates scientific notation
```

Researchers, and perhaps especially those engaged in program evaluation, may wish to know if their data differs from an external standard. In today's research vignette, we will ask if the time physicians spent with their patients differed from what was reported nationally. The one-sample *t*-test is an appropriate tool for this type of analysis. 

## Navigating this Lesson

There is about # hour and ## minutes of lecture.  If you work through the materials with me it would be plan for an additional TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReCenterPsychStats) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Learning objectives from this lecture include the following:

* Recognize the research questions for which utilization of a one sample *t*-test would be appropriate.
* Narrate the steps in conducting a one-sample *t*-test, beginning with testing the statistical assumptions through writing up an APA style results section.
* Calculate a one-sample *t*-test in R (including effect sizes).
* Interpret a 95% confidence interval around a mean difference score.
* Produce an APA style results for a one-sample *t*-test .
* Determine a sample size that (given a set of parameters) would likely result in a statistically significant effect, if there was one.

### Planning for Practice

The suggestions for homework are graded in complexity. The more complete descriptions at the end of the chapter follow these suggestions.

* Rework the one-sample *t*-test in the lesson by changing the random seed in the code that simulates the data.  This should provide minor changes to the data, but the results will likely be very similar.
* Rework the one-sample *t*-test in the lesson by changing something else about the simulation. For example, if you are interested in power, consider changing the sample size.
* Conduct a one sample *t*-test with data to which you have access and permission to use. This could include data you simulate on your own or from a published article.

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Navarro, D. (2020). Chapter 13: Comparing two means. In [Learning Statistics with R - A tutorial for Psychology Students and other Beginners](https://learningstatisticswithr.com/). Retrieved from https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)
  - Navarro's OER includes a good mix of conceptual information about *t* tests as well as R code. My lesson integrates her approach as well as considering information from Field's [-@field_discovering_2012] and Green and Salkind's [-@green_using_2014] texts (as well as searching around on the internet).
* Elliott, A. M., Alexander, S. C., Mescher, C. A., Mohan, D., & Barnato, A. E. (2016). Differences in Physicians’ Verbal and Nonverbal Communication With Black and White Patients at the End of Life. *Journal of Pain and Symptom Management, 51*(1), 1–8. https://doi.org/10.1016/j.jpainsymman.2015.07.008
  - The source of our research vignette.

### Packages

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. Remove the hashtags for the code to work.
```{r  tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#will install the package if not already installed
#if(!require(psych)){install.packages("psych")}
#if(!require(faux)){install.packages("faux")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(dplyr)){install.packages("dplyr")}
#if(!require(lsr)){install.packages("lsr")}
#if(!require(ggpubr)){install.packages("ggpubr")}
```

## *z* before *t*

**Probability density functions** are mathematical formula that specifies idealized versions of known distributions. The equations that define these distributions allow us to calculate the probability of obtaining a given score. This is a powerful tool. 

As students progress through statistics, they become familiar with a variety of these distributions including the *t*-distribution (commonly used in *t*-tests), *F*-distribution (commonly used in analysis of variance [ANOVA]), and Chi-square ($X^2$) distributions (used in a variety of statistics, including structural equation modeling). The *z* distribution is the most well-known of these distributions.

The *z* distribution is also known as the normal distribution, the bell curve, or the standard normal distribution. Its mean is always 0.00 and its standard deviation is always 1.00.

```{r echo=FALSE}
#http://rstudio-pubs-static.s3.amazonaws.com/78857_86c2403ca9c146ba8fcdcda79c3f4738.html
x=seq(-3,3,length=200)
y=dnorm(x,mean=0,sd=1)
plot(x,y, type="l")#a lower case "L" not the number 1
```
We can draw such function by plotting the value of a given variable (*x*) against the probability of it occurring (*y*). We will do this several times in this lesson.

Using the following formula, any set of scores can be transformed to *z*-scores:

$$z=\frac{X-\bar{X}}{s}$$
Later in this larger section on *t*-tests we introduce a research vignette that focuses on time physicians spend with patients. Because working with the *z*-test requires a minimum sample size of 120 (and the research vignette has a sample size of 33), I will quickly create normally distributed sample of 200 with a mean of 10 minutes per patient and a standard deviation of 2.

```{r}
#https://r-charts.com/distribution/histogram-curves/
set.seed(220821)
PhysTime <- data.frame(minutes = rnorm(200, mean=10, sd=2))
```

Using the *describe()* function from the *psych* package, we can see the resulting descriptive statistics.

```{r}
psych::describe(PhysTime$minutes)
```
Specifically, in this sample size of 200, our mean is 9.9 with a standard deviation of 2.0.

Using *z*-score formula $z=\frac{X-\bar{X}}{s}$, let's calculate the standard scores (i.e., *z*-scores) for values of 8, 9, and 12:
```{r}
(8 - 9.9)/2 #for 8 minutes
(9.9-9.9)/2 #for 9.9 minutes
(12-9.9)/2 #for 12 minutes
```
Our results show that 

* 8 minutes is -0.95 *SD* below the mean
* 9.9 minutes is at the mean
* 12 minutes is 1.05 *SD above the mean

We could create a column of *z*-scores, this way:

```{r}
PhysTime$zMinutes <- (PhysTime$minutes - mean(PhysTime$minutes))/sd(PhysTime$minutes)
head(PhysTime)
```
  




```{r}
hist(time, prob=TRUE)
```


```{r}
T2 <- seq(min(time), max(time), length = 40)
fun <- dnorm(T2, mean = mean(T2), sd = sd(T2))
hist(time, prob=TRUE, col="white", ylim = c(0, max(fun)), main="Histogram of Physician Time with Normal Curve")
lines(T2, fun, col=2, lwd=2)
lines(density(time), col=4, lwd=2)
```




```{r}
#I used the tutorial below to guide the creation of the figures used in this section
#http://rstudio-pubs-static.s3.amazonaws.com/78857_86c2403ca9c146ba8fcdcda79c3f4738.html
x=seq(-3,3,length=120)
y=dnorm(x,mean=0,sd=1)
plot(x,y,type="l")
x=seq(-3,0,length=60)
y=dnorm(x,mean=0,sd=1)
polygon(c(-3,x,0),c(0,y,0),col="darkmagenta")
```

```{r}
#I used the tutorial below to guide the creation of the figures used in this section
#http://rstudio-pubs-static.s3.amazonaws.com/78857_86c2403ca9c146ba8fcdcda79c3f4738.html
x=seq(-3,3,length=200)
y=dnorm(x,mean=0,sd=1)
plot(x,y,type="l")
x=seq(-3,0,length=100)
y=dnorm(x,mean=0,sd=1)
polygon(c(-3,x,0),c(0,y,0),col="purple")
```


```{r}
x=seq(20,80,length=200)
y=dnorm(x,mean=50,sd=10)
plot(x,y,type="l")
```




## Introducing the One Sample *t*-test

The one-sample *t* test is used to evaluate whether the mean of a sample differs from another value that, symbolically, is represented as the population mean. Green and Salkind [@green_using_2014] noted that this value is often the midpoint of set of scores, the average value of the test variable based on past research, or a test value as the chance level of performance. 

![An image of a row with two boxes labeled Condition A (in light blue) and the population mean (in dark blue) to which it is being compared. This represents the use of a one sample *t* test.](images/ttests/onesample.jpg)
This comparison is evident in the numerator of the formula for the *t* test that shows the population mean $\mu$ being subtracted from the sample mean$\bar{X}$.

$$
t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{N} }
$$
Although this statistic is straightforward, it is quite limited. If the researcher wants to compare an outcome variable across two groups of people, they should consider the [independent samples *t*-test](#tIndSample). If the participant wants to evaluate an outcome variable with two observations from the same group of people, they should consider the [paired samples *t*test](#tPaired)

### Workflow for the One Sample *t*-test

The following is a proposed workflow for conducting a one-sample *t*-test.

![A colorful image of a workflow for the one sample *t*-test](images/ttests/OneSampleWrkFlw.jpg) 

If the data meets the assumptions associated with the research design (e.g., independence of observations and a continuously scaled metric), these are the steps for the analysis of a one sample *t* test:

1. Prepare (upload) data.
2. Explore data with
   - graphs
   - descriptive statistics
3. Assess normality via skew and kurtosis
4. Select the comparison (i.e., test) value
5. Compute the one sample *t*-test
6. Compute an effect size (frequently the *d* statistic)
7. Manage Type I error
8. Sample size/power analysis (which you should think about first, but in the context of teaching statistics, it's more pedagogically sensible, here).

## Research Vignette

Empirically published articles where *t* tests are the primary statistic are difficult to locate. Having exhausted the psychology archives, I located this article in an interdisciplinary journal focused on palliative medicine. The research vignette for this lesson examined differences in physician's verbal and nonverbal communication with Black and White patients at the end of life [@elliott_differences_2016]. 

Elliott and colleagues [-@elliott_differences_2016] were curious to know if hospital-based physicians (56% White, 26% Asian, 7.4% each Black and Hispanic) engaged in verbal and nonverbal communication differently with Black and White patients. Black and White patient participants were matched on characteristics deemed important to the researchers (e.g., critically and terminally ill, prognostically similar, expressed similar treatment preferences). Interactions in the intensive care unit were audio and video recorded and then coded on dimensions of verbal and nonverbal communication.

Because each physician saw a pair of patients (i.e., one Black patient and one White patient), the researchers utilized a paired samples, or dependent *t*-test. This statistical choice was consistent with the element of the research design that controlled for physician effects through matching (and one we will work in a later lesson). Below are the primary findings of the study.

|                |Black Patients |White Patients |         |
|:---------------|:--------------|:--------------|:--------|
|Category        |*Mean*(*SD*)   |*Mean*(*SD*)   |*p*-value|
|Verbal skill score (range 0 - 27)|8.37(3.36) | 8.41(3.21) |0.958|
|Nonverbal skill score (range 0 - 5) |2.68(.84) | 2.93(.77)|0.014|

In the research vignette Elliott et al. [-@elliott_differences_2016] indicated that physician/patient visits lasted between 3 minutes and 40 seconds (220 seconds) to 20 minutes and 13 seconds (1213 seconds). For the purpose of demonstrating the one sample *t*-test, we might want to ask whether the length of patient visits in this research study were statistically significantly different than patient in the ICU or in palliative care, more broadly. Elliott et al.[-@elliott_differences_2016] did not indicate a measure of central tendency (i.e., mean, mode, median) therefore, I will simulate the data by randomly generating 33 numbers between 220 and 1213. I will use *random selection with replacement*, which allows the same number to be selected more than once.

A warning: this particularly analysis is "more simulated than usual" and does not represent reality. However, this research vignette lends itself for this type of question.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Setting the "random" seed ensures that everyone gets the same result, every time they rerun the analysis.
#My personal practice is to create a random seed that represents the day I write up the problem (in this case August, 15, 2022)
#When the Suggestions for Practice invite you to "change the random seed," simply change this number to anything you like (maybe your birthday or today's date)
set.seed(220815)

#Assigns as physician ID number to each row
#Simulates 33 numbers between 220 and 1213 
dfOneSample <- data.frame(
  ID = factor(seq(1:33)),
  PhysicianSeconds = sample(220: 1213,33, replace=TRUE)
)

#Displays the first 6 rows of the df
head(dfOneSample)
```

With our data in hand, let's examine its structure. The variable representing physician seconds represents the ratio scale of measurement and therefore should be noted as *num* (numerical) or *int* (integer; whole numbers) in R.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
str(dfOneSample)
```
Below is code for saving (and then importing) the data in .csv or .rds files. I make choices about saving data based on what I wish to do with the data. If I want to manipulate the data outside of R, I will save it as a .csv file. It is easy to open .csv files in Excel. A limitation of the .csv format is that it does not save any restructuring or reformatting of variables. For this lesson, this is not an issue. 

Here is code for saving the data as a .csv and then reading it back into R. I have hashtagged these out, so you will need to remove the hashtags if you wish to run any of these operations.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#writing the simulated data as a .csv 
#write.table(dfOneSample, file = "dfOneSample.csv", sep = ',', col.names=TRUE, row.names=FALSE) 
#at this point you could clear your environment and then bring the data back in as a .csv
#reading the data back in as a .csv file
#dfOneSample<- read.csv ('dfOneSample.csv', header = TRUE)
```

The .rds form of saving variables preserves any formatting (e.g., creating ordered factors) of the data. A limitation is that these files are not easily opened in Excel. Here is the hashtagged code (remove hashtags if you wish to do this) for writing (and then reading) this data as an .rds file.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#saveRDS(dfOneSample, 'dfOneSample.rds')
#dfOneSample <- readRDS('dfOneSample.rds')
```


## Working the Problem

### Stating the Hypothesis

A quick scan of the literature suggests that health care workers' visits to patients in the ICU are typically quite brief. Specifically, the average duration of a physician visit in a 2018 study was 73.5 seconds [@butler_estimating_2018]. A one-sample *t* test is appropriate for comparing the visit lengths from our sample to this external metric. 

As noted in the symbolic presentation below, our null hypothesis ($H_0$) states that our data will be equal to the test value of 73.5 seconds. In contrast, the alternative hypothesis ($H_A$) states that these values will not be equal.

$$
\begin{array}{ll}
H_0: & \mu = 73.5 \\
H_1: & \mu \neq 73.5
\end{array}
$$

### Preliminary Exploration

Plotting the data is best practice to any data analysis. The *ggpubr* package is one of my go-to-tools for quick and easy plots of data. Below, I have plotted the time-with-patient (Physician Seconds) variable and added the mean. As with most plotting packages, ggpubr will "bin" (or cluster) the data for plotting; this is especially true for data with a large number of units (a range from 220 to 1213 is quite large). The "rug = TRUE" command added a lower row of the table to identify where each of the datapoint follows.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggpubr::gghistogram(dfOneSample, x = "PhysicianSeconds",  add = "mean",  rug = TRUE)
```

The histogram makes it clear that our data does not reflect a standard normal curve. 

Another view of our data is with a boxplot. The box captures the middle 50% of data with the horizontal bar at the median. The whiskers extend three standard deviations around the mean with dots beyond the whiskers representing outliers. I personally like the *add="jitter"* statement because it shows where each case falls.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggpubr::ggboxplot(dfOneSample$PhysicianSeconds,
         ylab = "Seconds with Patient", xlab = FALSE, add="jitter"
         )
```
We can further evaluate normality by obtaining the descriptive statistics with the *describe()* function from the *psych* package.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describe(dfOneSample$PhysicianSeconds)
```

Here we see that our scores range from 225 to 1197 with a mean of 717.12 and a standard deviation of 312.22.  We're ready to calculate the one sample *t*-test.

### Hand-Calculations

In learning the statistic, hand-calculations can help understand what the statistic is doing. Here's the formula again:

$$
t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{N} }
$$

The denominator of the formula below subtracts the test value from the sample mean. The denominator involves multiplying the standard deviation by the square root of the sample size. The descriptive statistics provided the values we need to complete the analysis:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
(717.12 - 73.5)/(312.22/sqrt(33))
```

#### Statistical Significance  

If we ask about *statistical significance* then we are likely engaged in *null hypothesis significance testing* (NHST). In the case of a one sample test, we construct our hypothesis with a null and an alternative that are relatively straightforward. Specifically, we are interested in knowing if our sample mean (717.12) is statistically, significantly different from the test value of 73.5.  We can write the hypotheses in this way:

$$
\begin{array}{ll}
H_0: & \mu = 73.5 \\
H_1: & \mu \neq 73.5
\end{array}
$$
In two parts, our null hypothesis ($H_0$) states that the population mean ($H_0$) for physician visits with palliative care patients is 73.5; the alternative $\mu \neq$ states that it is not 73.5.

When we calculated the *t* test, we obtained a *t* value. We can check the statistical significance by determining the test critical value from a [table of critical values](https://www.statology.org/t-distribution-table/) for the *t* distribution. There are many freely available on the internet. If our *t* value exceeds the value(s) in the table of critical values, then we can claim that our sample mean is statistically significantly different from the hypothesized value.

Heading to the table of critical values we do the following:

* For the one-sample *t* test, the degrees of freedom (DF) is equal to the sample size (33). The closest value in our table is 30, so we will use that row.
* A priorily, we did not specify if we thought the difference would be greater, or lower. Therefore, we will use a column that indicates *two-tails*.
* A *p* value of .05 is customary.
* Thus, if our *t* value is lower than -2.042 or higher than 2.042 we know we have a statistically significant difference.

In our case, the *t* value of 11.03 exceeded the test critical value of 2.042. We would write the statistical string this way: *t*(33) = 11.84, *p* < .05.

In base R, the *qt()* function will look up a test critical value. For the one-sample *t* test, degrees of freedom (df) is equal to the sample size. We "divide the *p* value by 2" when we want a two-tailed test. Finally, the "lower.tail" command results in positive or negative values in the tail.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
qt(p=.05/2, df=33,lower.tail=FALSE)
```
Not surprisingly, this value is quite similar to the value we saw in the table. The *qt()* function is more accurate because it used df of 33 (not rounded down to 30).

#### Confidence Intervals

How confident are we in our result? With the one sample *t*-test, it is common to report an interval in which we are 95% confident that our true mean difference exists. Below is the formula, which involves:

* $\bar{X}$ is the sample mean; in our case this is 717.12
* $t_{cv}$ the test critical value for a two-tailed model (even if the hypothesis was one-tailed) where $\alpha = .05$ and the degrees of freedom are $N-1$
* $\frac{s}{\sqrt{n}}$ was the denominator of the test statistic it involves the standard deviation of our sample (312.22) and the square root of our sample size (33)

$$\bar{X} \pm t_{cv}(\frac{s}{\sqrt{n}})$$
Let's calculate it:

First, let's get the proper *t* critical value. Even though these are identical to the one above, I am including them again. Why? Because if the original hypothesis had been one-tailed, we would need to calculate a two-tailed confidence interval; this is a placeholder to remind us.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
qt(p=.05/2, df=32,lower.tail=FALSE)
```
Using the values from above, we can specify both the lower and upper bound of our confidence interval.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
(717.12) - ((2.0369)*(312.22/sqrt(33)))
(717.12) + ((2.0369)*(312.22/sqrt(33)))
```
The resulting interval is the 95% confidence interval around our sample mean. Stated another way, we are 95% certain that the true mean of time with patients in our sample ranges between 606.41 and 827.83 seconds.

#### Effect size

If you have heard someone say something like, "I see there is statistical significance, but it the difference *clinically significant*," the person is probably asking about *effect sizes.*  Effect sizes provide an indication of the magnitude of the difference.

The *d* statistic is commonly used with *t* tests; *d* assesses the degree that the mean on the test variable differs from the test value. Conveniently, *d* represents standard deviation units. A *d* value of 0 indicates that the mean of the sample equals the mean of the test value. As *d* moves away from 0 (in either direction), we can interpret the effect size to be stronger. Conventionally, the absolute values of .2, .5, and .8, represent small, medium, and large effect sizes, respectfully.

Calculating the *d* statistic is easy. Here are two equivalent formulas:

$$d = \frac{Mean Difference}{SD}=\frac{t}{\sqrt{N}}$$
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#First formula
(717.12 - 73.5)/312.22
#Second formula
11.842/sqrt(33)
```
The value of 2.06 indicates that the test value is approximately two standard deviations away from the sample mean. This is a very large difference.

## Computation in R

Calculating a one sample *t*-test is possible through base R and a number of packages. Navarro's [-@navarro_book_2020] *lsr* package provides output that is commonly used in psychology.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
lsr::oneSampleTTest(x=dfOneSample$PhysicianSeconds, mu=73.5)
```
This well-organized output has everything we need for an APA style presentation of results. Identical to all the information we hand-calculated, we would write the *t* string this way:  $t(32) = 11.842, p < .001, d = 2.061$. The *lsr* output also includes confidence intervals. These represent the 95% confidence interval of the true difference between the means. That is, we are 95% confident that the true mean of the physicians in our sample falls between 606.414 and 827.828.

## APA Style Results

Let's write up the results.  I would probably choose to include the boxplot produced in the initial exploration of the data.

> A one-sample *t*-test was used to evaluate whether average amount of time that a sample of physicians (palliative care physicians in the ICU) enrolled in a research study on patient communication was statistically significantly different from the amount of time that ICU physicians spend with their patients, in general. The sample mean 717.121 (312.216) was significantly different from 73.5, $t(33) = 11.84, p < .001., d = 2.061$. The effect size, (*d*) indicates a very large effect. Figure 1 illustrates the distribution of time physicians in the research study spent with their patients. The results support the conclusion that physicians in the research study spent more time with their patients than ICU physicians in general.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggpubr::ggboxplot(dfOneSample$PhysicianSeconds,
         ylab = "Seconds with Patient", xlab = FALSE, add="jitter", title = "Figure 1. Physician Time with Patients (in seconds)"
         )
```

Reflecting on these results, I must remind readers that this simulated data that is even further extrapolated. Although "data" informed both the amount of time spent by the physicians in the research study and data used as the test value, there are probably many reasons that the test value was not a good choice. For example, even though both contexts were ICU, palliative physicians may have a different standard of care than ICU physicians "in general." 


## Power in Independent Samples *t* tests

Researchers often use power analysis packages to estimate the sample size needed to detect a statistically significant effect, if, in fact, there is one. Utilized another way, these tools allows us to determine the probability of detecting an effect of a given size with a given level of confidence. If the probability is unacceptably low, we may want to revise or stop. A helpful overview of power as well as guidelines for how to use the *pwr* package can be found at a [Quick-R website](https://www.statmethods.net/stats/power.html) [@kabacoff_power_2017].

In Champely's *pwr* package, we can conduct a power analysis for a variety of designs, including the one sample *t* test that we worked in this lesson. There are a number of interrelating elements of power:

* Sample size, *n* refers to the number of observations; our vignette had 33
* *d* refers to the difference between means divided by the pooled standard deviation; ours was (
* *power* refers to the power of a statistical test; conventionally it is set at .80
* *sig.level* refers to our desired alpha level; conventionally it is set at .05
* *type* indicates the type of test we ran; this was "one.sample"
* *alternative* refers to whether the hypothesis is non-directional/two-tailed ("two.sided") or directional/one-tailed("less" or "greater")
 
In this script, we must specify *all-but-one* parameter; the remaining parameter must be defined as NULL. R will calculate the value for the missing parameter.

When we conduct a "power analysis" (i.e., the likelihood of a hypothesis test detecting an effect if there is one), we specify, "power=NULL". Using the data from our results, we learn from this first run, that our statistical power was 1.00. That is, given the value of the mean difference relative to the pooled standard deviation we had a 100% chance of detecting a statistically significant effect if there was one.


```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d= (717.121-73.5)/312.216,n = 33, power=NULL,sig.level=0.05,type="one.sample",alternative="two.sided")
```
Researchers frequently use these tools to estimate the sample size required to obtain a statistically significant effect. In these scenarios we set *n* to *NULL*. Using the results from the simulation of our research vignette, you can see that we would have needed 21,022 individuals for the *p* value to be < .05.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d= (717.121-73.5)/312.216, n = NULL, power=0.8,sig.level=0.05,type="one.sample",alternative="two.sided")
```
Shockingly, this suggests that a sample size of four could result in a statistically significant result. Let's see if this is true. Below I will re-simulate the data for the verbal scores, changing only the sample size:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Setting the "random" seed ensures that everyone gets the same result, every time they rerun the analysis.
#My personal practice is to create a random seed that represents the day I write up the problem (in this case August, 15, 2022)
#When the Suggestions for Practice invite you to "change the random seed," simply change this number to anything you like (maybe your birthday or today's date)
set.seed(220820)

#Assigns as physician ID number to each row
#Simulates 33 numbers between 220 and 1213 
rdfOneSample <- data.frame(
  ID = factor(seq(1:4)),
  PhysicianSeconds = sample(220: 1213,4, replace=TRUE)
)

#Displays the first 6 rows of the df
head(rdfOneSample)
```
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
lsr::oneSampleTTest(x=rdfOneSample$PhysicianSeconds, mu=73.5)
```
In this case our difference between the sample data and the external data is so huge, that a sample of four still nets a statistically significant result. This is unusual. Here's the *t* string:  $t(3) = 914.50, p = 0.001, d = 2.85, 95%CI[444.74, 1384.26]$.

## Practice Problems

The suggestions for homework are graded in complexity and I encourage you to select one or more that meets you where you are (e.g., in terms of your self-efficacy for statistics, your learning goals, and competing life demands).

### Problem #1: Rework the research vignette as demonstrated, but change the random seed

If this topic feels a bit overwhelming, simply change the random seed in the data simulation of the research vignette, then rework the problem. This should provide minor changes to the data but the results will likely be very similar. That said, don't be alarmed if what was non-significant in my working of the problem becomes significant. Our selection of *p* < .05 (and the corresponding 95% confidence interval) means that 5% of the time there could be a difference in statistical significance.

### Problem #2:  Rework the research vignette, but change something about the simulation

Rework the one sample *t*-test in the lesson by changing something else about the simulation. Perhaps estimate another comparative number. The 73.5 seconds was a dramatic difference from the mean of the research participants. Perhaps suggest (and, ideally, support with a reference) another number. Alternatively, if you are interested in issues of power, specify a different sample size.

### Problem #3:  Use other data that is available to you

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete an independent samples *t* test. 

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Narrate the research vignette, describing the variables and their role in the analysis | 5 |_____  |
|2. Simulate (or import) and format data |      5            |_____  |           
|3. Evaluate statistical assumptions     |      5            |_____  |
|4. Conduct a one sample *t* test (with an effect size) |5 | _____  |  
|5. APA style results with table(s) and figure  |    5  |_____  |       
|6 Explanation to grader                 |      5        |_____  |
|**Totals**                              |      30      |_____  |          

```{r include=FALSE}
sessionInfo()
```




<!--chapter:end:04-tOneSample.Rmd-->


# Independent Samples *t* test {#tIndSample}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing the Independent Samples *t* Test
### Workflow for Independent Samples *t* test
## Research Vignette
## Working the Problem
### Stating the Hypothesis
### Preliminary Exploration
### Hand-Calculations
#### Statistical Significance
#### Confidence Intervals
#### Effect Size
## Computation in R
### What if we had violated the homogeneity of variance assumption?
## APA Style Results
## Power in Independent Samples *t* tests
## Practice Problems
### Problem #1: Rework the research vignette as demonstrated, but change the random seed
### Problem #2:  Rework the research vignette, but change something about the simulation
### Problem #3:  Rework the research vignette, but swap one or more variables
### Problem #4:  Use other data that is available to you

<!--chapter:end:05-tIndSample.Rmd-->


# Paired Samples *t*-test {#tPaired}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing the Paired Samples *t*-test
## Workflow for Paired Samples *t*-test
## Research Vignette
### Simulating Data for the Paired Samples *t* test
## Working the Problem
### Stating the Hypothesis
### Preliminary Exploration
### Hand Calculations
#### Statistical Significance
#### Confidence Intervals
#### Effect Size
## Computation in R
## APA Style Results
## Power in Paired Samples *t* tests
## Practice Problems
### Problem #1: Rework the research vignette as demonstrated, but change the random seed
### Problem #2:  Rework the research vignette, but change something about the simulation
### Problem #3:  Rework the research vignette, but swap one or more variables
### Problem #4:  Use other data that is available to you

<!--chapter:end:06-tPairedSamples.Rmd-->


# Analysis of Variance {-}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Workflow for One-Way ANOVA
## Research Vignette
### Data Simulation
## Working the Problem
### Preparing the Data
### Exploring the Distributional Characteristics Numerically
### Exploring the Distributional Characteristics Graphically
## Understanding ANOVA with *Hand Calculations*
### Sums of Squares Total
### Sums of Squares for the Model (or Between)
### Sums of Squares Residual (or within)
#### On the relationship between standard deviation and variance
### Relationship between $SS_T$, $SS_M$, and $SS_R$. 
### Mean Squares Model & Residual
### Calculating the *F* Statistic
### Source Table Games
## Working the One-Way ANOVA in R
### Evaluating the Statistical Assumptions
#### Is the dependent variable normally distributed across levels of the factor?
#### Are the variances of the dependent variable similar across the levels of the grouping factor?
#### Summarizing results from the analysis of assumptions
### Computing the Omnibus ANOVA
#### Effect size for the one-way ANOVA
#### Summarizing results from the omnibus ANOVA
### Follow-up to the Omnibus *F*
#### OPTION 1: Post-hoc, pairwise, comparisons 
#### OPTION 2: Planned contrasts (non-orthogonal)
#### OPTION 3: Planned contrasts (orthogonal)
#### OPTION 4: Trend (polynomial) analysis
#### Which set of follow-up tests do we report?
### What if we Violated the Homogeneity of Variance test?
## Power Analysis
## APA Style Results
## A Conversation with Dr. Tran
## Practice Problems
### Problem #1: Play around with this simulation.
### Problem #2: Conduct a one-way ANOVA with the *moreTalk* dependent variable.
### Problem #3: Try something entirely new.
### Grading Rubric
## Bonus Reel: 
### What's with the inline code?

<!--chapter:end:07-OnewayANOVA.Rmd-->


# Factorial (Between-Subjects) ANOVA {#between}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing Factorial ANOVA
### Workflow for Two-Way ANOVA
## Research Vignette
### Preliminary exploration of our research vignette
## Working the Factorial ANOVA (by hand)
### Sums of Squares Total
### Sums of Squares for the Model 
### Sums of Squares Residual (or within)
### A Recap on the Relationship between $SS_T$, $SS_M$, and $SS_R$
### Calculating SS for Each Factor and Their Products
#### Rater Main Effect
#### Photo Main Effect
#### Interaction effect
### Source Table Games!
### Interpreting the results
## Working the Factorial ANOVA with R packages
### Evaluating the statistical assumptions
#### DV is normally distributed
#### Homogeneity of variance
### Evaluating the Omnibus ANOVA
#### Effect sizes
#### APA Write-up of the omnibus results
### Follow-up a significant interaction effect
#### Option #1 the simple main effect of photo stimulus within ethnicity of the rater
#### Option #2 the simple main effect of ethnicity of rater within photo stimulus.
#### Option #3 post hoc comparisons
#### Option #4 polynomial trends 
## Investigating Main Effects
### Follow-up with all Post-Hocs
### Follow-up with planned contrasts
### Polynomial Trends
## My APA Style Results Section
### Comparing Our Results to Rhamdani et al. [-@ramdhani_affective_2018]
## Options for Assumption Violations
## Power 
### Post Hoc Power Analysis
### Estimating Sample Size Requirements
## Practice Problems
### Problem #1: Play around with this simulation.
### Problem #2: Conduct a factorial ANOVA with the *positive evaluation* dependent variable.
### Problem #3: Try something entirely new.
### Grading Rubric

<!--chapter:end:08-FactorialANOVA.Rmd-->


# One-Way Repeated Measures ANOVA {#Repeated}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing One-way Repeated Measures ANOVA
### Workflow for Oneway Repeated Measures ANOVA
## Research Vignette
### Code for simulating the data used today.
### Quick peek at the data
## Working the One-Way Repeated Measures ANOVA (by hand)
### Sums of Squares Total
### Sums of Squares Within for Repated Measures ANOVA
### Sums of Squares Model -- Effect of Time
### Sums of Squares Residual
### Sums of Squares Between
### Mean Squares Model & Residual
### *F* ratio
## Working the One-Way ANOVA with R packages
### Testing the assumptions
#### Univariate assumptions for repeated measures ANOVA
#### Demonstrating sphericity
#### Any outliers?
#### Assessing normality 
#### Assumption of Sphericity
### Omnibus Repeated Measures ANOVA
### Follow-up
### Results Section
#### Creating an APA Style Table**
#### Comparison with Amodeo et al.[-@amodeo_empowering_2018]
## Power in Repeated Measures ANOVA
## Practice Problems
### Problem #1: Change the Random Seed
### Problem #2: Increase *N*
### Problem #3: Try Something Entirely New
### Grading Rubric
## Bonus Reel: 

<!--chapter:end:09-OneWayRepeated.Rmd-->


# Mixed Design ANOVA {#Mixed}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing Mixed Design ANOVA
## Research Vignette
### Simulating the data from the journal article
## Working the Mixed Design ANOVA with R packages
### Exploring data and testing assumptions
#### Assumption of Normality
#### Homogeneity of variance assumption
#### Assumption of homogeneity of covariance matrices
#### APA style writeup of assumptions
### Omnibus ANOVA
#### Checking the sphericity assumption
### Simple main effect of condition within wave
### Simple main effect of wave within condition
### If we only had a main effect
### APA Style Write-up of the Results
#### Results
#### Comparing our findings to Murrar and Brauer [-@murrar_entertainment-education_2018] 
## Power in Mixed Design ANOVA
## Practice Problems
### Problem #1: Play around with this simulation.
### Problem #2: Conduct a one-way ANOVA with a different dependent variable.
### Problem #3: Try something entirely new.
### Grading Rubric

<!--chapter:end:10-MixedANOVA.Rmd-->


# Analysis of Covariance {#ANCOVA}

Placeholder


## Navigating this Lesson
### Learning Objectives
### Planning for Practice
### Readings & Resources
### Packages
## Introducing Analysis of Covariance (ANCOVA)
## Research Vignette
### Simulating the data from the journal article
## Scenario #1: Controlling for the pretest
### Preparing the data
### Checking the assumptions
#### Linearity assumption
#### Homogeneity of regression slopes
#### Normality of residuals
#### Homogeneity of variances
#### Outliers
#### Write-up of Assumptions
### Calculating the Omnibus ANOVA
### Post-hoc pairwise comparisons (controlling for the covariate)
### Toward an APA style results section
## Scenario #2: Controlling for a confounding or covarying variable
### Preparing the data
### Checking the assumptions
#### Linearity assumption
#### Homogeneity of regression slopes
#### Normality of residuals
#### Homogeneity of variances
#### Outliers
#### Write-up of Assumptions
### Calculating the Omnibus ANOVA
### Post-hoc pairwise comparisons (controlling for the covariate)
### Toward an APA style results section
## More (and a recap) on covariates
## Practice Problems
### Problem #1: Play around with this simulation.
### Problem #2: Conduct a one-way ANCOVA with the DV and covariate at post2.
### Problem #3: Try something entirely new.
### Grading Rubric

<!--chapter:end:11-ANCOVA.Rmd-->

