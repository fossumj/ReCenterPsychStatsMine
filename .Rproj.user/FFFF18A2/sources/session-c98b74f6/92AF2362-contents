## Homeworked Example
[Screencast Link]()

*If you wanted to use this example and dataset as a basis for a homework assignment, you could create a different subset of data. I worked the example for students taking the ANOVA class. You could choose multivariate or psychometrics. You could also choose a different dependent variable. I chose the traditional pedagogy subscale. Two other subscales include socially responsive pedagogy and valued by the student.*

### Working the Problem with R and R Packages

#### Narrate the research vignette, describing the IV and DV. The data you analyze should have at least 3 levels in the independent variable; at least one of the attempted problems should have a significant omnibus test so that follow-up is required).

I want to ask the question, do course evaluation ratings for traditional pedagogy differ for students as we enacted a substantive revision to our statistics series.  The evaluative focus is on the ANOVA course and we will compare ratings from the stable, transition, and resettled stages of the transitional period. The variable (Stage) of interest will have three levels:  

* STABLE:  2017 represents the last year of "stability during the old way" when we taught with SPSS and during the 2nd year of the doctoral programs.
* TRANSITION:  2018 & 2019 represent the transition to R, when the classes were 30% larger because each of the IOP and CPY departments were transitioning to the 1st year (they did it separately, so as not to double the classes)
* RESETTLED:  2020 & 2021 represent the "resettled" phase where the transition to R was fairly complete and the class size returned to normal because the classes were offered in the first year.

This is not a variable that was included in the dataset posted to the OSF repository, so we will need to create it.

#### Simulate (or import) and format data.  

```{r}
big <- readRDS("ReC.rds")
```

This df includes course evaluations from ANOVA, multivariate, and psychometrics. To include up to three evaluations per student would violate the assumption of independence, therefore, I will only select the students in ANOVA course.

```{r}
big <- subset(big, Course == "ANOVA") 
```


Let's first create the "Stage" variable that represents the three levels of transition.

First I will map the years to the three levels (factors).

```{r}
big$Stage <- plyr::mapvalues(big$Year, from = c(2017, 2018, 2019, 2020, 2021), to = c("Stable", "Transition", "Transition", "Resettled", "Resettled"))
```

Then check the structure.
```{r}
str(big$Stage)
```
R is reading the variable as a character, so I need to make it to be an ordered factor.

```{r}
big$Stage <- factor(big$Stage, levels = c("Stable", "Transition", "Resettled"))
```

Let's check the structure again:

```{r}
str(big$Stage)
```

The TradPed (traditional pedagogy) variable is an average of the items on that scale. I will first create that variable.

```{r}
#Creates a list of the variables that belong to that scale
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')

#Calculates a mean if at least 75% of the items are non-missing; adjusts the calculating when there is missingness
big$TradPed <- sjstats::mean_n(big[, TradPed_vars], .75)

```

With our variables properly formatted, let's trim it to just the variables we need.
```{r}
OneWay_df <-(dplyr::select (big, Stage, TradPed))
```

Although we would handle missing data more carefully in a "real study," I will delete all cases with any missingness. This will prevent problems in the hand-calculations section, later (and keep the two sets of results more similar).

```{r}
df <- na.omit(OneWay_df)
```


#### Evaluate statistical assumptions. 

**Is the dependent variable normally distributed across levels of the factor?**

```{r}
psych::describeBy(TradPed ~ Stage, mat = TRUE, digits = 3, data = OneWay_df, type = 1)
```

We'll use Kline's (2016) threshholds of the absolute values of 3 (skew) and 10 (kurtosis). The highest absolute value of skew is -1.31; the highest absolute value of kurtosis is 1.18. These are well below the areas of concern.

the Shapiro-wilk test is a formal assessment of normality. It is a 2-part test that begins with creating an ANOVA model from which we can extract residuals, then testing the residuals.  

```{r}
TradPed_res <- lm(TradPed ~ Stage, data = OneWay_df)
rstatix::shapiro_test(residuals(TradPed_res))
```
The Shapiro-Wilk test suggests that the our distribution of residuals is statistically significantly different from a normal distribution $(W = 0.910, p < .001)$.

It is possible to plot the residuals to see how and where they deviate from the line.

```{r}
ggpubr::ggqqplot(residuals(TradPed_res))
```
Ooof!  at the ends of the distribution they really deviate. 

**Should we remove outliers?**

The *rstatix::identify_outliers()* function identifies outlers and extreme outliers.

```{r}
library(tidyverse)
OneWay_df %>%
  rstatix::identify_outliers(TradPed)
```

There are 4 cases identified with outliers; none of those is extreme. I also notice that these outliers are low course evaluations. It seems only fair to retain the data from individuals who were not satisfied with the course.

**Are the variances of the dependent variable similar across the levels of the grouping factor?**

We want the results of the Levene's homogeneity of variance test to be non-significant. This would support the notion that the TradPed variance is equivalent across the three stages of the transition.

```{r}
rstatix::levene_test(OneWay_df, TradPed ~ Stage)
```
The non-significant *p* value suggests that the variances across the three stages are not statistically significantly different:  $F(2, 109) = 2.094, p = 0.128$.


Before moving on, I will capture our findings in an APA style write-up of the testing of assumptions:

>Regarding the assumption of normality, skew and kurtosis values at each of the levels of program year fell well below the thresholds that Kline (2016a) identified as concerning (i.e., below |3| for skew and |10| for kurtosis). In contrast, results of a model-based Shapiro-Wilk test of normality, indicated that the model residuals differed from a normal distribution $(W = 0.910, p < .001)$. Although 4 outliers were identified none were extreme, thus we retained all cases. Finally, Levene’s homogeneity of variance test indicated no violation of the homogeneity of variance assumption $F(2, 109) = 2.094, p = 0.128$.

#### Conduct omnibus ANOVA (w effect size). 

The *rstatix::anova_test()* function calculates the one-way ANOVA and includes the effect size, $\eta^2$ in the column, *ges*. Values of .01, .07, and .14 are considered to be small, medium, and large. The value of .05 would be small-to-medium.

```{r}
omnibus1w <- rstatix::anova_test(OneWay_df, TradPed ~ Stage, detailed = FALSE)
omnibus1w
```
The one-way ANOVA is not statistically significant. This means there should not be differences between any combination of variables in the dependent variable. Before moving on, I will capture the *F* string:  $F(2, 109) = 2.61, p = 0.078, \eta^2 = 0.046$.

Normally, the researcher would stop here. However, because the homework requires follow-up, I will continue.

#### Conduct one set of follow-up tests; narrate your choice.

I will simply calculate post-hoc comparisons. That is, all possible pairwise comparisons. I will specify the traditional Bonferroni as the approach to managing Type I error.

```{r}
phoc <- rstatix::t_test(OneWay_df, TradPed ~ Stage, p.adjust.method = "bonferroni", detailed = TRUE)
phoc
```

Curiously, the post hoc tests suggested statistically significant differences between the stable and resettled stages, favoring the stable period of time (i.e., using SPSS and taught in the second year).

#### Describe approach for managing Type I error. 

We used the Bonferroni. The Bonferroni divides the overall alpha (.05) by the number of comparisons (3). In this case, a *p* value woul dhave to be lower than 0.017 to be statistically significant. The calulation reverse-engineers this so that we can interpret the *pI values by the traditional. 0.05. In the output, it is possible to see the higher threshholds necessary to claim statistical significance.

#### APA style results with table(s) and figure.  

>A one-way analysis of variance was conducted to evaluate the effects significant transitions (e.g., from SPSS to R; to the second to the first year in a doctoral program) on students ratings of traditional pedagogy. The independent variable, stage, included three levels: stable (with SPSS and taught in the second year of a doctoral program), transitioning (with R and students moving from second to first year), and resettled (with R and in the first year of the program).

>We began by testing the statistical assumptions associated with one-way ANOVA. Regarding the assumption of normality, skew and kurtosis values at each of the levels of program year fell well below the thresholds that Kline (2016a) identified as concerning (i.e., below |3| for skew and |10| for kurtosis). In contrast, results of a model-based Shapiro-Wilk test of normality, indicated that the model residuals differed from a normal distribution $(W = 0.910, p < .001)$. Although 4 outliers were identified none were extreme, thus we retained all cases. Finally, Levene’s homogeneity of variance test indicated no violation of the homogeneity of variance assumption $F(2, 109) = 2.094, p = 0.128$.

>Results of the omnibus ANOVA indicated a non-significant effect of stage on students assessments of traditional pedagogy, $F(2, 109) = 2.61, p = 0.078, \eta^2 = 0.046$. The effect size was small-to-medium.  We followed up the non-significant omnibus with all possible pairwise comparisons. We controlled for Type I error with the traditional Bonferroni adjustment. Curiously, results suggested that there were statistically significant differences between the transition and resettled $(Mdiff=0.511, p = 0.009)$ stages, but not between stable and transition $(Mdiff=0.374,p = 0.181)$ or transition and resettled $(Mdiff=−.137,p = 1.000)$. Given that the doctoral programs are unlikely to transition back to SPSS or into the second year, the instructor(s) are advised to consider ways that could result in greater student satisfaction. Means and standard deviations are presented in Table 1 and complete ANOVA results are presented in Table 2. Figure 1 provides an illustration of the results.

```{r}
apaTables::apa.1way.table(iv = Stage, dv = TradPed, show.conf.interval = TRUE,
    data = OneWay_df, table.number = 1, filename = "1wayHWTable.doc")
```

```{r}
omnibus1wHW_b <- aov(TradPed ~ Stage, data = OneWay_df)
apaTables::apa.aov.table(omnibus1wHW_b, table.number = 2, filename = "1wayHWTable2.doc")
```

```{r}
phoc <- phoc %>%
    rstatix::add_xy_position(x = "Stage")

ggpubr::ggboxplot(OneWay_df, x = "Stage", y = "TradPed", add = "jitter",
    color = "Stage", title = "Figure 1. Evaluations of Traditional Pedagogy as a Result of Transition") +
    ggpubr::stat_pvalue_manual(phoc, label = "p.adj.signif", tip.length = 0.02,
        hide.ns = TRUE, y.position = c(5.5))
```

#### Conduct power analyses to determine the power of the current study and a recommended sample size.

The *pwr.anova.test()* has five parameters:

* *k* = # groups
* *n* = sample size per gropu
* *f* = effect sizes, where 0.1/small, 0.25/medium, and 0.4/large
  - In the absence from an estimate from our own data, we make a guess about the expected effect size value based on our knowledge of the literature
* *sig.level* = *p* value that you will use
* *power* = .80 is the standard value

In the script below, we simply add our values. So long as we have four values, the fifth will be calculated for us.

Because this calculator requires the effect size in the metric of Cohen's *f* (this is not the same as the *F* ratio), we need to convert it. The *effectsize* package has a series of converters. We can use the *eta2_to_f()* function. 

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
effectsize::eta2_to_f(.046) 
```
We simply plug this value into the "f =".

First let's ask what our level of power was?  Our goal would be 80%.

Given that our design was unbalanced (21, 44, 47 across the three stages), I used 38 (114/3).

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.anova.test (k = 3, f = .219586, sig.level = .05, n = 38)
```
Our power was 0.53. That is, we had 53% chance to find a statistically significant result if one existed. In the next power analysis, let's see what sample size is recommended.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.anova.test (k = 3, f = .219586, sig.level = .05, power = .80)
```
In order to be at 80% power to find a statistically significant result if there is one, we would need 68 people per group. We currently had an unbalanced design of 21, 44, and 47.

### Hand Calculations

Before we continue: 

>You may notice that the results from the hand calculation are slightly different from the results I will obtain with the R packages. This is because the formula we have used for the hand-calculations utilizes an  approach to calculating the sums of squares that presumes that we have a balanced design (i.e., that the cell sizes are equal). When cell sizes are unequal (i.e., an unbalanced design) the Type II package in *rstatix::anova_test* will produce different result.

> Should we be concerned? No (and yes). My purpose in teaching hand calculations is for creating a conceptual overview of what is occurring in ANOVA models. If this lesson was a deeper exploration into the inner workings of ANOVA, we would take more time to understand what is occurring. My goal is to provide you with enough of an introduction to ANOVA that you would be able to explore further which sums of squares type would be most appropriate for your unique ANOVA model.

I will use the same example (and same dataset) for hand calculations. Because of the unbalanced design (e.g., unequal cell sizes across stages), my hand calculations will likely be different from the results from the *rstatix::anova_test()* function.

#### Using traditional NHST (null hypothesis testing language), state your null and alternative hypotheses.

Regarding the evaluation of traditional pedgagoy across three stages of transitions to a doctoral ANOVA course, the null hypothesis predicts no differences between the three levels of the dependent variable:

$$H_{O}: \mu _{1} = \mu _{2} = \mu _{3}$$

In contrast, the alternative hypothesis suggests there will be differences. Apriorily, I did not make any specific predictions.

$$H_{a1}: \mu _{1} \neq \mu _{2} \neq \mu _{3}$$

#### Calculate sums of squares total (SST). Steps in this calculation must include calculating a grand mean and creating variables representing the mean deviation and mean deviation squared.

I will use this approach to calculating sums of squares total:

$$SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}$$

I will use the *psych::describe()* function to obtain the overall mean:

```{r}
psych::describe(OneWay_df)
```

Next, I will subtract this value from each person's TradPed value. This will create a mean deviation.

```{r}
str(OneWay_df$TradPed)
```


```{r}
OneWay_df$mdevTP <- OneWay_df$TradPed - 4.06
#I could also calculate it by using the "mean" function
#I had to include an na.rm=TRUE; this appears to be connected to missingness
OneWay_df$mdevTPb <- OneWay_df$TradPed - mean(OneWay_df$TradPed, na.rm=TRUE)
head(OneWay_df)
```

```{r}
library(tidyverse)
OneWay_df <- OneWay_df %>% 
  dplyr::mutate(m_devSQTP = mdevTP^2)

#so we can see this in the textbook
head(OneWay_df)
```

I will ask for a sum of the mean deviation squared column. The function was not running, sometimes this occurs when there is missing data. While I didn't think that was true, adding "na.rm = TRUE" solved the problem.
```{r}
SST <- sum(OneWay_df$m_devSQTP, na.rm = TRUE)
SST
```
SST = 83.0332

#### Calculate the sums of squares for the model (SSM). A necessary step in this equation is to calculate group means. 

The formula for SSM is $$SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$$

We will need:

* *n* for each group,
* Grand mean (earlier we learned it was 4.06),
* Group means

We can obtain the group means several ways. I think the *psych::describeBy()* function is one of the easiest.

```{r}
psych::describeBy(TradPed ~ Stage, mat = TRUE, digits = 3, data = OneWay_df, type = 1)
```

Now we can pop these values into the formula.

```{r}
SSM <- 21 * (4.419 -4.06)^2 + 44 * (4.045 - 4.06)^2 + 30 * (3.909 - 4.06)^2
SSM
```
SSM = 3.400


#### Calculate the sums of squares residual (SSR). A necessary step in this equation is to calculate the variance for each group. 

The formula for I will use to calculate SSR is $$SS_{R}= s_{group1}^{2}(n-1) + s_{group2}^{2}(n-1) + s_{group3}^{2}(n-1))$$

We will need:

* *n* for each group,
* variance (standard deviation, squared) for each group

We can obtain these values from the previous run of the *psych::describeBy()* function.

```{r}
SSR <- (0.544^2)*(21 - 1) + (1.029^2)*(44 - 1) + (0.778^2)*(47-1)
SSR
```
SSR = 79.29


#### Calculate the mean square model, mean square residual, and *F*-test. 

The formula for mean square model is $$MS_M = \frac{SS_{M}}{df{_{M}}}$$

* $SS_M$ was 3.400
* $df_M$ is *k* - 1 (where *k* is number of groups/levels)

```{r}
MSM <- 3.400/2
MSM
```

The formula for mean square residual is $$MS_R = \frac{SS_{R}}{df{_{R}}}$$

* $SS_R$ was 79.292
* $df_R$ is $N - k$ (114 - 3 = 111)

```{r}
MSR = 79.292/111
MSR
```

The formula for the *F* ratio is $$F = \frac{MS_{M}}{MS_{R}}$$

```{r}
F <- 1.7/0.714
F
```
*F* = 2.381

As I noted before we started the hand calculations, this "isn't exactly" what we found for the same data using R and R packages. However, the algorithms for those packages would take into consideration the unbalanced design (i.e., unequal cell sizes). Such a characteristic is a limitation, but is beyond this lesson.

#### What are the degrees of freedom for your numerator and denominator? 

Numerator or $df_M$:  2
Denominator or $df_R$:  111

#### Locate the test critical value for your one-way ANOVA.  

We could use use a [table of critical values](https://www.statology.org/how-to-read-the-f-distribution-table/) for the *F* distribution.

The closest *N* in the table I am using is 120. If we set alpha at 0.05, our test value would need to exceeed the absolute value of 3.0718.

We can also use a look-up function, which follows this general form: qf(p, df1, df2. lower.tail=FALSE)
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
qf(.05, 2, 111, lower.tail=FALSE)
```
Not surprisingly the values are quite similar.

#### Is the *F*-test statistically significant? Why or why not? 

Because the value of the *F* test (2.381) did not exceed the absolute value of the critical value (3.078), the *F* test is not statistically significant.

#### Calculate and interpret the $\eta^2$ effect size

The formula to calculate the effect size is $$\eta ^{2}=\frac{SS_{M}}{SS_{T}}$$

* $SS_M$ was 3.400
* $SS_R$ was 79.292

```{r}
etaSQ <- 3.400/79.292
etaSQ
```
Eta square is 0.043. Values of .01, .06, and .14 are interpreted as small, medium, and large. Our value of 0.043 is small-to-medium.


#### Assemble the results into a statistical string. 

$$F(2, 111) = 2.381, p > .05, \eta^2 = 0.43$$