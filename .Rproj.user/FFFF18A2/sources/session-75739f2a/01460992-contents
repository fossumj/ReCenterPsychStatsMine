---
title: "Repeated Measures (within subjects) ANOVA"
author: "lhbikos"
date: '2022-10-24'
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen = 999)
```

# Repeated Measures ANOVA

The repeated measures ANOVA is useful for experiments (or arguably closely related designs). It requires:

* one factor, with two or more levels, that is categorical in nature
* participants must have DV scores at each level
* a continuous DV

I want to ask the question, do course evaluation ratings for the traditional pedagogy dimension differ for students across the ANOVA, Multivariate, and Psychometrics courses (in that order, because that's the order in which the students take the class.)


## Properly format data/variables

The BIGdf is from a project that evaluated three changes to our own stats courses, over time. As a whole, this dataset violates a ton of assumptions of ANOVA, but we can create a tiny df and use it for demonstrations.

This data is part of an IRB-approved study, with consent to use in teaching demonstrations and to be made available to the general public via the open science framework. Hence, it is appropriate to share in class.  You will notice there are student- and teacher- IDs. These numbers are not connected to the SPU student ID. Rather, the subcontractor who does the course evals for SPU creates a third, not-SPU-related identifier.

```{r}
big <- readRDS("TEPPout.rds")
```

This gets a little convoluted, here's why:
* We need "long" and "wide" forms of the data, AND
* In repeated measures ANOVA every case has to have non-missing data (i.e., to be included, a student must have contributed course evals to all three courses)

The way the data comes to us is the "long" form -- where each student's course evaluation has its own row (i.e., each student has up to 3 rows of data).

Let's trim it to just the variables of interest
```{r}
rm1w_df <- (dplyr::select (big, StndtID, Course, TradPed))
```

* Grouping variables: factors
* Dependent variable: numerical or integer

```{r}
str(rm1w_df)
```

```{r}
rm1w_df$Course <- factor(rm1w_df$Course, levels = c("ANOVA", "Multivariate", "Psychometrics"))
str(rm1w_df)
```

We will also need to create the "wide" form -- where each student has one row of data with all three TradPed scores (ANOVA, Multivariate, Psychometrics) in one row.

```{r}
rm1wWIDE_df <- reshape2::dcast(data = rm1w_df, formula = StndtID ~ Course,
    value.var = "TradPed")
str(rm1wWIDE_df)
```

Let's update the df to have only complete cases.

```{r}
rm1wWIDE_df <- na.omit(rm1wWIDE_df)
```
Ooof!  It took us to 70 cases.

Regarding missingness, I need the LONG file to have the same data. So let's update that.

```{r}
rm1wLONG_df <- (data.table::melt(rm1wWIDE_df, id.vars = c("StndtID"), measure.vars = c("ANOVA", "Multivariate", "Psychometrics")))
#This process  does not preserve the variable names, so we need to rename them
rm1wLONG_df<-  dplyr::rename(rm1wLONG_df, Course = "variable", TradPed = "value")
str(rm1wLONG_df)
```

Before we even start, let's get a plot of what's happening:

```{r}
bxp <- ggpubr::ggboxplot(rm1wLONG_df, x = "Course", y = "TradPed", add = "point",
    xlab = "Statistics Course", ylab = "Traditional Pedagogy Course Eval Ratings")
bxp
```


## Evaluate statistical assumptions

### Are there outliers?

```{r}
library(tidyverse)#need to open so I can use the pipes
rm1wLONG_df %>%
  group_by(StndtID)%>%
  rstatix::identify_outliers(TradPed)
```

No outliers are detected.

### DV is normally distributed for each level of the within-subjects factor (skew, kurtosis, Shapiro Wilks -- want a non-significant p value); 

```{r}
psych::describeBy(TradPed ~ Course, mat = TRUE, data = rm1wLONG_df, digits = 3)
```
Skew for each level of the IV is < 3.0.
Kurtosis for each level of the IV < 8.0

The Shapiro-Wilk examines residuals from the ANOVA model. We can quickly/preliminarily run the two-way ANOVA and apply the Shapiro-Wilk to the residuals:

```{r}
rm1wLONG_df %>%
  group_by(Course) %>%
  rstatix::shapiro_test(TradPed)
```
Oooof!  We violate normality at each level of the Course factor. That is $p < 0.001$ for evaluation of TradPed in ANOVA, Multivariate, and Psychometrics!

ANOVA $W = 0.89, p < 0.001$ 
Multivariate $W = .83, p < 0.001$
Psychometrics $W = .78, p < 0.001$
```{r}
ggpubr::ggqqplot(rm1wLONG_df, "TradPed", facet.by = "Course")
```

The dots should align along the trendline. You see how the negative skew makes them top out at 5 (which is the upper limit of the scale).

**APA Assumption Write-Up So Far**

Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the identify_outliers() function in the rstatix package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (for each of the courses) fell within cautionary ranges for skew and kurtosis (Kline, 2016). In contrast, the Shapiro-Wilks tests applied at each level of the course variable were statistically significant(ANOVA, $W = 0.89, p < 0.001$;  Multivariate, $W = .83, p < 0.001$; Psychometrics $W = .78, p < 0.001$). Although these values indicate a violation of the normality assumption, repeated measures ANOVA is relatively robust to such when cell sizes are large and reasonably balanced (Green & Salkind, 2016).


### Sphericity:  population variances of difference scores b/t any two levels are the same (Mauchley's, want a non-significant p-value)

Although I have this as a placeholder in the .rmd file, the sphericity assumption is automatically checked with Mauchley's test during the computation of ANOVA. Even better, it applies a corresponding correction.


## Conduct omnibus ANOVA (with effect size)

```{r}
RM_AOV <- rstatix::anova_test(data = rm1wLONG_df, dv = TradPed, wid = StndtID, within = Course)
RM_AOV
```

Mauchly's test indicated no violation of the sphericity assumption $(W = 0.992, p = 0.762)$.

Results of the omnibus ANOVA indicated a non-significant effect of course on the evaluations of Traditional Pedagogy $(F[2, 138] = 2.042, p = 0.134, \eta^2 = 0.009)$


## Conduct all possible pairwise comparisons (like in the lecture)

If I were doing this "for real", I would stop because there was a non-significant omnibus. This means that there should not be any differences in the follow-up pairwise comparisons. For the sake of working a complete problem, I will continue (you should, too....but like me, acknowledge that you normally would not).

```{r}
pwc <- rm1wLONG_df %>%
    rstatix::pairwise_t_test(TradPed ~ Course, paired = TRUE, p.adjust.method = "none")
pwc
```
 
Consistent with the non-significant omnibus, there are NS results. Check out, however, the "hovering around significance" between the ANOVA and Psychometrics classes.
 
ANOVA versus Multivariate, $t(69) = -1.37, p = 0.176$
ANOVA versus Psychometrics, $t(69) = -2.019, p = 0.047$
Multivariate versus Psychometrics, $t(69) = -.536, p = 0.594$

## APA Style Results with table(s) and figure(s)

Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the identify_outliers() function in the rstatix package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (for each of the courses) fell within cautionary ranges for skew and kurtosis (Kline, 2016). In contrast, the Shapiro-Wilks tests applied at each level of the course variable were statistically significant(ANOVA, $W = 0.89, p < 0.001$;  Multivariate, $W = .83, p < 0.001$; Psychometrics $W = .78, p < 0.001$). Although these values indicate a violation of the normality assumption, repeated measures ANOVA is relatively robust to such when cell sizes are large and reasonably balanced (Green & Salkind, 2016). Mauchly's test indicated no violation of the sphericity assumption $(W = 0.992, p = 0.762)$.

Results of the omnibus ANOVA indicated a non-significant effect of course on the evaluations of Traditional Pedagogy $(F[2, 138] = 2.042, p = 0.134, \eta^2 = 0.009)$. Although  would have normally stopped the ANOVA here, because this is a demonstration for class, I followed up with all pairwise comparisons. Curiously, and in spite of a non-significant omnibus test, there was one statistically significant difference among the pairs. When ANOVA and Psychometrics were compared, ratings for traditional pedagogy was lower in the ANOVA class, $t(69) = -2.019, p = 0.047$. There were no differences between ANOVA and Multivariate ($t[69] = -1.37, p = 0.176$) or Multivariate and Psychometrics ($t[69] = -.536, p = 0.594$)

Because there were only three pairwise comparisons subsequent to the omnibus test, alpha was retained at .05 (Green & Salkind, 2014b). Descriptive statistics are reported in Table 1 and the differences are illustrated in Figure 1.

*I would use the MASS package to make a table of pairwise comparisons.*

```{r}
pwc <- pwc %>%
    rstatix::add_xy_position(x = "Course")
bxp + ggpubr::stat_pvalue_manual(pwc) + labs(subtitle = rstatix::get_test_label(RM_AOV,
    detailed = TRUE), caption = rstatix::get_pwc_label(pwc))
```
## Power Analysis

In the WebPower package, we specify 6 of 7 interrelated elements; the package computes the missing one.

    n = sample size (number of individuals in the whole study)
    ng = number of groups
    nm = number of measurements/conditions/waves
    f = Cohen’s f (an effect size; we can use a conversion calculator)
    nscor = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package’s default; < 1 means some correction was applied.
    alpha = is the probability of Type I error; we traditionally set this at .05
    power = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
    type = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect.

I used effectsize packages converter to transform our η2

to Cohen’s f.


In the WebPower package, we specify 6 of 7 interrelated elements; the package computes the missing one.

    n = sample size (number of individuals in the whole study)
    ng = number of groups
    nm = number of measurements/conditions/waves
    f = Cohen’s f (an effect size; we can use a conversion calculator)
    nscor = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package’s default; < 1 means some correction was applied.
    alpha = is the probability of Type I error; we traditionally set this at .05
    power = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
In the WebPower package, we specify 6 of 7 interrelated elements; the package computes the missing one.

    n = sample size (number of individuals in the whole study)
    ng = number of groups
    nm = number of measurements/conditions/waves
    f = Cohen’s f (an effect size; we can use a conversion calculator)
    nscor = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package’s default; < 1 means some correction was applied.
    alpha = is the probability of Type I error; we traditionally set this at .05
    power = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
    type = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect.

I used effectsize packages converter to transform our η2

to Cohen’s f.


I used effectsize packages converter to transform our η2 to Cohen’s f.

```{r}
effectsize::eta2_to_f(.009) 

WebPower::wp.rmanova(n = NULL, ng = 1, nm = 3, f = 0.095, nscor = 0.992,
    alpha = 0.05, power = 0.8, type = 1)
```

We would need 1075 to detect statistically significant differences.
