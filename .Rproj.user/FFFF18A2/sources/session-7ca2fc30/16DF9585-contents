```{r include = FALSE}
options(scipen=999)
```

## Homeworked Example

[Screencast Link](https://youtu.be/qtA-tkDma3Q)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introduction](ReCintro).

The one-sample test comes in handy when you want to compare your dataset to an external benchmark or standard. It can be a real helper in program evaluation

### Working the Problem with R and R Packages

#### Narrate the research vignette, describing the variables and their role in the analysis {-}

From my course evaluation data, I want to ask the question, "Are ratings for the Overall Instructor for the ANOVA course evals statistically significantly different from the overall departmental averages for that same item?"  In CPY the overall average for that specific item is 4.4. 

*If you wanted to use this example and dataset as a basis for a homework assignment, you could select a different course (i.e., Multivariate or Psychometrics) and/or compare the mean for the ORG department ($M = 4.1$).*

#### Simulate (or import) and format data {-}

First I will open the entire dataset.
```{r}
ReCdf <- readRDS("ReC.rds")
```

Let's first trim it to just students who took ANOVA
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
JustANOVA <- subset(ReCdf, Course == "ANOVA") 
```

And further trim to our variable of interest
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
tiny1 <- JustANOVA %>%
    dplyr::select (OvInstructor)
```

And further trim to non-missing data
```{r}
tiny1 <- na.omit(tiny1)
```

* Is the sample variable on a continuous scale of measurement and formatted as *num* or *int* in R?
* Is the external score evaluated on the same continuous scale?
```{r}
str(tiny1$OvInstructor)
```
Yes. The format for the OvInstructor variable is integer (which is numerical); the overall course evaluation is on an equivalent (1 to 5) scale.


#### Evaluate statistical assumptions {-}

* Are the skew and kurtosis values within the range expected?
* Does the distribution of the variable differ significantly from a normal distribution?

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pastecs::stat.desc(tiny1$OvInstructor, norm=TRUE)
```
The skew value is -.984 is well below the absolute value of 3. The skew.2SE is -2.164 (larger than the absolute value of 2.0) is a bit discrepant and suggests a negative skew. Thus, we want to remain open to the possibility of skew.

The kurtosis value is -0.074 and is below the absolute value of 10. The kurt.2SE value is -0.082 which is below the absolute value of 2.0. The data does not appear to be kurtotic.

The Shapiro Wilk test value is 0.7728 (*p* < 0.001). This significant value suggests a distribution that is not normally distributed.


#### Conduct a one sample *t* test (with an effect size) {-}

We will compare the overall instructor from the data to the CPY average of 4.4.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rstatix::t_test(tiny1, OvInstructor ~ 1, mu = 4.4, detailed = TRUE)
```
We can begin to create our *t* string:

$t(112) = -2.246, p = 0.027, CI95(3.997, 4.374)$  

Let's interpret the results.  With 112 degrees of freedom, our *t* value is -2.245. Because the *p* value is less than .05, this is statistically significant. This means that my course evaluations in ANOVA were statistically significantly lower than the average for CPY.  We are 95% confident that the true course evaluation mean (for my courses) fell between 3.997 and 4.374.

Let's calculate the effect size. We will use a Cohen's *d* which is interpreted in standard deviation units. 
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rstatix::cohens_d(tiny1, OvInstructor ~ 1, ref.group = NULL, mu = 4.4)
```
Cohen's *d* was 0.211. This is a small effect. We can add it to the *t* string. 

$t(112) = -2.246, p = 0.027, CI95(3.997, 4.374), d = -0.211$


#### APA style results with table(s) and figure {-}

* t-test results should include t, df, p, d-or-eta, and CI95%
* Table
* Figure
* Grammar/style

>A one-sample *t*-test was used to evaluate whether the *overall instructor* course evaluation ratings from the ANOVA courses were statistically significant different from the departmental averages for the Clinical (CPY; *M* = 4.4) department.  The sample mean for the ANOVA course evaluations was 4.186 (*SD* = 1.013). Although this mean was statistically significantly different from the average CPY course evaluation ratings of the same item, $t(112) = -2.246, p = 0.027, CI95(3.997, 4.374)$, the effect size was quite small $(d = -0.211)$. A distribution of the ANOVA course ratings is found in Figure 1.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggpubr::ggboxplot(tiny1$OvInstructor, ylab = "Course Evaluation Ratings", xlab = FALSE, add = "jitter", title = "Figure 1. Overall Instructor Ratings for ANOVA")
```

#### Conduct power analyses to determine the power of the current study and a recommended sample size {-}

A quick reminder that the *d* in the power analysis is the difference between the means divided by the pooled standard deviation. This is the same as Cohen's d that we just calculated.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d = -0.211	, n = 113, power = NULL, sig.level = 0.05, type = "one.sample", alternative = "two.sided")
```

For the comparison to the CPY departmental average, power was 60%. That is, given the value of the mean difference relative to the pooled standard deviation we had a 60% chance of detecting a statistically significant effect if there was one.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d = -0.211, n = NULL, power = 0.8, sig.level = 0.05, type = "one.sample", alternative = "two.sided")
```
For the CPY departmental comparison, the recommended sample size would be 178. This means there would need to be 178 individuals to find a statistically significant difference, if one existed (at a power of 80%).

### Hand Calculations

#### Using traditional NHST (null hypothesis testing language), state your null and alternative hypotheses {-}

$$
\begin{array}{ll}
H_0: & \mu = 4.4 \\
H_A: & \mu \neq 4.4
\end{array}
$$

#### Calculate the mean of your sample; identify the mean of your benchmarking sample {-}

I will continue with the *tiny1* dataset and calculate the mean of the OvInstructor variable from my ANOVA course evaluations.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
mean(tiny1$OvInstructor, na.rm=TRUE)
```

The mean of my benchmarking sample is 4.4. This number is a "departmental standard" and did not need to be calculated by me for this purpose.

#### Using the steps from the previous lesson, hand-calculate the standard deviation of your sample. This should involve variables representing the mean, mean deviation, and mean deviation squared {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#first the mean
tiny1$M_OvInst <- mean(tiny1$OvInstructor, na.rm=TRUE)
#second the mean deviation
tiny1$Mdev_OvInst <- (tiny1$OvInstructor-tiny1$M_OvInst)
#third the mean deviation squared
tiny1$mdev2_OvInst <- (tiny1$Mdev_OvInst  * tiny1$Mdev_OvInst)
#fourth the variance
var_OvInst <- sum(tiny1$mdev2_OvInst /((nrow(tiny1) - 1)))
var_OvInst
#finally the standard deviation
sd_OvInst <- sqrt(var_OvInst)
sd_OvInst

head(tiny1)
```
The variance is 1.028; the standard deviation is 1.014.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
sd(tiny1$OvInstructor)#checking my work
```

#### Calculate the one-sample *t*-test {-}

Here's the formula:  

$$
t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{N} }
$$

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
(4.185841 - 4.4)/(1.013733/sqrt(113))
```

#### Identify the degrees of freedom associated with your *t*-test {-}     

For the one-sample *t*-test, $df = N - 1$. In our case

```{r}
113 - 1
```

#### Locate the test critical value for your test {-}
We can use a table of critical values for the one sample *t*-test:  https://www.statology.org/t-distribution-table/

A 2-tail test, when p - .05, with ~120 individuals is 1.98

Or, this code:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
qt(p = 0.05/2, df = 112, lower.tail = FALSE)
```
#### Is the *t*-test statistically significant? Why or why not? {-}

Yes *t* = -2.245701 exceeds the (absolute) test critical value of 1.98.

#### What is the confidence interval around your sample mean? {-}

Here is a reminder of the formula:

$$\bar{X} \pm t_{cv}(\frac{s}{\sqrt{n}})$$

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
(4.185841) - ((1.98118)*(1.013733/sqrt(113)))
(4.185841) + ((1.98118)*(1.013733/sqrt(113)))
```

We are 95% confident that the sample mean for the students in the ANOVA classes is between 3.997, 4.375.

#### Calculate the effect size (i.e., Cohen's *d* associated with your *t*-test {-}

A reminder of the two formula:

$$d = \frac{Mean Difference}{SD}=\frac{t}{\sqrt{N}}$$
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#First formula
(4.185841 - 4.4)/1.013733
#Second formula
-2.245701/sqrt(113)
```
