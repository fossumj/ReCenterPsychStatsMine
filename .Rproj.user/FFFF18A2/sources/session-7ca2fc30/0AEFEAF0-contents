```{r include=FALSE}
options(scipen=999)#eliminates scientific notation
```

## Homeworked Example

[Screencast Link](https://youtu.be/3UhTJXp8uNI)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introduction](ReCintro).

### Working the Problem with R and R Packages

#### Narrate the research vignette, describing the IV and DV. The data you analyze should have at least 3 levels in the independent variable; at least one of the attempted problems should have a significant omnibus test so that follow-up is required) {-}

I want to ask the question, do course evaluation ratings for the traditional pedagogy dimension differ for students across the ANOVA, multivariate, and psychometrics courses (in that order, because that's the order in which the students take the class.)

The dependent variable is the evaluation of traditional pedagogy. The independent variable is course/time (i.e., each student offers course evaluations in each of the three classes).

*If you wanted to use this example and dataset as a basis for a homework assignment, the three different classes are the only repeated measures variable.  Rather, you could choose a different dependent variable. I chose the traditional pedagogy subscale. Two other subscales include socially responsive pedagogy and valued by the student.*

#### Check and, if needed, format data {-}

```{r}
big <- readRDS("ReC.rds")
```

The TradPed (traditional pedagogy) variable is an average of the items on that scale. I will first create that variable.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#Creates a list of the variables that belong to that scale
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')

#Calculates a mean if at least 75% of the items are non-missing; adjusts the calculating when there is missingness
big$TradPed <- sjstats::mean_n(big[, TradPed_vars], .75)
#If the above code doesn't work use the one below; the difference is the two periods in front of the variable list
#big$TradPed <- sjstats::mean_n(big[, ..TradPed_vars], .75)
```

Let's trim to just the variables we need.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rm1wLONG_df <- (dplyr::select (big, deID, Course, TradPed))
head(rm1wLONG_df)
```

* Grouping variables: factors
* Dependent variable: numerical or integer

```{r}
str(rm1wLONG_df)
```

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rm1wLONG_df$Course <- factor(rm1wLONG_df$Course, levels = c("ANOVA", "Multivariate", "Psychometrics"))
str(rm1wLONG_df)
```
Let's update the df to have only complete cases.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rm1wLONG_df <- na.omit(rm1wLONG_df)
nrow(rm1wLONG_df)#counts number of rows (cases)
```
This took us to 307 cases.

These analyses require that students have completed evaluations for all three courses. In the lesson, I restructured the data from long, to wide, back to long again. While this was useful pedagogy in understanding the difference between the two data structures, there is also super quick code that will simply retain data that has at least three observations per student.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
rm1wLONG_df <- rm1wLONG_df%>%
  dplyr::group_by(deID)%>%
  dplyr::filter(n()==3)
```

This took the data to 210 observations. Since each student contributed 3 observations, we know  $N = 70$.

```{r}
210/3
```
Before we start, let's get a plot of what's happening:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
bxp <- ggpubr::ggboxplot(rm1wLONG_df, x = "Course", y = "TradPed", add = "point", color = "Course",
    xlab = "Statistics Course", ylab = "Traditional Pedagogy Course Eval Ratings", title = "Course Evaluations across Doctoral Statistics Courses")
bxp
```

#### Evaluate statistical assumptions {-}

**Is the dependent variable normally distributed?**

Given that this is a one-way repeated measures ANOVA model, the dependent variable must be normally distributed within each of the cells of the factor.

We can examine skew and kurtosis in each of the levels of the TradPed variable with *psych::describeBy()*.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#R wasn't recognizing the data, so I quickly applied the as.data.frame function
rm1wLONG_df <- as.data.frame(rm1wLONG_df)
psych::describeBy(TradPed ~ Course, mat = TRUE, type = 1, data = rm1wLONG_df)
```

Although we note some skew and kurtosis, particularly for the multivariate class, none exceed the critical thresholds of |3| for skew and |10| identified by Kline [-@kline_data_2016].

I can formally test for normality with the Shapiro-Wilk test. If I use the residuals from an evaluated model, with one test, I can determine if TradPed is distributed normally within each of the courses.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
#running the model
RMres_TradPed <- lm(TradPed ~ Course, data = rm1wLONG_df)
summary(RMres_TradPed)
```
We will ignore this for now, but use the residuals in the formal Shapiro-Wilk test.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rstatix::shapiro_test(residuals(RMres_TradPed))
```
The distribution of model residuals is statistically significantly different than a normal distribution $(W = 0.876, p < .001)$. Although we have violated the assumption of normality, ANOVA models are relatively robust to such a violation when cell sizes are roughly equal and greater than 15 each [@green_one-way_2017-1].

Creating a QQ plot can let us know how badly the distribution departs from a normal one.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
ggpubr::ggqqplot(residuals(RMres_TradPed))
```

We can identify outliers and see if they are reasonable or should be removed.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
rm1wLONG_df %>%
  group_by(Course)%>%
  rstatix::identify_outliers(TradPed)
```
Outliers for the TradPed variable are among the lowest evaluations (these can be seen on the boxplot). Although there are six outliers identified, none are extreme. Although they contribute to non-normality, I think it's important that this sentiment be retained in the dataset.

**Assumption of sphericity**

We will need to evaluate and include information about violations related to sphericity. Because these are calculated at the same time as the ANOVA, itself, I will simply leave this here as a placeholder.

Here's how I would write up the evaluation of assumptions so far:

>We utilized one-way repeated measures ANOVA to determine if there were differences in students' evaluation of traditional pedagogy across three courses -- ANOVA, multivariate, and psychometrics -- taught in that order.

>Repeated measures ANOVA has several assumptions regarding normality, outliers, and sphericity. Although we note some skew and kurtosis, particularly for the multivariate class, none exceed the critical thresholds of |3| for skew and |10| identified by Kline [-@kline_data_2016]. We formally evaluated the normality assumption with the Shapiro-Wilk test. The distribution of model residuals was statistically significantly different than a normal distribution $(W = 0.876, p < .001)$. Although we violated the assumption of normality, ANOVA models are relatively robust to such a violation when cell sizes are roughly equal and greater than 15 each [@green_one-way_2017-1]. Although our data included six outliers, none were classified as extreme. Because they represented lower course evaluations, we believed it important to retain them in the dataset. PLACEHOLDER FOR SPHERICITY.

#### Conduct omnibus ANOVA (w effect size) {-}

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rmAOV <- rstatix::anova_test(data = rm1wLONG_df, dv = TradPed, wid = deID, within = Course)
rmAOV
```
Let's start first with the sphericity test. Mauchly's test for sphericity was statistically significant, $(W = 0.878, p = 0.012)$. Had we not violated the assumption, our *F* string would have been created from the data in the ANOVA section of the output : $F(2, 138) = 2.838, p = 0.062, \eta^2 = 0.14$ However, because we violated the assumption, we need to use the degrees-of-freedom adjusted output under the "Sphericity Corrections" section; $F(1.78, 122.98) = 2.838, p = 0.068, ges = 0.014$.

While the ANOVA is non-significant, because this is a homework demonstration, I will behave *as if* the test is significant and continue with the pairwise comparisons.

#### Conduct all possible pairwise comparisons (like in the lecture) {-}

I will follow up with a test of all possible pairwise comparisons and adjust with the bonferroni.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwc <- rstatix::pairwise_t_test(TradPed ~ Course, paired = TRUE, p.adjust.method = "bonf", data = rm1wLONG_df)
pwc
```

Consistent with the non-significant omnibus, there were non-significant differences between the pairs. This included, ANOVA and multivariate $(t[69] = -1.215, p = 0.687)$; ANOVA and psychometrics courses $(t[69] = -2.065, p = 0.128)$; and multivariate and psychometrics $(t[69] = -0.772, p = 1.000)$.

#### Describe approach for managing Type I error {-}

I used a traditional Bonferroni for the three, follow-up, pairwise comparisons. 

#### APA style results with figure {-}

>We utilized one-way repeated measures ANOVA to determine if there were differences in students' evaluation of traditional pedagogy across three courses -- ANOVA, multivariate, and psychometrics -- taught in that order.

>Repeated measures ANOVA has several assumptions regarding normality, outliers, and sphericity. Although we note some skew and kurtosis, particularly for the multivariate class, none exceed the critical thresholds of |3| for skew and |10| identified by Kline [-@kline_data_2016]. We formally evaluated the normality assumption with the Shapiro-Wilk test. The distribution of model residuals was statistically significantly different than a normal distribution $(W = 0.876, p < .001)$. Although we violated the assumption of normality, ANOVA models are relatively robust to such a violation when cell sizes are roughly equal and greater than 15 each [@green_one-way_2017-1]. Although our data included six outliers, none were classified as extreme. Because they represented lower course evaluations, we believed it important to retain them in the dataset. Mauchly's test indicated a violation of the sphericity assumption $(W = 0.878, p = 0.012)$.

>Given the violation of the homogeneity of sphericity assumption, we are reporting the Greenhouse-Geyser adjusted values. Results of the omnibus ANOVA were not statistically significant $F(1.78, 122.98) = 2.838, p = 0.068, ges = 0.014$. 

>Although we would normally not follow-up a non-significant omnibus ANOVA with more testing, because this is a homework demonstration, we will follow-up the ANOVA with pairwise comparisons and manage Type I error with the traditional Bonferroni approach. Consistent with the non-significant omnibus, there were non-significant differences between the pairs. This included, ANOVA and multivariate $(t[69] = -1.215, p = 0.687)$; ANOVA and psychometrics courses $(t[69] = -2.065, p = 0.128)$; and multivariate and psychometrics $(t[69] = -0.772, p = 1.000)$.

I can update the figure to include star bars.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
library(tidyverse)
pwc <- pwc %>%
    rstatix::add_xy_position(x = "Course")
bxp <- bxp + ggpubr::stat_pvalue_manual(pwc, label = "p.adj.signif", tip.length = 0.01, hide.ns = FALSE, y.position = c(5.25, 5.5, 5.75)) 
bxp
```

#### Conduct power analyses to determine the power of the current study and a recommended sample size {-}

In the *WebPower* package, we specify 6 of 7 interrelated elements; the package computes the missing one.

* *n* = sample size (number of individuals in the whole study).
* *ng* = number of groups.
* *nm* = number of measurements/conditions/waves.
* *f* = Cohen's *f* (an effect size; we can use an effect size converter to obtain this value)
   - Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively.
* *nscor* = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package's default; < 1 means some correction was applied. 
* *alpha* = is the probability of Type I error; we traditionally set this at .05 
* *power* = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want).
* *type* = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect. 

I used *effectsize::eta2_to_f* packages convert our $\eta^2$ to Cohen's *f*.

```{r message=FALSE, warning=FALSE}
effectsize::eta2_to_f(.014) 
```

Retrieving the information about our study, we add it to all the arguments except the one we wish to calculate. For power analysis, we write "power = NULL."

```{r message=FALSE, warning=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=70)}
WebPower::wp.rmanova(n=70, ng=1, nm=3, f = .1192, nscor = .891, alpha = .05, power = NULL, type = 1)
```
The study had a power of 13%. That is, we had a 13% probability of finding a statistically significant result if one existed.

In reverse, setting *power* at .80 (the traditional value) and changing *n* to *NULL* yields a recommended sample size.   

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
WebPower::wp.rmanova(n=NULL, ng=1, nm=3, f = .1192, nscor = .891, alpha = .05, power = .80, type = 1)
```
With these new values, we learn that we would need 737 individuals in order to obtain a statistically significant result 80% of the time.

### Hand Calculations

For hand calculations, I will use the same dataframe (rm1wLONG_df) as I did for the calculations with R and R packages.Before we continue: 

>You may notice that the results from the hand calculation are slightly different from the results I will obtain with the R packages. This was true in the lesson as well. Hand calculations and those used in the R packages likely differ on how the sums of squares is calculated. While the results are "close-ish" they are not always identical. 

> Should we be concerned? No (and yes). My purpose in teaching hand calculations is for creating a conceptual overview of what is occurring in ANOVA models. If this lesson was a deeper exploration into the inner workings of ANOVA, we would take more time to understand what is occurring. My goal is to provide you with enough of an introduction to ANOVA that you would be able to explore further which sums of squares type would be most appropriate for your unique ANOVA model.

#### Calculate sums of squares total (SST) for the omnibus ANOVA. Steps in this calculation must include calculating a grand mean and creating variables representing the mean deviation and mean deviation squared {-}

The formula for sums of squares total: $$SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}$$.

We can use the mean function from base R to calculate the grand mean:

```{r}
mean(rm1wLONG_df$TradPed)
```
I will create a mean deviation variable by subtracting the mean from each score:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rm1wLONG_df$mDev <- rm1wLONG_df$TradPed - 4.319286
head(rm1wLONG_df)#shows first six rows of dataset
```

Now I will square the mean deviation:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
rm1wLONG_df$mDev2 <- rm1wLONG_df$mDev * rm1wLONG_df$mDev
head(rm1wLONG_df)#shows first six rows of dataset
```

Sums of squares total is the sum of the mean deviation squared scores.

```{r}
SST <- sum(rm1wLONG_df$mDev2)
SST
```
The sums of squares total is 103.9144.

#### Calculate the sums of squares within (SSW) for the omnibus ANOVA. A necessary step in this equation is to calculate the variance for each student {-} 

Here is the formula for sums of squares within:  

$$SS_W = s_{person1}^{2}(n_{1}-1)+s_{person2}^{2}(n_{2}-1)+s_{person3}^{2}(n_{3}-1)+...+s_{personk}^{2}(n_{k}-1)$$

I can get the use the *psych::describeBy()* to obtain the standard deviations for each student's three ratings. I can square each of those for the variance to enter into the formula.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describeBy(TradPed ~ deID, mat=TRUE, type = 1, data = rm1wLONG_df)
```

Someone who codes in R could probably write a quick formula to do this -- in this case, I will take the time (and space) to copy each student's standard deviation into a formula that squares it, multiplies it by $n-1$ and then sums all 70 of those calculations.
```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
SSW <- (0.8717798^2 * (3 - 1)) + (0.4163332	^2 * (3 - 1)) + (0.6928203	^2 * (3 - 1)) + (0.4000000	^2 * (3 - 1)) +   (0.4163332^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.5291503^2 * (3 - 1)) + (0.5291503^2 * (3 - 1)) +   (0.7023769^2 * (3 - 1)) + (0.4618802^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (1.2220202^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.9018500^2 * (3 - 1)) + (0.5033223^2 * (3 - 1)) + (0.0000000^2 * (3 - 1)) + (0.9451631^2 * (3 - 1)) + 
(1.0066446^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.5484828^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (1.6165808^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.9165151^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.6429101^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.5291503^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.2309401^2 * (3 - 1)) + 
(0.1154701^2 * (3 - 1)) + (0.9165151^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.5291503^2 * (3 - 1)) +
(0.5291503^2 * (3 - 1)) + (0.5291503^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) +
(0.2309401^2 * (3 - 1)) + (0.2309401^2 * (3 - 1)) + (0.0000000^2 * (3 - 1)) + (0.2000000^2 * (3 - 1)) + (0.1154701^2 * (3 - 1)) + (0.5033223^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) + (0.0000000^2 * (3 - 1)) +  (0.2309401^2 * (3 - 1)) + (0.4618802^2 * (3 - 1)) + (0.4163332^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) +   (0.4000000^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) + (0.8326664^2 * (3 - 1)) + (0.5773503^2 * (3 - 1)) +
(0.0000000^2 * (3 - 1)) + (0.2000000^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) + (0.3464102^2 * (3 - 1)) +
(0.4618802^2 * (3 - 1)) + (0.3055050^2 * (3 - 1)) 

SSW
```
Our sums of squares within is 36.895.

#### Calculate sums of squares model (SSM) for for the effect of time (or repeated measures) {-} 

The formula for the sums of squares model in repeated measures captures the effect of time (or the repeated measures nature of the design):  $$SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$$

Earlier we learned that the grand mean is 4.319286. 

I can obtain the means for each course with *psych::describeBy()*.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
psych::describeBy(TradPed ~ Course, mat = TRUE, digits = 3, type = 1, data = rm1wLONG_df)
```

I can put it in the formula:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
(70 * (4.211 - 4.319286)^2) + (70 * (4.332 - 4.319286)^2) + (70 * (4.414 - 4.319286)^2)
```
Sums of squares model is 1.4601


#### Calculate sums of squares residual (SSR) {-} 

In repeated measures ANOVA $SS_W = SS_M + SS_R$.  Knowing SSW (34.255) and SSM (1.460), we can do simple arithmetic to obtain SSR.

```{r}
SSR <- 36.895 - 1.460
SSR
```
Sums of squares residual is 35.435.


#### Calculate the sums of squares between (SSB) {-} 
In repeated measures ANOVA $SS_T = SS_W + SS_B$.  Knowing SST (103.9144) and SSW (34.255), we can do simple arithmetic to obtain SSB.

```{r}
SSB <- 103.9144 - 35.435
SSB
```
Sums of squares between is 68.4794.


#### Create a source table that includes the sums of squares, degrees of freedom, mean squares, *F* values, and *F* critical values {-}

|One Way Repeated Measures ANOVA Source Table
|:--------------------------------------------------------------|

|Source    |SS       |df                |$MS = \frac{SS}{df}$ |$F = \frac{MS_{source}}{MS_{resid}}$ |$F_{CV}$|
|:---------|:--------|:-----------------|:------|:------|:------|
|Within    |36.895   |(N-k) = 67        |       |       |       |
|Model     |1.4601   |(k-1) = 2         |0.7301 |1.3391 |3.138  |
|Residual  |35.435   |(dfw - dfm) = 65  |0.5452 |       |       |
|Between   |68.4794  |(N-1) = 69        |       |       |       |
|Total     |103.9144 |(cells-1) = 209   |       |       |       |

```{r}
#calculating degrees of freedom for the residual
67-2
```
Calculating mean square model and residual.
```{r}
1.4601/2#MSM
35.435/65#MSR
```
Calculating the F ratio
```{r}
.7301/.5452
```

Obtaining the F critical value: 

```{r}
qf(.05, 2, 65, lower.tail = FALSE)
```

We can see the same in an [F distribution table](https://www.statology.org/f-distribution-table/). 

#### Is the *F*-tests statistically significant? Why or why not? {-}  

No. The *F* value did not exceed the *F* critical value. To achieve statistical significance, my F value has to exceed 3.138.

#### Assemble the results into a statistical string {-} 

$F(2, 65) = 1.339, p > 0.05$

