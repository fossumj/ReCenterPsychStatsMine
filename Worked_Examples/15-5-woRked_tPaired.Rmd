```{r include = FALSE}
options(scipen=999)
```
## Homeworked Example

[Screencast Link](link)

For more information about the data used in this homeworked example, please refer to the description and codebook located at the end of the [introduction](ReCintro).

### Working the Problem with R and R Packages

#### Narrate the research vignette, describing the variables and their role in the analysis

I want to ask the question, "Do students' evaluations of traditional pedagogy (TradPed) change from ANOVA (the first course in the series) to Multivariate (the second course in the series)?." Unlike the independent samples *t*-test where we compared students in two different departments, we are comparing *the same* students across two different conditions. In this particular analysis, there is also an element of time. That is the ANOVA class always precedes the multivariate class (with a regression class, taught by a different instructor) in the intervening academic quarter.  

This research design has some clear limitations. Threats to internal validity are caused by issues like history and maturation. None-the-less, for the purpose of a statistical demonstration, this dataset works. 

*If you wanted to use this example and dataset as a basis for a homework assignment, you could compare a different combination of courses and/or score one of the other course evaluation subscales (e.g., socially responsive pedagogy or valued-by-me). *

Like most data, some manipulation is required before we can begin the analyses.

#### Simulate (or import) and format data

Let's import the larger dataset. 
```{r}
larger <- readRDS("ReC.rds")
```

The TradPed (traditional pedagogy) variable is an average of the items on that scale. I will first create that variable.

```{r}
#Creates a list of the variables that belong to that scale
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')

#Calculates a mean if at least 75% of the items are non-missing; adjusts the calculating when there is missingness
larger$TradPed <- sjstats::mean_n(larger[, ..TradPed_vars], .75)

```

From the "larger" data, let's select only the variable we will use in the analysis. I have included "long" in the filename because the structure of the dataset is that course evaluation by each student is in its own row. That is, each student could have up to three rows of data.

We need both "long" and "wide" forms to conduct the analyses required for both testing the statistical assumptions and performing the paired samples *t*-test.
```{r}
paired_long <-(dplyr::select (larger, deID, Course, TradPed))
```

From that reduced variable set, let's create a subset with students only from those two courses.
```{r}
paired_long <- subset(paired_long, Course == "ANOVA" | Course == "Multivariate") 
```

Regarding the structure of the data, we want the conditions (ANOVA, multivariate) to be factors and the TradPed variable to be continuously scaled. The format of the deID variable can be any numerical or categorical format -- just not a "chr" (character) variable.

```{r}
str(paired_long)
```
R correctly interpreted our variables.

For analyzing the assumptions associated with the paired-samples *t*-test, the format needs to be "wide" form (where each student has both observations on one row). Our data is presently in "long" form (where each observation is listed in each row). Here's how to reshape the data.

```{r}
paired_wide <- reshape2::dcast(data = paired_long, formula =deID ~ Course, value.var = "TradPed")
```

Let's recheck the structure.
```{r}
str(paired_wide)
```
You will notice that there is a good deal of missingness in the Multivariate condition. This is caused because the most recent cohort of students had not yet taken the course. While managing missingness is more complex than this, for the sake of simplicity, I will create a dataframe with non-missing data.

Doing so should also help with the hand-calculations later in the worked example.

```{r}
paired_wide <- na.omit(paired_wide)
```

#### Evaluate statistical assumptions

We need to evaluate the *distribution of the difference score* in terms of skew and kurtosis. We want this distribution of difference scores to be normally distributed.

This means we need to create a difference score:  

```{r}
paired_wide$DIFF <- paired_wide$ANOVA - paired_wide$Multivariate
```

We can use the *psych::describe()* function to obtain skew and kurtosis. 
```{r}
psych::describe(paired_wide)
```

Regarding the DIFF score, the skew (0.56) and kurtosis (3.15) values were well below the threshholds of concern identified by Klein (2016). 

We can formally test for deviations from normality with a Shapiro-Wilk. We want the results to be non-significant.

```{r}
rstatix::shapiro_test(paired_wide, DIFF)
```
Results of the Shapiro-Wilk test of normality are statistically significant $(W = 0.943, p = 0.002)$. This means that the distribution of difference scores are statistically significantly different from a normal distribution.

Although not required in the formal test of instructions, a *pairs panel* of correlations and distributions can be useful in undersatnding our data.

```{r}
psych::pairs.panels(paired_wide)
```
Visual inspection of the distributions of the specific course variables were negatively skewed, with values clustered at the high end of the course evaluation ratings. However, the distribution for the DIFF variable seems relatively normal (although maybe a bit leptokurtic). This is consistent with the statistically significant Shapiro-Wilk test.

Before moving forward, I want to capture my analysis of assumptions:

>We began by analyzing the data to see if it met the statistical assumptions for analysis with a paired samples t-test. Regarding the assumption of normality, the skew (0.56) and kurtosis (3.15) values associated with the difference between conditions (ANOVA and multivariate) were  below the threshholds of concern identified by Klein (2016). In contrast, results of the Shapiro-Wilk test of normality suggested that the distribution of difference scores was statistically significantly different than a normal distribution $(W = 0.943, p = 0.002)$. 

#### Conduct a paired samples t-test (with an effect size & 95% CIs)

So this may be a bit tricky, but our original "long" form of the data has more ANOVA evaluations (students who had taken ANOVA had not yet taken multivariate) than multivariate. The paired samples *t* test requires the design to be balanced.  When we used the *na.omit()* function with the wide case, we effectively balanced the design, eliminating students who lacked observations across both courses. Let's restructure that wide format back to long format so that the design will be balanced.

```{r}
paired_long2 <- data.table::melt(data.table::setDT(paired_wide), id.vars = c("deID"), measure.vars = list(c("ANOVA", "Multivariate")))

paired_long2 <- dplyr::rename(paired_long2, Course = variable, TradPed = value)

head(paired_long2)
```

```{r}
rstatix::t_test(paired_long2, TradPed ~ Course, paired = TRUE, detailed = TRUE)
```

I'll begin the *t* string with this output:  $t(76) = -1.341, p = 0.184, CI95(-0.305, 0.069)$. The difference in course evaluations is not statistically significantly difference. We are 955 confident that the true difference in means is as low as -0.301 or as high as 0.060.

We calculate the Cohen's *d* (the effect size) this way:

```{r}
rstatix::cohens_d(paired_long2, TradPed ~ Course, paired = TRUE)
```
The value of -0.153 is quite small. We can add this value to our statistical string:  $t(76) = -1.341, p = 0.184, CI95(-0.305, 0.069), d = -0.153$

#### APA style results with table(s) and figure

>A paired samples *t*-test was conducted to evaluate the hypohtesis that there would be statistically significant differences in students' course evaluations of ANOVA and multivariate statistics classses.

>We began by analyzing the data to see if it met the statistical assumptions for analysis with a paired samples t-test. Regarding the assumption of normality, the skew (0.56) and kurtosis (3.15) values associated with the difference between conditions (ANOVA and multivariate) were  below the threshholds of concern identified by Klein (2016). In contrast, results of the Shapiro-Wilk test of normality suggested that the distribution of difference scores was statistically significantly different than a normal distribution $(W=0.943, p = 0.002)

>Results of the paired samples *t*-test suggested nonsignificant differences $t(76) = -1.341, p = 0.184,d = -0.153$. The 95% confidence interval crossed zero, ranging from -0.305 to 0.069. Means and standard deviations are presented in Table 1 and illustrated in Figure 1.

```{r}
library(tidyverse)  #needed to use the pipe 
# Creating a smaller df to include only the variables I want in the
# table
Descripts_paired <- paired_wide %>%
    select(ANOVA, Multivariate, DIFF)
# using the apa.cor.table function for means, standard deviations,
# and correlations the filename command will write the table as a
# word document to your file
apaTables::apa.cor.table(Descripts_paired, table.number = 1, filename = "Tab1_PairedT.doc")
```
For the figure, let's re-run the paired samples *t* test, save it as an object, and use the "add_significance" function so that we can add it to our figure.

```{r}
paired_T <- rstatix::t_test(paired_long2, TradPed ~ Course, paired = TRUE, detailed = TRUE)%>%
  rstatix::add_significance()
paired_T
```
Next, we create boxplot:

```{r}
pairT.box <- ggpubr::ggpaired(paired_long2, x = "Course", y = "TradPed", order = c("ANOVA",
    "Multivariate"), line.color = "gray", palette = c("npg"), color = "Course",
    ylab = "Traditional Pedagogy", xlab = "Statistics Course", title = "Figure 1. Evaluation of Traditional Pedagogy as a Function of Course")

paired_T <- paired_T %>%
    rstatix::add_xy_position(x = "Course")  #autocomputes p-value labels positions

pairT.box <- pairT.box + ggpubr::stat_pvalue_manual(paired_T, tip.length = 0.02, y.position = c(5.5)) + labs(subtitle = rstatix::get_test_label(paired_T, detailed = TRUE))

pairT.box
```

#### Conduct power analyses to determine the power of the current study and a recommended sample size

Script for estimating current power:  

* d is Cohen's *d*
* n is number of pairs, but set to NULL if we want to estimate sample size 
* power is conventionally set at .80, but left at NULL when we want to estimate power
* sig.level is conventionlaly set at 0.05
* type indicates the type of *t*-test; in this example it is "paired"
* alternative indicates one or two.sided

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d=-0.153, n = 77, power=NULL, sig.level=0.05, type="paired", alternative="two.sided")
```
We had a 26% chance of finding a statistically significant result if, in fact, one existed.

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
pwr::pwr.t.test(d=-0.153,n = NULL, power=.80,sig.level=0.05,type="paired",alternative="two.sided")
```
If we presumed power were at 80%, we would need a sample size of 337.

### Hand Calculations

For these hand calculations I will used the "paired_wide" dataframe that we had prepared for the homework assignment intended for R and R packages.

#### Using traditional NHST (null hypothesis testing language), state your null and alternative hypotheses

The null hypotheses states that the true difference in means is zero.
$H_{O}: \mu _{D} = 0$

The alternative hypothesis states that the true difference in means is not zero.
$H_{A}: \mu _{D}\neq 0$

#### Using an R package or functions in base R (and with data in the "wide" format), calculate the *difference* score between the two observations of the dependent variable 

We had already calculated a difference score in the earlier assignment. Here it is again. 
```{r}
paired_wide$DIFF <- paired_wide$ANOVA - paired_wide$Multivariate
```


#### Obtain the mean and standard deviation of the *difference* score 

We can obtain the mean and standard deviation for the difference score with this script.

```{r}
psych::describe(paired_wide$DIFF)
```

The mean difference ($\bar{D}$) is -0.12; the standard deviation ($\hat\sigma_D$) of the difference score is 0.8.

#### Calculate the paired samples *t*-test 

Here is the formula for the paired samples *t*-test:

$$t = \frac{\bar{D}}{\hat\sigma_D / \sqrt{N}}$$
Using the values we located we can calculate the value of the *t* statistic.


```{r}
-0.12/(0.8/sqrt(77))
```
The value we calculated with the *rstatix::t_test()* function was -1.34. Considering rounding error, I think we got it!

#### Identify the degrees of freedom associated with your paired samples *t*-test 

We have 77 pairs.  The degrees of freedom for the paired samples *t*-test is $N - 1$. Therefore, df = 76.

#### Locate the test critical value for your paired samples *t*-test

I could look at the [table of critical values](https://www.statology.org/t-distribution-table/) for the *t*-distribution. Because I have non-directional hypotheses, I would use the column for a *p*-value of .05 for a two-tailed test. I roll down to the closest sample size (I'll pick 60). This suggests that my *t*-test statistic would need to be greater than 2.0 in order to be statistically significant.

I can also use the *qt()* function in base R. This function requires that I specify the alpha level (0.05), whether the test is one- or two-tailed (2), and my degrees of freedom (76). Specifying "TRUE" and "FALSE" after the lower.tail command gives the positive and negative regions of rejection.

```{r}
qt(0.05/2, 76, lower.tail = TRUE)
qt(0.05/2, 76, lower.tail = FALSE)
```
It is not surprising that these values are a smidge lower than 2.0. Why? Because in the table we stopped at df of 60, when it is actually 76.

#### Is the paired samples *t*-test statistically significant? Why or why not? 

The paired samples *t*-test is not statistically significant because the *t*-value of -1.316245 does not exceed -1.992.

#### What is the confidence interval around the mean difference? 

Here is the formula for hand-calculating the confidence interval. 

$$\bar{D}\pm t_{cv}(s_{d}/\sqrt{n})$$

* $\bar{D}$ the mean difference score
* $t_{cv}$ the test critical value for a two-tailed model (even if the hypothesis was one-tailed) where $\alpha = .05$ and the degrees of freedom are $N-1$
* $s_{d}$ the standard deviation of $\bar{D}$
* $N$ sample size

Let's calculate it:

```{r tidy=TRUE, tidy.opts=list(width.cutoff=70)}
-0.12-(-1.991673*((0.8/(sqrt(77)))))
-0.12+(-1.991673*((0.8/sqrt(77))))
```
These values indicate the range of scores in which we are 95% confident that our true $\bar{D}$ lies. Stated another way, we are 95% confident that the true mean difference lies between -0.302 and 0.062. Because this interval crosses zero, we cannot rule out that the true mean difference is 0.00. This result is consistent with our non-significant *p* value. For these types of statistics, the 95% confidence interval and *p* value will always be yoked together. 

#### Calculate the effect size (i.e., Cohen's *d* associated with your paired samples *t*-test

Cohen's *d* measures, in standard deviation units, the distance between the two means. Regardless of sign, values of .2, .5, and .8 are considered to be small, medium, and large, respectively. 

Because the paired samples *t*-test used the difference score in the numerator, there are two easy options for calculating this effect:

$$d=\frac{\bar{D}}{\hat\sigma_D}=\frac{t}{\sqrt{N}}$$
Here's a demonstration of both:

```{r}
-0.12/.8
-1.316245/sqrt(77)
```

#### Assemble the results into a statistical string.

$t(76) = -1.316, p > .05, CI95(-0.302, 0.062), d = -0.15$
