
## Homeworked Example

[Screencast Link]()

*If you wanted to use this example and dataset as a basis for a homework assignment, you could compare a differenc combination of courses and/or score one of the other course evaluation subscales (e.g., socially responsive pedagogy or valued-by-me). *

### Working the Problem with R and R Packages

#### Narrate the research vignette, describing the variables and their role in the analysis

I want to ask the question, "Do students' evaluations of traditional pedagogy (TradPed) change from ANOVA (the first course in the series) to Multivariate (the second course in the series)?." Unlike the independent samples *t*-test where we compared students in two different departments, we are comparing *the same* students across two different conditions. In this particular analysis, there is also an element of time. That is the ANOVA class always precedes the multivariate class (with a regression class, taught by a different instructor) in the intervening academic quarter.  

This research design has some clear limitations. Threats to internal validity are caused by issues like history and maturation. None-the-less, for the purpose of a statistical demonstration, this dataset works. 

Like most data, some manipulation is required before we can begin the analyses.

#### Simulate (or import) and format data

Let's import the larger dataset. 
```{r}
larger <- readRDS("ReC.rds")
```

The TradPed (traditional pedagogy) variable is an average of the items on that scale. I will first create that variable.

```{r}
#Creates a list of the variables that belong to that scale
TradPed_vars <- c('ClearResponsibilities', 'EffectiveAnswers','Feedback', 'ClearOrganization','ClearPresentation')

#Calculates a mean if at least 75% of the items are non-missing; adjusts the calculating when there is missingness
larger$TradPed <- sjstats::mean_n(larger[, ..TradPed_vars], .75)

```

From the "larger" data, let's select only the variable we will use in the analysis. I have included "long" in the filename because the structure of the dataset is that course evaluation by each student is in its own row. That is, each student could have up to three rows of data.

We need both "long" and "wide" forms to conduct the analyses required for both testing the statistical assumptions and performing the paired samples *t*-test.
```{r}
paired_long <-(dplyr::select (larger, deID, Course, TradPed))
```

From that reduced variable set, let's create a subset with students only from those two courses.
```{r}
paired_long <- subset(paired_long, Course == "ANOVA" | Course == "Multivariate") 
```

Regarding the structure of the data, we want the conditions (ANOVA, multivariate) to be factors and the TradPed variable to be continuously scaled. The format of the deID variable can be any numerical or categorical format -- just not a "chr" (character) variable.

```{r}
str(paired_long)
```
R correctly interpreted our variables.

For analyzing the assumptions associated with the paired-samples *t*-test, the format needs to be "wide" form (where each student has both observations on one row). Our data is presently in "long" form (where each observation is listed in each row). Here's how to reshape the data.

```{r}
paired_wide <- reshape2::dcast(data = paired_long, formula =deID ~ Course, value.var = "TradPed")
```

Let's recheck the structure.
```{r}
str(paired_wide)
```
You will notice that there is a good deal of missingness in the Multivariate condition. This is caused because the most recent cohort of students had not yet taken the course. While managing missingness is more complex than this, for the sake of simplicity, I will create a dataframe with non-missing data.

Doing so should also help with the hand-calculations later in the worked example.

```{r}
paired_wide <- na.omit(paired_wide)
```

#### Evaluate statistical assumptions

We need to evaluate the *distribution of the difference score* in terms of skew and kurtosis. We want this distribution of difference scores to be normally distributed.

This means we need to create a difference score:  

```{r}
paired_wide$DIFF <- paired_wide$ANOVA - paired_wide$Multivariate
```

We can use the *psych::describe()* function to obtain skew and kurtosis. 
```{r}
psych::describe(paired_wide)
```

Regarding the DIFF score, the skew (0.56) and kurtosis (3.15) values were well below the threshholds of concern identified by Klein (2016). 

We can formally test for deviations from normality with a Shapiro-Wilk. We want the results to be non-significant.

```{r}
rstatix::shapiro_test(paired_wide, DIFF)
```
Results of the Shapiro-Wilk test of normality are statistically significant $(W = 0.943, p = 0.002)$. This means that the distribution of difference scores are statistically significantly different from a normal distribution.

although not required in the formal test of instructions, a *pairs panel* of correlations and distributions can be useful in undersatnding our data.

```{r}
psych::pairs.panels(paired_wide)
```
Visual inspection of the distributions of the specific course variables were negatively skewed, with values clustered at the high end of the course evaluation ratings. However, the distribution for the DIFF variable seems relatively normal (although maybe a bit leptokurtic). This is consistent with the statistically significant Shapiro-Wilk test.

Before moving forward, I want to capture my analysis of assumptions:

>We began by analyzing the data to see if it met the statistical assumptions for analysis with a paired samples t-test. Regarding the assumption of normality, the skew (0.56) and kurtosis (3.15) values associated with the difference between conditions (ANOVA and multivariate) were  below the threshholds of concern identified by Klein (2016). In contrast, results of the Shapiro-Wilk test of normality suggested that the distribution of difference scores was statistically significantly different than a normal distribution $(W = 0.943, p = 0.002)$. 

#### Conduct a paired samples t-test (with an effect size & 95%CIs)

So this may be a bit tricky, but our original "long" form of the data has more ANOVA evaluations (students who had taken ANOVA had not yet taken multivariate) than multivariate. The paired samples *t* test requires the design to be balanced.  When we used the *na.omit()* function with the wide case, we effectively balanced the design, eliminating students who lacked observations across both courses. Let's restructure that wide format back to long format so that the design will be balanced.

```{r}

paired_long2 <- data.table::melt(data.table::setDT(paired_wide), id.vars = c("deID"),
    measure.vars = list(c("ANOVA", "Multivariate")))

paired_long2 <- dplyr::rename(paired_long2, Course = variable, TradPed = value)

head(paired_long2)
```

```{r}
rstatix::t_test(paired_long2, TradPed ~ Course, paired = TRUE, detailed = TRUE)
```
I'll begin the *t* string with this output:  $t(76) = -0.113, p = 0.184, CI95(-0.305, 0.069)$. The difference in course evaluations is not statistically significantly difference. We are 955 confident that the true difference in means is as low as -0.301 or as high as 0.060.

we calculate the Cohen's *d* (the effect size) this way:

```{r}
rstatix::cohens_d(paired_long2, TradPed ~ Course, paired = TRUE)
```
 The value of -0.153 is quite small. We can add this value to our statistical string:  $t(76) = -0.113, p = 0.184, CI95(-0.305, 0.069), d = -0.153$

#### APA style results with table(s) and figure

>A paired samples *t*-test was conducted to evaluate the hypohtesis that there would be statistically significant differences in students' course evaluations of ANOVA and multivariate statistics classses.

>We began by analyzing the data to see if it met the statistical assumptions for analysis with a paired samples t-test. Regarding the assumption of normality, the skew (0.56) and kurtosis (3.15) values associated with the difference between conditions (ANOVA and multivariate) were  below the threshholds of concern identified by Klein (2016). In contrast, results of the Shapiro-Wilk test of normality suggested that the distribution of difference scores was statistically significantly different than a normal distribution $(W=0.943, p = 0.002)

>Results of the paired samples *t*-test suggested nonsignificant differences $t(76) = -0.113, p = 0.184,d = -0.153$. The 95% confidence interval crossed zero, ranging from -0.305 to 0.069. Means and standard deviations are presented in Table 1 and illustrated in Figure 1.

```{r}
library(tidyverse)  #needed to use the pipe 
# Creating a smaller df to include only the variables I want in the
# table
Descripts_paired <- paired_wide %>%
    select(ANOVA, Multivariate, DIFF)
# using the apa.cor.table function for means, standard deviations,
# and correlations the filename command will write the table as a
# word document to your file
apaTables::apa.cor.table(Descripts_paired, table.number = 1, filename = "Tab1_PairedT.doc")
```


For the figure, let's re-run the paired samples *t* test, save it as an object, and use the "add_significance" function so that we can add it to our figure.

```{r}
paired_T <- rstatix::t_test(paired_long2, TradPed ~ Course, paired = TRUE, detailed = TRUE)%>%
  rstatix::add_significance()
paired_T
```
Next, we create boxplot:

```{r}
pairT.box <- ggpubr::ggpaired(paired_long2, x = "Course", y = "TradPed", order = c("ANOVA",
    "Multivariate"), line.color = "gray", palette = c("npg"), color = "Course",
    ylab = "Traditional Pedagogy", xlab = "Statistics Course", title = "Figure 1. Evaluation of Traditional Pedagogy as a Function of Course")

paired_T <- paired_T %>%
    rstatix::add_xy_position(x = "Course")  #autocomputes p-value labels positions

pairT.box <- pairT.box + ggpubr::stat_pvalue_manual(paired_T, tip.length = 0.01,
    y.position = c(5.5)) + labs(subtitle = rstatix::get_test_label(paired_T,
    detailed = TRUE))

pairT.box
```


### Hand Calculations

