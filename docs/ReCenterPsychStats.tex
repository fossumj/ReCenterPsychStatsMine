% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  11pt,
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}


\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={ReCentering Psych Stats},
  pdfauthor={Lynette H. Bikos, PhD, ABPP},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{ReCentering Psych Stats}
\author{Lynette H. Bikos, PhD, ABPP}
\date{21 Aug 2022}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{3}
\tableofcontents
}
\hypertarget{book-cover}{%
\chapter*{BOOK COVER}\label{book-cover}}
\addcontentsline{toc}{chapter}{BOOK COVER}

\includegraphics{images/ReCenterPsychStats-bookcover2.jpg}
This open education resource is available in three formats:

\begin{itemize}
\tightlist
\item
  Formatted as an \href{https://lhbikos.github.io/ReCenterPsychStats/}{html book} via GitHub Pages
\item
  As a \href{https://github.com/lhbikos/ReCenterPsychStats/blob/main/docs/ReCenterPsychStats.pdf}{PDF} available in the \href{https://github.com/lhbikos/ReCenterPsychStats/tree/main/docs}{docs} folder at the GitHub repository
\item
  As an \href{https://github.com/lhbikos/ReCenterPsychStats/blob/main/docs/ReCenterPsychStats.epub}{ebook} available in the \href{https://github.com/lhbikos/ReCenterPsychStats/tree/main/docs}{docs} folder at the GitHub repository
\end{itemize}

All materials used in creating this OER are available at its \href{https://github.com/lhbikos/ReCenterPsychStats}{GitHub repo}.

As a perpetually-in-progress, open education resource, feedback is always welcome. This IRB-approved (SPU IRB \#202102010R, no expiration) \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_0OnBLfut3VIOIS2}{Qualtrics-hosted survey} includes formal rating scales, open-ended text boxes, and a portal for uploading attachments (e.g., marked up PDFs). You are welcome to complete only the portions that are relevant to you.

\hypertarget{preface}{%
\chapter*{PREFACE}\label{preface}}
\addcontentsline{toc}{chapter}{PREFACE}

\textbf{If you are viewing this document, you should know that this is a book-in-progress. Early drafts are released for the purpose teaching my classes and gaining formative feedback from a host of stakeholders. The document was last updated on 21 Aug 2022}. Emerging volumes on other statistics are posted on the \href{https://lhbikos.github.io/BikosRVT/ReCenter.html}{ReCentering Psych Stats} page at my research team's website.

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c932455e-ef06-444a-bdca-acf7012d759a}{Screencasted Lecture Link}

To \emph{center} a variable in regression means to set its value at zero and interpret all other values in relation to this reference point. Regarding race and gender, researchers often center male and White at zero. Further, it is typical that research vignettes in statistics textbooks are similarly seated in a White, Western (frequently U.S.), heteronormative, framework. The purpose of this project is to create a set of open educational resources (OER) appropriate for doctoral and post-doctoral training that contribute to a socially responsive pedagogy -- that is, it contributes to justice, equity, diversity, and inclusion.

Statistics training in doctoral programs are frequently taught with fee-for-use programs (e.g., SPSS/AMOS, SAS, MPlus) that may not be readily available to the post-doctoral professional. In recent years, there has been an increase and improvement in R packages (e.g., \emph{psych}, \emph{lavaan}) used for analyses common to psychological research. Correspondingly, many graduate programs are transitioning to statistics training in R (free and open source). This is a challenge for post-doctoral psychologists who were trained with other software. This OER will offer statistics training with R and be freely available (specifically in a GitHub repository and posted through GitHub Pages) under a Creative Commons Attribution - Non Commercial - Share Alike license {[}CC BY-NC-SA 4.0{]}.

Training models for doctoral programs in health service psychology are commonly scholar-practitioner, scientist-practitioner, or clinical-scientist. An emerging model, the \emph{scientist-practitioner-advocacy} training model, incorporates social justice advocacy so that graduates are equipped to recognize and address the sociocultural context of oppression and unjust distribution of resources and opportunities \citep{mallinckrodt_scientist-practitioner-advocate_2014}. In statistics textbooks, the use of research vignettes engages the learner around a tangible scenario for identifying independent variables, dependent variables, covariates, and potential mechanisms of change. Many students recall examples in Field's \citeyearpar{field_discovering_2012} popular statistics text: Viagra to teach one-way ANOVA, beer goggles for two-way ANOVA, and bushtucker for repeated measures. What if the research vignettes were more socially responsive?

In this OER, research vignettes will be from recently published articles where:

\begin{itemize}
\tightlist
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC{[}low-middle income countries{]}),
\item
  the research is responsive to issues of justice, equity, inclusion, diversity,
\item
  the lesson's statistic is used in the article, and
\item
  there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s); or it is publicly available.
\end{itemize}

In training for multicultural competence, the saying, ``A fish doesn't know that it's wet'' is often used to convey the notion that we are often unaware of our own cultural characteristics. In recent months and years, there has been an increased awakening to institutional and systemic factors that contribute to discrimination as a function of race, gender, nationality, class, and so forth. Queuing from the water metaphor, I am hopeful that a text that is recentered in the ways I have described can contribute to \emph{changing the water} in higher education and in the profession of psychology.

\hypertarget{copyright-with-open-access}{%
\section*{Copyright with Open Access}\label{copyright-with-open-access}}
\addcontentsline{toc}{section}{Copyright with Open Access}

This book is published under a a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. This means that this book can be reused, remixed, retained, revised and redistributed (including commercially) as long as appropriate credit is given to the authors. If you remix, or modify the original version of this open textbook, you must redistribute all versions of this open textbook under the same license: CC BY-SA 4.0.

A \href{https://github.com/lhbikos/ReCenterPsychStats}{GitHub open-source repository} contains all of the text and source code for the book, including data and images.

\hypertarget{acknowledgements}{%
\chapter*{ACKNOWLEDGEMENTS}\label{acknowledgements}}
\addcontentsline{toc}{chapter}{ACKNOWLEDGEMENTS}

As a doctoral student at the University of Kansas (1992-1996), I learned that ``a foreign language'' was a graduation requirement. \emph{Please note that as one who studies the intersections of global, vocational, and sustainable psychology, I regret that I do not have language skills beyond English.} This could have been met with credit from high school, but my rural, mid-Missouri high school did not offer such classes. This requirement would have typically been met with courses taken during an undergraduate program -- but my non-teaching degree in the University of Missouri's School of Education was exempt from this. The requirement could have also been met with a computer language (FORTRAN, C++) -- but I did not have any of those either. There was a tiny footnote on my doctoral degree plan that indicated that a 2-credit course, ``SPSS for Windows'' would substitute for the language requirement. Given that it was taught by my one of my favorite professors, I readily signed up. As it turns out, Samuel B. Green, PhD, was using the course to draft chapters in the textbook \citep{green_using_2014} that has been so helpful for so many. Unfortunately, Drs. Green (1947 - 2018) and Salkind (1947 - 2017) are no longer with us. I have worn out numerous versions of their text. Another favorite text of mine has been Dr.~Barbara Byrne's \citeyearpar{byrne_structural_2016}, ``Structural Equation Modeling with AMOS.'' I loved the way she worked through each problem and paired it with a published journal article, so that the user could see how the statistical evaluation fit within the larger project/article. I took my tea-stained text with me to a workshop she taught at APA and was proud of the signature she added to it. Dr.~Byrne created SEM texts for a number of statistical programs (e.g., LISREL, EQS, MPlus). As I was learning R, I wrote Dr.~Byrne, asking if she had an edition teaching SEM/CFA with R. She promptly wrote back, saying that she did not have the bandwidth to learn a new statistics package. We lost Dr.~Byrne in December 2020. I am so grateful to these role models for their contributions to my statistical training. I am also grateful for the doctoral students who have taken my courses and are continuing to provide input for how to improve the materials.

The inspiration for training materials that re*center statistics and research methods came from the \href{https://www.academics4blacklives.com/}{Academics for Black Survival and Wellness Initiative}. This project, co-founded by Della V. Mosley, Ph.D., and Pearis L. Bellamy, M.S., made clear the necessity and urgency for change in higher education and the profession of psychology.

At very practical levels, I am indebted to SPU's Library, and more specifically, SPU's Education, Technology, and Media Department. Assistant Dean for Instructional Design and Emerging Technologies, R. John Robertson, MSc, MCS, has offered unlimited consultation, support, and connection. Senior Instructional Designer in Graphics \& Illustrations, Dominic Wilkinson, designed the logo and bookcover. Psychology and Scholarly Communications Librarian, Kristin Hoffman, MLIS, has provided consultation on topics ranging from OERS to citations. I am alo indebted to Associate Vice President, Teaching and Learning at Kwantlen Polytechnic University, Rajiv Jhangiani, PhD. Dr.~Jhangiani's text \citeyearpar{jhangiani_research_2019} was the first OER I ever used and I was grateful for his encouraging conversation.

Financial support for this project has been provided the following:

\begin{itemize}
\tightlist
\item
  \emph{Call to Action on Equity, Inclusion, Diversity, Justice, and Social Responsivity Request for Proposals} grant from the Association of Psychology Postdoctoral and Internship Centers (2021-2022).
\item
  \emph{Diversity Seed Grant}, Office of Inclusive Excellence and Advisory Council for Diversity and Reconciliation (ACDR), Seattle Pacific University.
\item
  \emph{ETM Open Textbook \& OER Development Funding}, Office of Education, Technology, \& Media, Seattle Pacific University.
\end{itemize}

\hypertarget{ReCintro}{%
\chapter{Introduction}\label{ReCintro}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=cc9b7c0d-e5c3-4e4e-a469-acf7013ee761}{Screencasted Lecture Link}

\hypertarget{what-to-expect-in-each-chapter}{%
\section{What to expect in each chapter}\label{what-to-expect-in-each-chapter}}

This textbook is intended as \emph{applied,} in that a primary goal is to help the scientist-practitioner-advocate use a variety of statistics in research problems and \emph{writing them up} for a program evaluation, dissertation, or journal article. In support of that goal, I try to provide just enough conceptual information so that the researcher can select the appropriate statistic (i.e., distinguishing between when ANOVA is appropriate and when regression is appropriate) and assign variables to their proper role (e.g., covariate, moderator, mediator).

This conceptual approach does include occasional, step-by-step, \emph{hand-calculations} (using R to do the math for us) to provide a \emph{visceral feeling} of what is happening within the statistical algorithm that may be invisible to the researcher. Additionally, the conceptual review includes a review of the assumptions about the characteristics of the data and research design that are required for the statistic.

Statistics can be daunting, so I have worked hard to establish a \emph{workflow} through each analysis. When possible, I include a flowchart that is referenced frequently in each chapter and assists the researcher keep track of their place in the many steps and choices that accompany even the simplest of analyses.

As with many statistics texts, each chapter includes a \emph{research vignette.} Somewhat unique to this resource is that the vignettes are selected from recently published articles. Each vignette is chosen with the intent to meet as many of the following criteria as possible:

\begin{itemize}
\tightlist
\item
  the statistic that is the focus of the chapter was properly used in the article,
\item
  the author's identity is from a group where scholarship is historically marginalized (e.g., BIPOC, LGBTQ+, LMIC {[}low middle income countries{]}),
\item
  the research has a justice, equity, inclusion, diversity, and social responsivity focus and will contribute positively to a social justice pedagogy, and
\item
  there is sufficient information in the article to simulate the data for the chapter example(s) and practice problem(s); or the data is available in a repository.
\end{itemize}

In each chapter we employ \emph{R} packages that will efficiently calculate the statistic and the dashboard of metrics (e.g., effect sizes, confidence intervals) that are typically reported in psychological science.

\hypertarget{strategies-for-accessing-and-using-this-oer}{%
\section{Strategies for Accessing and Using this OER}\label{strategies-for-accessing-and-using-this-oer}}

There are a number of ways you can access this resource. You may wish to try several strategies and then select which works best for you. I demonstrate these in the screencast that accompanies this chapter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Simply follow along in your preferred format of the book (html, PDF, or ebook) and then

  \begin{itemize}
  \tightlist
  \item
    open a fresh .rmd file of your own, copying (or retyping) the script and running it
  \end{itemize}
\item
  Locate the original documents at the \href{https://github.com/lhbikos/ReCenterPsychStats}{GitHub repository}. You can

  \begin{itemize}
  \tightlist
  \item
    open them to simply take note of the ``behind the scenes'' script
  \item
    copy/download individual documents that are of interest to you
  \item
    clone a copy of the entire project to your own GitHub site and further download it (in its entirety) to your personal workspace. The \href{https://desktop.github.com/}{GitHub Desktop app} makes this easy!
  \end{itemize}
\item
  Listen to the accompanying lectures (I think sound best when the speed is 1.75). The lectures are being recorded in Panopto and should include the closed captioning.
\item
  Each time the book is updated, new ebook and PDF versions are also pushed out. You can access these in the ``docs'' folder at the \href{https://github.com/lhbikos/ReCenterPsychStats}{GitHub repository}.
\item
  Provide feedback to me! If you fork a copy to your own GitHub repository, you can

  \begin{itemize}
  \tightlist
  \item
    open up an editing tool and mark up the document with your edits,
  \item
    start a discussion by leaving comments/questions, and then
  \item
    sending them back to me by committing and saving. I get an e-mail notiying me of this action. I can then review (accepting or rejecting) them and, if a discussion is appropriate, reply back to you.
  \item
    I am also seeking peer-review feedback at this \href{https://spupsych.az1.qualtrics.com/jfe/form/SV_0OnBLfut3VIOIS2}{Qualtrics-hosted survey}. You are welcome to complete only the portions that are relevant to you.
  \end{itemize}
\end{enumerate}

\hypertarget{if-you-are-new-to-r}{%
\section{If You are New to R}\label{if-you-are-new-to-r}}

R can be oveRwhelming. Jumping right into advanced statistics might not be the easiest way to start. The \href{https://lhbikos.github.io/ReCenterPsychStats/Ready.html}{Ready\_Set\_R}lesson of this volume provides an introduction and the \href{https://lhbikos.github.io/ReCenterPsychStats/waRmups.html}{waRming up}lesson walks through simple data preparation and descriptive statistics.

In the remaining lessons, I have attempted to provide complete code for every step of the process, starting with uploading the data. To help explain what R script is doing, I sometimes write it in the chapter text; sometimes leave hashtagged-comments in the chunks; and, particularly in the accompanying screencasted lectures, try to take time to narrate what the R script is doing.

I've found that, somewhere on the internet, there's almost always a solution to what I'm trying to do. I am frequently stuck and stumped and have spent hours searching the internet for even the tiniest of tasks. When you watch my videos, you may notice that in my R studio, there is a ``scRiptuRe'' file. I take notes on the solutions and scripts here -- using keywords that are meaningful to me so that when I need to repeat the task, I can hopefully search my own prior solutions and find a fix or a hint. You may also find it useful to create a working document of your own tips and tricks.

\hypertarget{Ready}{%
\chapter{Ready\_Set\_R}\label{Ready}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=6b27a60c-edcb-4565-aaf1-ad890174586e}{Screencasted Lecture Link}

With the goal of creating a common, system-wide approach to using the platform, this lesson was originally created for Clinical and Industrial-Organizational doctoral students who are entering the ``stats sequence.'' I hope it will be useful for others (e.g., faculty, post-doctoral researchers, and practitioners) who are also making the transition to R.

\hypertarget{navigating-this-lesson}{%
\section{Navigating this Lesson}\label{navigating-this-lesson}}

There is about 45 minutes of lecture.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives}{%
\subsection{Learning Objectives}\label{learning-objectives}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Downloading/installing R's parts and pieces.
\item
  Using R-Markdown as the interface for running R analyses and saving the script.
\item
  Recognizing and adopting best practices for ``R hygiene.''
\item
  Identifying effective strategies for troubleshooting R hiccups.
\end{itemize}

\hypertarget{downloading-and-installing-r}{%
\section{downloading and installing R}\label{downloading-and-installing-r}}

\hypertarget{so-many-parts-and-pieces}{%
\subsection{So many paRts and pieces}\label{so-many-parts-and-pieces}}

Before we download R, it may be helpful to review some of R's many parts and pieces.

The base software is free and is available \href{https://www.r-project.org/}{here}

Because R is already on my machine (and because the instructions are sufficient), I will not walk through the demo, but I will point out a few things.

\begin{itemize}
\tightlist
\item
  The ``cran'' (I think ``cranium'') is the \emph{Comprehensive R Archive Network.} In order for R to run on your computer, you have to choose a location -- and it should be geographically ``close to you.''

  \begin{itemize}
  \tightlist
  \item
    Follow the instructions for your operating system (Mac, Windows, Linux)
  \item
    You will see the results of this download on your desktop (or elsewhere if you chose to not have it appear there) but you won't ever use R through this platform.
  \end{itemize}
\item
  \href{https://www.rstudio.com/products/RStudio/}{R Studio} is the way in which we operate R. It's a separate download. Choose the free, desktop, option that is appropriate for your operating system:\\
\item
  \emph{R Markdown} is the way that many analysts write \emph{script}, conduct analyses, and even write up results. These are saved as .rmd files.

  \begin{itemize}
  \tightlist
  \item
    In R Studio, open an R Markdown document through File/New File/R Markdown
  \item
    Specify the details of your document (title, author, desired ouput)
  \item
    In a separate step, SAVE this document (File/Save{]} into a NEW FILE FOLDER that will contain anything else you need for your project (e.g., the data).
  \item
    \emph{Packages} are at the heart of working in R. Installing and activating packages require writing script.
  \end{itemize}
\end{itemize}

\textbf{Note} If you have an enterprise-owned machine (e.g,. in my specific context, if you are a faculty/staff or have a lab with institution-issued laptops) there can be complications caused by how documents are stored. In recent years we have found that letting the computer choose where to load base R, R Studio, and the packages generally works. The trick, though, is to save R projects (i.e., folder with .rmd files and data) into the OneDrive folder that syncs to your computer. If you have difficulty knitting that is unrelated to code (verified by getting a classmates or colleague to successfully knit it), it is likely because you have saved the files to the local hard drive and not OneDrive. If you continue to have problems I recommend consulting with your computer and technology support office.

\hypertarget{orienting-to-r-studio-focusing-only-on-the-things-we-will-be-using-first-and-most-often}{%
\subsection{oRienting to R Studio (focusing only on the things we will be using first and most often)}\label{orienting-to-r-studio-focusing-only-on-the-things-we-will-be-using-first-and-most-often}}

R Studio is organized around four panes. These can be sized and rearranged to suit your personal preferences.

\begin{itemize}
\tightlist
\item
  Upper right window

  \begin{itemize}
  \tightlist
  \item
    Environment: lists the \emph{objects} that are available to you (e.g., dataframes)
  \end{itemize}
\item
  Lower right window
\item
  \emph{Files}: Displays the file structure in your computer's environment. Make it a practice to (a) organize your work in small folders and (b) navigating to that small folder that is holding your project when you are working on it.
\item
  \emph{Packages}: Lists the packages that have been installed. If you navigate to it, you can see if it is ``on.'' You can also access information about the package (e.g., available functions, examples of script used with the package) in this menu. This information opens in the Help window.
\item
  The \emph{Viewer} and \emph{Plots} tabs will be useful, later, in some advanced statistics when we can simultaneously examine output and script in windows that are side-by-side.
\item
  Upper left window

  \begin{itemize}
  \tightlist
  \item
    If you are using R Markdown, that file lives here and is composed of open space and chunks.
  \end{itemize}
\item
  Lower left window

  \begin{itemize}
  \tightlist
  \item
    R Studio runs in the Console (the background). Very occasionally, I can find useful troubleshooting information here.
  \item
    More commonly, I open my R Markdown document so that it takes the whole screen.
  \end{itemize}
\end{itemize}

\hypertarget{best-practices}{%
\section{best pRactices}\label{best-practices}}

Many initial problems in R can be solved with good R hygiene. Here are some suggestions for basic practices. It can be tempting to ``skip this.'' However, in the first few weeks of class, these are the solutions I am presenting (and repeating, ad nauseum) to my students.

\hypertarget{everything-is-documented-in-the-.rmd-file}{%
\subsection{Everything is documented in the .rmd file}\label{everything-is-documented-in-the-.rmd-file}}

Although others do it differently, I put everything is in my .rmd file. That is, for uploading data I write the code in my .rmd file. For opening packages, I include the package in my script. I also use the .rmd file to make notes about what I was thinking and why I made the choices I did. I also keep a ``bug log'' -- noting what worked and what did not work. I will also begin my APA style results section directly in the .rmd file.

Why do I do all this? Because when I return to my project hours or years later, I have a permanent record of very critical things like (a) where my data is located, (b) what version I was using, and (c) what package was associated with the functions.

\hypertarget{setting-up-the-file}{%
\subsection{Setting up the file}\label{setting-up-the-file}}

File organization is a critical key to success:

\begin{itemize}
\tightlist
\item
  Create a project file folder.
\item
  Put the data file in it.
\item
  Open an R Markdown file.
\item
  Save it in the same file folder.
\item
  When your data and .rmd files are in the same folder (not your desktop, but a shared folder) the data can be pulled into the .rmd file without creating a working directory.
\end{itemize}

\hypertarget{script-in-chunks-and-everything-else-in-the-inline-text-sections}{%
\subsection{Script in chunks and everything else in the ``inline text'' sections}\label{script-in-chunks-and-everything-else-in-the-inline-text-sections}}

The R Markdown document is an incredible tool for integrating text, tables, and analyses. This entire OER is written in R Markdown. A central feature of this is ``chunks.''

The only thing in the chunks should be script for running R. You can also hashtag-out comments so they won't run.

You can put almost anything you want in the ``inline text with simple formatting.'' Syntax for simple formatting in the text areas (e.g., using italics, making headings, bold) is found here: \url{https://rmarkdown.rstudio.com/authoring_basics.html}

``Chunks'' start and end with with three tic marks and will show up in a shaded box. Chunks have three symbols in their upper right. Those controls will disappear and your script will not run) if you have replaced them with double or single quotation marks or one or more of the tics are missing.

The easiest way to insert a chunk is to use the INSERT/R command at the top of this editor box. You can also insert a chunk with the keyboard shortcut: CTRL/ALT/i

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#hashtags let me write comments to remind myself what I did}
\CommentTok{\#here I am simply demonstrating arithmetic (but I would normally be running code)}
\DecValTok{2021} \SpecialCharTok{{-}} \DecValTok{1966}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 55
\end{verbatim}

\hypertarget{managing-packages}{%
\subsection{Managing packages}\label{managing-packages}}

As scientist-practitioners (and not coders), we will rely on \emph{packages} to do much of the work. At first you may feel overwhelmed about the large number of packages that are available. Soon, though, you will become accustomed to the ones most applicable to our work (e.g., psych, tidyverse, lavaan, apaTables).

Researchers treat packages differently. In these lectures, I list all the packages we will use in an opening chunk at the beginning of the lecture. When the hashtags are removed, the script will ask R to check to see if the package is installed. If it is, installation is skipped. If it is not, R installs it. Simply remove the hashtag to run the code the first time, then hashtag them back out so R is not always re-checking.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

To make a package operable, you need to open it. There are two primary ways to do this. The first is to use the library function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages ("psych")}
\FunctionTok{library}\NormalTok{ (psych)}
\end{Highlighting}
\end{Shaded}

The second way is to place a double colon between the package and function. This second method has become my preferred practice because it helps me remember what package goes with each function. It can also prevent R hiccups when there are identical function names and R does not know which package to use. Below is an example where I might ask for descriptives from the psych package. Because I have not yet uploaded data, I have hashtagged it out, making the command inoperable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#psych::describe(mydata)}
\end{Highlighting}
\end{Shaded}

There are exceptions. One is the \emph{tidyverse} package. Some of my script uses pipes (\%\textgreater\%) and they require tidyverse to be activated this is why you will often see me call the tidyverse package with the \emph{library()} function (as demonstrated above.)

\hypertarget{upload-the-data}{%
\subsection{Upload the data}\label{upload-the-data}}

When imported (or simulated) properly, data will appear as an object in the global environment.

In the context of this OER, I will be simulating data right in each lesson for use in the lesson. This makes the web-based platform more \emph{portable.} This means that when working the problems in the chapter we do not (a) write the data to a file and (b) import data from files. Because these are essential skills, I will demonstrate this process here -- starting with simulating data.

At this point, simulating data is beyond the learning goals I have established for the chapter. I do need to include the code so that we get some data. The data I am simulating is used in the \protect\hyperlink{oneway}{one-way ANOVA lesson}. The data is from the Tran and Lee \citeyearpar{tran_you_2014} random clinical trial.

In this simulation, I am simply creating an ID number, a condition (High, Low, Control), and a score on the dependent variable, ``Accurate.'' More information about this study is included in the \protect\hyperlink{oneway}{one-way ANOVA chapter}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note, this simulation results in a different datset than is in the}
\CommentTok{\# OnewayANOVA lesson sets a random seed so that we get the same}
\CommentTok{\# results each time}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2021}\NormalTok{)}
\CommentTok{\# sample size, M and SD for each group}
\NormalTok{Accurate }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.18}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.8}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.83}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.58}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.76}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.56}\NormalTok{))}
\CommentTok{\# set upper bound for DV}
\NormalTok{Accurate[Accurate }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{3}
\CommentTok{\# set lower bound for DV}
\NormalTok{Accurate[Accurate }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\CommentTok{\# IDs for participants}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{90}\NormalTok{))}
\CommentTok{\# name factors and identify how many in each group; should be in same}
\CommentTok{\# order as first row of script}
\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"High"}\NormalTok{, }\DecValTok{30}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\DecValTok{30}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\DecValTok{30}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID, DV, condition}
\NormalTok{Acc\_sim30 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, COND, Accurate)}
\end{Highlighting}
\end{Shaded}

At this point, this data lives only in this .rmd file after the above code is run. Although there are numerous ways to export and import data, I have a preference for two.

\hypertarget{to-and-from-.csv-files}{%
\subsubsection{To and from .csv files}\label{to-and-from-.csv-files}}

The first is to write the data to a .csv file. In your computer's environment (outside of R), these files are easily manipulated in Excel. I think of them as being ``Excel lite'' because although Excel can operate them, they lack some of the more advanced features of an Excel spreadsheet.

In the code below, I identify the R object ``Acc\_sim30'' and give it a file name, ``to\_CSV.csv''. This file name must have the .csv extension. I also indicate that it should preserve the column names (but ignore row names; since we don't have row names).

This file will save in the same folder as wherever you are using this .rmd file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to write it to an outfile as a .csv}
\FunctionTok{write.table}\NormalTok{(Acc\_sim30, }\AttributeTok{file =} \StringTok{"to\_CSV.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Importing this object back into the R environment can be accomplished with some simple code. For the sake of demonstration,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .csv (think \textquotesingle{}Excel lite\textquotesingle{}) file on your}
\CommentTok{\# computer; it should save in the same file as the .rmd file you are}
\CommentTok{\# working with}
\NormalTok{from\_CSV }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"to\_CSV.csv"}\NormalTok{, }\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The advantage of working with .csv files is that it is then easy to inspect and manipulate them outside of the R environment. The disadvantage of .csv files is that each time they are imported they lose any formatting you may have meticulously assigned to them.

\hypertarget{to-and-from-.rds-files}{%
\subsubsection{To and from .rds files}\label{to-and-from-.rds-files}}

While it is easy enough to rerun the code (or copy it from data prep .rmd and paste it into an .rmd you are using for advanced analysis), there is a better way! Saving the data as an R object preserves all of its characteristics.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds file on your computer; it should save in}
\CommentTok{\# the same file as the .rmd file you are working with}
\FunctionTok{saveRDS}\NormalTok{(Acc\_sim30, }\StringTok{"to\_Robject.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This file will save to your computer (and you can send it to colleagues). However, it is not easy to ``just open it'' in Excel. To open an .rds file and use it (whether you created it or it is sent to you by a colleague), use the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{from\_rds }\OtherTok{\textless{}{-}} \FunctionTok{readRDS}\NormalTok{(}\StringTok{"to\_Robject.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you are the recipient of an R object, but want to view it as a .csv, simply import the .rds then use the above code to export it as a .csv.

\hypertarget{from-spss-files}{%
\subsubsection{From SPSS files}\label{from-spss-files}}

Your data may come to you in a variety of ways. One of the most common is SPSS. The \emph{foreign} package is popular for importing SPSS data. Below is code which would import an SPSS file \emph{if I had created one}. You'll see that this script is hashtagged out because I rarely use SPSS and do not have a handy file to demo.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# opening an SPSS file requires the foreign package which I opened}
\CommentTok{\# earlier from\_SPSS \textless{}{-} foreign::read.spss (\textquotesingle{}SPSSdata.sav\textquotesingle{},}
\CommentTok{\# use.value.labels = TRUE, to.data.frame = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{quick-demonstration}{%
\section{quick demonstRation}\label{quick-demonstration}}

Let's run some simple descriptives. In the script below, I am using the \emph{psych} package. Descriptive statistics will appear for all the data in the dataframe and the output will be rounded to three spaces. Note that rather than opening the psych package with the library function, I have used the double colon convention.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Acc\_sim30), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars  n  mean    sd median trimmed   mad min max range  skew kurtosis
ID*         1 90 45.50 26.12  45.50   45.50 33.36   1  90    89  0.00    -1.24
COND*       2 90  2.00  0.82   2.00    2.00  1.48   1   3     2  0.00    -1.53
Accurate    3 90  1.52  0.68   1.55    1.54  0.70   0   3     3 -0.19    -0.34
           se
ID*      2.75
COND*    0.09
Accurate 0.07
\end{verbatim}

Because ``ID'' is the case ID and COND is the factor (high, low, control), the only variable for which this data is sensible is ``Accurate.'' Nonetheless, this provides an example of how to apply a package's function to a dataset. As we progress through the text we will learn how to manage the data so that we get the specific output we are seeking.

\hypertarget{the-knitted-file}{%
\section{the knitted file}\label{the-knitted-file}}

One of the coolest things about R Markdown is its capacity to \emph{knit} to HTML, PPT, or WORD.

\begin{itemize}
\tightlist
\item
  In this OER, I am writing the lessons in R markdown (.rmd files), with the package \emph{bookdown} as a helper, and knitting the files to the .html format. In prior years, I knitted these documents to .doc formats. There are numerous possibilities!
\item
  The package \emph{papaja} is designed to prepare APA manuscripts where the writing, statistics, and references are all accomplished in a single file. This process contributes to replicability and reproducibility.
\item
  If you are using the PDF or ebook version of this OER you will realize that it is also possible to render to these formats. Albeit slighy more complicated, this is possible, too! More detailed instructions for this are provided in the \href{https://lhbikos.github.io/extRas/}{extRas} mini-volume of \href{https://lhbikos.github.io/BikosRVT/ReCenter.html}{ReCentering Psych Stats}.
\end{itemize}

\hypertarget{troubleshooting-in-r-markdown}{%
\section{tRoubleshooting in R maRkdown}\label{troubleshooting-in-r-markdown}}

Hiccups are normal. Here are some ideas that I have found useful in getting unstuck.

\begin{itemize}
\tightlist
\item
  In a given set of operations, you must run/execute each piece of code in order: every, single, time. That is, all the packages have to be in your library and activated.

  \begin{itemize}
  \tightlist
  \item
    If you open an .rmd file, you cannot just scroll down to make a boxplot. You need to run any \emph{prerequisite} script (like loading files, putting the data in the global environment, etc.)
  \item
    Lost? Clear your global environment (broom icon in the upper right) and start over. Fresh starts are good.
  \end{itemize}
\item
  Your .rmd file and your data need to be stored in the same file folder. Make unique folders for each project (even if each contains only a few files).
\item
  If you have tried what seems apparent to you and cannot solve your challenge, do not wait long before typing warnings into a search engine. Odds are, you'll get some useful hints in a manner of seconds. Especially at first, these are common errors:

  \begin{itemize}
  \tightlist
  \item
    The package isn't loaded.
  \item
    The .rmd file hasn't been saved yet, or isn't saved in the same folder as the data.
  \item
    There are errors in punctuation or spelling.
  \end{itemize}
\item
  Restart R (it's quick -- not like restarting your computer). I like to restart and clear my output and environment so that I can better track my order of operations.
\item
  If you receive an error indicating that a function isn't working or recognized, and you have loaded the package, type the name of the package in front of the function with two colons (e.g., psych::describe(df)). If multiple packages are loaded with functions that have the same name, R can get confused.
\end{itemize}

\hypertarget{just-why-have-we-transitioned-to-r}{%
\section{\texorpdfstring{just \emph{why} have we tRansitioned to R?}{just why have we tRansitioned to R?}}\label{just-why-have-we-transitioned-to-r}}

\begin{itemize}
\tightlist
\item
  It (or at least it appears to be) is the futuRe.
\item
  SPSS site (and individual) licenses are increasingly expensive and limited (e.g., we need Mplus, AMOS, HLM, or R). As package development for R is exploding, we have tools to ``do just about anything.''
\item
  Most graduate psychology programs are scientist/practitioner in nature and include training in ``high end'' statistics. Yet, many of your employing organizations will not have SPSS. R is a free, universally accessible program, that our graduates can use anywhere.
\end{itemize}

\hypertarget{strategies-for-success}{%
\section{stRategies for success}\label{strategies-for-success}}

\begin{itemize}
\tightlist
\item
  Engage with R, but don't let it overwhelm you.

  \begin{itemize}
  \tightlist
  \item
    The \emph{mechanical is also the conceptual}. Especially while it's \emph{simpler}, do try to retype the script into your own .rmd file and run it. Track down the errors you are making and fix them.
  \item
    If this stresses you out, move to simply copying the code into the .rmd file and running it. If you continue to have errors, you may have violated one of the best practices above (ask, ``Is the package activated?'' ``Are the data and .rmd files in the same place?'' ``Is all the prerequisite script run?'').
  \item
    Still overwhelmed? Keep moving forward by (retrieving the original.rmd file from the GitHub repository) opening a copy of the .rmd file and just ``run it along'' with the lecture. Spend your mental power trying to understand what each piece does so you can translate it for any homework assignments. My suggestions for practice aspire to be parallel to the lecture with no sneaky trix.
  \end{itemize}
\item
  Copy script that works elsewhere and replace it with your datafile, variables, and so forth.
\item
  The leaRning curve is steep, but not impossible. Gladwell \citeyearpar{gladwell_outliers_2008} taught us that it takes about 10,000 hours to get great at something (2,000 to get reasonably competent). Practice. Practice. Practice.
\item
  Updates to R, R Studio, and the packages are necessary, but can also be problematic. Sometimes updates cause programs/script to fail (e.g., ``X has been deprecated for version X.XX''). My personal practice is to update R, R Studio, and the packages a week or two before each academic term. I expect that prior scripts may need to be updated or revised with package updates and incongruencies between base R, R Studio, and the packages.
\item
  Embrace your downward dog. And square breathing. Also, walk away, then come back.
\end{itemize}

\hypertarget{resources-for-getting-started}{%
\section{Resources for getting staRted}\label{resources-for-getting-started}}

R for Data Science: \url{https://r4ds.had.co.nz/}

R Cookbook: \url{http://shop.oreilly.com/product/9780596809164.do}

R Markdown homepage with tutorials: \url{https://rmarkdown.rstudio.com/index.html}

R has cheatsheets for everything, here's the one for R Markdown: \url{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}

R Markdown Reference guide: \url{https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf}

Using R Markdown for writing reproducible scientific papers: \url{https://libscie.github.io/rmarkdown-workshop/handout.html}

Script for all of Field's text: \url{https://studysites.uk.sagepub.com/dsur/study/scriptfi.htm}

LaTeX equation editor: \url{https://www.codecogs.com/latex/eqneditor.php}

\#Preliminary Analyses \{-\}
\# Preliminary Results \{\#preliminaries\}

\href{}{Screencasted Lecture Link}

The beginning of any data analysis means familiarizing yourself with the data. Among other things, this includes producing and interpreting its distributional characteristics. In this lesson we mix common R operations for formatting, preparing, and analyzing the data with foundational statistical concepts in statistics.

\hypertarget{navigating-this-lesson-1}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-1}}

There is about \textbf{XX} minutes of lecture.

While the majority of R objects and data you will need are created within the R script that sources the lesson, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-1}{%
\subsection{Learning Objectives}\label{learning-objectives-1}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Determine the appropriate scale of measurement for variables and format them properly in R
\item
  Produce and interpret measures of central tendency
\item
  Analyze the distributional characteristics of data
\item
  Describe the steps in calculating a standard deviation.
\item
  Describe the steps in calculating a bivarite correlation coefficient (i.e., Pearson \emph{r}).
\item
  Create an APA Style table and results section that includes means, standard deviations, and correlations and addresses skew and kurtosis.
\end{itemize}

\hypertarget{planning-for-practice}{%
\subsection{Planning for Practice}\label{planning-for-practice}}

The practice assignment at the end of the lesson is designed as a ``get (or `get back') into it'' assignment. You will essentially work through this very same lecture, using the same dataframe; you will simply use a different set of continuous variables.

\hypertarget{readings-resources}{%
\subsection{Readings \& Resources}\label{readings-resources}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Revelle, W. (2021). An introduction to the psych package: Part I: data entry and data description. 60.

  \begin{itemize}
  \tightlist
  \item
    Revelle is the author/creator of the \emph{psych} package. His tutorial provides both technical and interpretive information. Read pages 1-17.
  \end{itemize}
\item
  Lui, P. P. (2020). Racial microaggression, overt discrimination, and distress: (In)Direct associations with psychological adjustment. \emph{The Counseling Psychologist, 32}.

  \begin{itemize}
  \tightlist
  \item
    This is the research vignette from which I simulate data that we can use in the lesson and practice problem.
  \end{itemize}
\end{itemize}

\hypertarget{research-vignette}{%
\section{Research Vignette}\label{research-vignette}}

We will use data that has been simulated data from Lui \citeyearpar{lui_racial_2020} as the research vignette. Controlling for overt discrimination, and neuroticism, Lui examined the degree to which racial microaggressions contributed to negative affect, alcohol consumption, and drinking problems in African American, Asian American, and Latinx American college students (\emph{N} = 713).

Using the means, standard deviations, correlation matrix, and group sizes (\emph{n}) I simulated the data. While the process of simulation is beyond the learning goals of this lesson (you can skip that part), I include it here so that it is easy to work the rest of the script.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210807}\NormalTok{)  }\CommentTok{\#sets the random seed so that we consistently get the same results}
\CommentTok{\# for practice, you could change (or remove) the random seed and try}
\CommentTok{\# to interpret the results (they should be quite similar) There are}
\CommentTok{\# probably more efficient ways to simulate data. Given the}
\CommentTok{\# information available in the manuscript, my approach was to first}
\CommentTok{\# create the datasets for each of the racial ethnic groups that were}
\CommentTok{\# provided and then binding them together.}

\CommentTok{\# First, the data for the students who identified as Asian American}
\NormalTok{Asian\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{1.52}\NormalTok{, }\FloatTok{1.72}\NormalTok{, }\FloatTok{2.69}\NormalTok{, }\FloatTok{1.71}\NormalTok{, }\FloatTok{2.14}\NormalTok{, }\FloatTok{2.35}\NormalTok{, }\FloatTok{2.42}\NormalTok{)}
\NormalTok{Asian\_stddev }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{2.52}\NormalTok{, }\FloatTok{2.04}\NormalTok{, }\FloatTok{0.47}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{2.41}\NormalTok{, }\FloatTok{3.36}\NormalTok{)}
\NormalTok{Asian\_corMat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.69}\NormalTok{, }\FloatTok{0.19}\NormalTok{, }\FloatTok{0.28}\NormalTok{, }\FloatTok{0.32}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.23}\NormalTok{, }\FloatTok{0.69}\NormalTok{, }\DecValTok{1}\NormalTok{,}
    \FloatTok{0.2}\NormalTok{, }\FloatTok{0.29}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.13}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.19}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.04}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.28}\NormalTok{,}
    \FloatTok{0.29}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.76}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.32}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.76}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\FloatTok{0.21}\NormalTok{,}
    \FloatTok{0.08}\NormalTok{, }\FloatTok{0.13}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.04}\NormalTok{, }\FloatTok{0.04}\NormalTok{, }\FloatTok{0.1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.42}\NormalTok{, }\FloatTok{0.23}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.18}\NormalTok{, }\FloatTok{0.21}\NormalTok{,}
    \FloatTok{0.42}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{7}\NormalTok{)}
\NormalTok{Asian\_covMat }\OtherTok{\textless{}{-}}\NormalTok{ Asian\_stddev }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Asian\_stddev) }\SpecialCharTok{*}\NormalTok{ Asian\_corMat}

\NormalTok{Asian\_dat }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{398}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ Asian\_mu, }\AttributeTok{Sigma =}\NormalTok{ Asian\_covMat,}
    \AttributeTok{empirical =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{Asian\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Asian\_dat)}

\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{Asian\_df }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(Asian\_df, }\AttributeTok{OvDisc =}\NormalTok{ V1, }\AttributeTok{mAggr =}\NormalTok{ V2, }\AttributeTok{Neuro =}\NormalTok{ V3, }\AttributeTok{nAff =}\NormalTok{ V4,}
    \AttributeTok{psyDist =}\NormalTok{ V5, }\AttributeTok{Alcohol =}\NormalTok{ V6, }\AttributeTok{drProb =}\NormalTok{ V7)}

\CommentTok{\# set upper and lower bound for each variable}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Asian\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Asian\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{mAggr[Asian\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{mAggr[Asian\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{Neuro[Asian\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{Neuro[Asian\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{nAff[Asian\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{nAff[Asian\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{psyDist[Asian\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{psyDist[Asian\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Asian\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Asian\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{drProb[Asian\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{drProb[Asian\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Asian\_df}\SpecialCharTok{$}\NormalTok{RacEth }\OtherTok{\textless{}{-}} \StringTok{"Asian"}

\CommentTok{\# Second, the data for the students who identified as Black/African}
\CommentTok{\# American}
\NormalTok{Black\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{4.45}\NormalTok{, }\FloatTok{3.84}\NormalTok{, }\FloatTok{2.6}\NormalTok{, }\FloatTok{1.84}\NormalTok{, }\FloatTok{2.1}\NormalTok{, }\FloatTok{2.81}\NormalTok{, }\FloatTok{2.14}\NormalTok{)}
\NormalTok{Black\_stddev }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{4.22}\NormalTok{, }\FloatTok{3.08}\NormalTok{, }\FloatTok{0.89}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{2.49}\NormalTok{, }\FloatTok{3.24}\NormalTok{)}
\NormalTok{Black\_corMat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.16}\NormalTok{, }\FloatTok{0.81}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\FloatTok{0.11}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.59}\NormalTok{, }\FloatTok{0.54}\NormalTok{, }\FloatTok{0.05}\NormalTok{,}
    \FloatTok{0.24}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\FloatTok{0.59}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.72}\NormalTok{, }\FloatTok{0.12}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.11}\NormalTok{, }\FloatTok{0.54}\NormalTok{, }\FloatTok{0.72}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.09}\NormalTok{, }\FloatTok{0.05}\NormalTok{, }\FloatTok{0.12}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.65}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.16}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.01}\NormalTok{,}
    \FloatTok{0.24}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.65}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{7}\NormalTok{)}
\NormalTok{Black\_covMat }\OtherTok{\textless{}{-}}\NormalTok{ Black\_stddev }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Black\_stddev) }\SpecialCharTok{*}\NormalTok{ Black\_corMat}
\NormalTok{Black\_dat }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{133}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ Black\_mu, }\AttributeTok{Sigma =}\NormalTok{ Black\_covMat,}
    \AttributeTok{empirical =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{Black\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Black\_dat)}
\NormalTok{Black\_df }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(Black\_df, }\AttributeTok{OvDisc =}\NormalTok{ V1, }\AttributeTok{mAggr =}\NormalTok{ V2, }\AttributeTok{Neuro =}\NormalTok{ V3, }\AttributeTok{nAff =}\NormalTok{ V4,}
    \AttributeTok{psyDist =}\NormalTok{ V5, }\AttributeTok{Alcohol =}\NormalTok{ V6, }\AttributeTok{drProb =}\NormalTok{ V7)}

\CommentTok{\# set upper and lower bound for each variable}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Black\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Black\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{mAggr[Black\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{mAggr[Black\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{Neuro[Black\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{Neuro[Black\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{nAff[Black\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{nAff[Black\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{psyDist[Black\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{psyDist[Black\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Black\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Black\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{drProb[Black\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{drProb[Black\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Black\_df}\SpecialCharTok{$}\NormalTok{RacEth }\OtherTok{\textless{}{-}} \StringTok{"Black"}

\CommentTok{\# Third, the data for the students who identified as Latinx American}
\NormalTok{Latinx\_mu }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{1.56}\NormalTok{, }\FloatTok{2.34}\NormalTok{, }\FloatTok{2.69}\NormalTok{, }\FloatTok{1.81}\NormalTok{, }\FloatTok{2.17}\NormalTok{, }\FloatTok{3.47}\NormalTok{, }\FloatTok{2.69}\NormalTok{)}
\NormalTok{Latinx\_stddev }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{2.46}\NormalTok{, }\FloatTok{2.49}\NormalTok{, }\FloatTok{0.86}\NormalTok{, }\FloatTok{0.71}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{2.59}\NormalTok{, }\FloatTok{3.76}\NormalTok{)}
\NormalTok{Latinx\_corMat }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.78}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.36}\NormalTok{, }\FloatTok{0.42}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.06}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.78}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.11}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.02}\NormalTok{, }\FloatTok{0.27}\NormalTok{, }\FloatTok{0.33}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.62}\NormalTok{, }\FloatTok{0.64}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.04}\NormalTok{,}
    \FloatTok{0.15}\NormalTok{, }\FloatTok{0.36}\NormalTok{, }\FloatTok{0.26}\NormalTok{, }\FloatTok{0.62}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.81}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.08}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{0.42}\NormalTok{, }\FloatTok{0.35}\NormalTok{, }\FloatTok{0.64}\NormalTok{, }\FloatTok{0.81}\NormalTok{,}
    \DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.06}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.06}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.11}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.04}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.08}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.06}\NormalTok{, }\DecValTok{1}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{0.02}\NormalTok{,}
    \FloatTok{0.15}\NormalTok{, }\FloatTok{0.17}\NormalTok{, }\FloatTok{0.15}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\DecValTok{1}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{7}\NormalTok{)}
\NormalTok{Latinx\_covMat }\OtherTok{\textless{}{-}}\NormalTok{ Latinx\_stddev }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(Latinx\_stddev) }\SpecialCharTok{*}\NormalTok{ Latinx\_corMat}
\NormalTok{Latinx\_dat }\OtherTok{\textless{}{-}}\NormalTok{ MASS}\SpecialCharTok{::}\FunctionTok{mvrnorm}\NormalTok{(}\AttributeTok{n =} \DecValTok{182}\NormalTok{, }\AttributeTok{mu =}\NormalTok{ Latinx\_mu, }\AttributeTok{Sigma =}\NormalTok{ Latinx\_covMat,}
    \AttributeTok{empirical =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{Latinx\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(Latinx\_dat)}
\NormalTok{Latinx\_df }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(Latinx\_df, }\AttributeTok{OvDisc =}\NormalTok{ V1, }\AttributeTok{mAggr =}\NormalTok{ V2, }\AttributeTok{Neuro =}\NormalTok{ V3, }\AttributeTok{nAff =}\NormalTok{ V4,}
    \AttributeTok{psyDist =}\NormalTok{ V5, }\AttributeTok{Alcohol =}\NormalTok{ V6, }\AttributeTok{drProb =}\NormalTok{ V7)}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Latinx\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{OvDisc[Latinx\_df}\SpecialCharTok{$}\NormalTok{OvDisc }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{mAggr[Latinx\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textgreater{}} \DecValTok{16}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{16}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{mAggr[Latinx\_df}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{Neuro[Latinx\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{Neuro[Latinx\_df}\SpecialCharTok{$}\NormalTok{Neuro }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{nAff[Latinx\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{4}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{nAff[Latinx\_df}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{psyDist[Latinx\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{psyDist[Latinx\_df}\SpecialCharTok{$}\NormalTok{psyDist }\SpecialCharTok{\textless{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Latinx\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{Alcohol[Latinx\_df}\SpecialCharTok{$}\NormalTok{Alcohol }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{drProb[Latinx\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textgreater{}} \DecValTok{12}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{12}
\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{drProb[Latinx\_df}\SpecialCharTok{$}\NormalTok{drProb }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{Latinx\_df}\SpecialCharTok{$}\NormalTok{RacEth }\OtherTok{\textless{}{-}} \StringTok{"Latinx"}

\NormalTok{Lui\_sim\_df }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(Asian\_df, Black\_df, Latinx\_df)}
\end{Highlighting}
\end{Shaded}

If you have simulated the data, you can continue using the the ``Lui\_sim\_df'' object that we created. In your own research you may wish to save data as a file. Although I will hashtag the code out (making it inoperable until the hashtags are removed), here is script to save the simulated data both .csv (think ``Excel lite'') and .rds (it retains all the properties we specified in R) files and then bring/import them back into R. For more complete instructions see the \protect\hyperlink{Ready}{Ready\_Set\_R} lesson.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Lui\_sim\_df,}
\CommentTok{\# file=\textquotesingle{}Lui\_CSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file df \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}Lui\_CSV.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Lui\_sim\_df, \textquotesingle{}Lui\_RDS.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file df \textless{}{-} readRDS(\textquotesingle{}Lui\_RDS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

You may have noticed a couple of things in each of these operations

\begin{itemize}
\tightlist
\item
  First, I named the data object to include a ``df'' (i.e., dataframe).

  \begin{itemize}
  \tightlist
  \item
    It is a common (but not required) practice for researchers to simply use ``df'' or ``dat'' as the name of the object that holds their data. This practice has advantages (e.g., as making the re-use of code quite easy across datasets) and disadvantages (e.g., it is easy to get confused about what data is being used).
  \end{itemize}
\item
  Second, when you run the code, any updating \emph{replaces} the prior object.

  \begin{itemize}
  \tightlist
  \item
    While this is irrelevant today (we are saving the same data with different names), it points out the importance of creating a sensible and systematic \emph{order of operations} in your .rmd files and then knowing where you are in the process.
  \end{itemize}
\end{itemize}

Because the data is simulated, I can simply use the data I created in the simulation, however, I will go ahead and use the convention of renaming it, ``df'', shich stands for \emph{dataframe} and is the common term for a dataset for users of R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ Lui\_sim\_df}
\end{Highlighting}
\end{Shaded}

\hypertarget{variable-types-scale-of-measurement}{%
\section{Variable Types (Scale of Measurement)}\label{variable-types-scale-of-measurement}}

Starting with raw data always begins with inspecting the data preparing it for the planned analyses. The \emph{type} of variables we have influences what statistics we choose to analyze our data. Further, the data must be formatted as that type in order for the statistic to properly execute. Variable types (or formats) are directly connected to the statistical concept of \emph{measurement scale} (or \emph{scale of measurement}). Researchers often thing of the \emph{categorical versus continuous} distinction, but it's even more nuanced than that.

\hypertarget{measurement-scale}{%
\subsection{Measurement Scale}\label{measurement-scale}}

\textbf{Categorical} variables name \emph{discrete} or \emph{distinct} entities where the categorization has no inherent value or order. When there are two categories, the variable type is \textbf{binary} (e.g., pregnant or not, treatment and control conditions). When there are more than two categories, the variable type is \textbf{nominal} (e.g., teacher, student, or parent; Republican, Democrat, or Independent).

\textbf{Ordinal} variables are technically categorical variables where the score reflects a logical order or relative rank (e.g., the order of finishing in a race). A challenge with the ordinal scale is the inability to determine the distance between rankings. The percentile rank is a (sometimes surprising) example of the ordinal scale. Technically, Likert type scaling (e.g., providing ratings on a 1-to-5 scale) is considered to be ordinal because it is uncertain that the distance between each of the numbers is equal. Practically, though, most researchers treat the Likert type scale as interval. This is facilitated, in part, because most Likert-type scales have multiple items which are averaged into a single score. Navarro\citeyearpar{navarro_book_2020} terms the Likert a \textbf{quasi-interval scale.}

\textbf{Continuous} variables can take on any value in the measurement scale that is being used. \textbf{Interval} level data have equal distances between each unit on the scale. Two classic examples of interval level data are temperature and year. Whether using Fahrenheit or Celsius, the rating of 0 does not mean there is an absence of temperature, rather, it is simply a number along a continuum of temperature. Another interval example is calendrical time. In longitudinal research, we frequently note the date or year (e.g., 2019) of an event. It is highly unlikely that the value zero will appear in our research and if it did, it would not represent the absence of time. A researcher can feel confident that a variable is on the interval scale if the values can be meaningfully added and subtracted.

\textbf{Ratio} level data also has equal distances between each unit on the scale, plus it has a true zero point where the zero indicates an absence. Examples are behavioral counts (e.g., cigarettes smoked) and time-on-task (e.g., 90 seconds). Ratio data offers more manipulative power because researchers can add, subtract, multiply, and divide ratio level data.

\hypertarget{corresponding-variable-structure-in-r}{%
\subsection{Corresponding Variable Structure in R}\label{corresponding-variable-structure-in-r}}

With these definitions in mind, we will see if R is reading our variables correctly. R will provide the following designations of variables:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1287}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4059}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3366}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Abbreviation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Unabbreviated
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Used for
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Scale of Measurement
\end{minipage} \\
\midrule()
\endhead
num & numerical & numbers that allow decimals or fractional values & quasi-interval, interval, or ratio \\
int & integer & whole numbers (no decimals) & quasi-interval, interval, or ratio \\
chr & character & sometimes termed ``string'' variables, these are interpreted as words & NA \\
Factor & factor & two or more categories; R imposes an alphabetical order; the user can re-specify the order based on the logic of the design & nominal \\
\bottomrule()
\end{longtable}

Looking back at the Lui \citeyearpar{lui_racial_2020} article we can determine what the scale of measurement is for each variable and what the corresponding R format for that variable should be:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0706}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2235}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3412}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2471}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1176}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
How assessed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Scale of measurement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
R format
\end{minipage} \\
\midrule()
\endhead
OvDis & Overt racial discrimination & 9 items, 1-to-4 Likert scaling for frequency and stressfulness assessed separately, then multiplied & quasi-interval & numerical \\
mAggr & Racial and ethnic microaggressions & 28 items, 1-to-4 Likert scaling for frequency and stressfulness assessed separately, then multiplied & quasi-interval & numerical \\
Neuro & Neuroticism & 4 items, 1-to-5 Likert scaling & quasi-interval & numerical \\
nAff & Negative affect & 6 items, 1-to-4 Likert scaling & quasi-interval & numerical \\
psyDist & Psychological distress & 6 items, 1-to-5 Likert scaling & quasi-interval & numerical \\
Alcohol & Hazardous alcohol use & 10 items, 0-to-4 Likert scaling & quasi-interval & numerical \\
drProb & Drinking problems & 10 items, 0-to-4 Likert scaling & quasi-interval & numerical \\
RacEth & Race Ethnicity & 3 categories & nominal & factor \\
\bottomrule()
\end{longtable}

We can examine the accuracy with which R interpreted the type of data with the \emph{structure()} command.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   713 obs. of  8 variables:
 $ OvDisc : num  1.45 4.9 0.45 2.85 2.09 ...
 $ mAggr  : num  0.682 4.383 0.225 2.235 1.977 ...
 $ Neuro  : num  3.11 3.69 3.5 2.68 2.08 ...
 $ nAff   : num  2.32 2.59 2.27 2.28 2.01 ...
 $ psyDist: num  1.83 3.41 2.75 2.11 3.12 ...
 $ Alcohol: num  3.125 4.388 0.999 0.137 0 ...
 $ drProb : num  2.5 0 3.04 0 0 ...
 $ RacEth : chr  "Asian" "Asian" "Asian" "Asian" ...
\end{verbatim}

Only Race/Ethnicity needs to be transformed from a character (``chr) variable to a factor. I will use the \emph{mutate()} function in the \emph{dplyr} package to convert the RacEth variable to be a factor with three levels.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# A .csv file is uninformed {-}{-} it just holds data (and R guesses what}
\CommentTok{\# it is); respecifying the type of variable will likely need to be}
\CommentTok{\# completed each time the file is used.}

\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{RacEth =} \FunctionTok{as.factor}\NormalTok{(RacEth))}
\end{Highlighting}
\end{Shaded}

Let's check the structure again. Below we see that the RacEth variable is now a factor. R has imposed an alphabetical order: Asian, Black, Latinx.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# checking the structure of the data}
\FunctionTok{str}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   713 obs. of  8 variables:
 $ OvDisc : num  1.45 4.9 0.45 2.85 2.09 ...
 $ mAggr  : num  0.682 4.383 0.225 2.235 1.977 ...
 $ Neuro  : num  3.11 3.69 3.5 2.68 2.08 ...
 $ nAff   : num  2.32 2.59 2.27 2.28 2.01 ...
 $ psyDist: num  1.83 3.41 2.75 2.11 3.12 ...
 $ Alcohol: num  3.125 4.388 0.999 0.137 0 ...
 $ drProb : num  2.5 0 3.04 0 0 ...
 $ RacEth : Factor w/ 3 levels "Asian","Black",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

It is possible to work with this data without restructuring them into a ``tiny df.'' However, this function is often one of the first skills we want to use so I will demonstrate it here.

\hypertarget{descriptive-statistics}{%
\section{Descriptive Statistics}\label{descriptive-statistics}}

While the majority of this OER (and statistics training in general) concerns the ability to make predictions or inferences (hence \emph{inferential statistics}) from data, we almost always begin data analysis by describing it (hence, \emph{descriptive statistics}).

Our research vignette contains a number of variables. Lui \citeyearpar{lui_racial_2020} was interested in predicting negative affect, alcohol consumption, and drinking problems from overt discrimination, microaggressions, neuroticism, through psychological distress. This research model is a \emph{mediation} model (or model of indirect effects) and is beyond the learning objectives of today's instruction. In demonstrating descriptive statistics, we will focus on one of the dependent variables: negative affect.

As we begin to explore the descriptive and distributional characteristics of this variable, it may be helpful to visualize it through a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\FloatTok{0.05}\NormalTok{), }\AttributeTok{border =} \StringTok{"white"}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Negative Affect"}\NormalTok{, }\AttributeTok{main =} \StringTok{""}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{ReCenterPsychStats_files/figure-latex/histogram1-1.pdf}
\caption{\label{fig:histogram1}A histogram of the negative affect variable.}
\end{figure}

\hypertarget{measures-of-central-tendency}{%
\subsection{Measures of Central Tendency}\label{measures-of-central-tendency}}

Describing data almost always begins with \emph{measures of central tendency}: the mean, median, and mode.

\hypertarget{mean}{%
\subsubsection{Mean}\label{mean}}

The \textbf{mean} is simply a mathematical average of the non-missing data. The mathematical formula is frequently expressed this way:

\[\bar{X} = \frac{X_{1} + X_{2} + X_{3}...+ X_{N}}{N}\]
Because such a formula is clumsy to write, there is statistical shorthand to help us convey it more efficiently (not necessarily, more easily).

Placing information below (where to start), above (where to stop), and to the right (what data to use) of the summation operator (\(\sum\)), provides instructions about the nature of the data. In the formula below, we learn from the notation to the right that we use the individual data in the vector X. We start with the first piece of data (\emph{i} = 1) and stop with the \emph{Nth} (or last) case.

\[\sum_{i=1}^{N}X_{i}\]
The \(\frac{1}{N}\) notation to the left tells us that we are calculating the mean.

\[\bar{X}=\frac{1}{N}\sum_{i=1}^{N}X_{i}\]
R is an incredible tool in that we can type out mathematical operations, use functions from base R, and use packages to do the work for us. If we had the following toy dataset (2, 3, 2, 1, 5, NA) we could calculate the mean by typing it out:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{2} \SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{+} \DecValTok{5}\NormalTok{)}\SpecialCharTok{/}\DecValTok{5}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.6
\end{verbatim}

Alternatively we could use the built-in functions in base R to do the work for us. Let me add a little complexity by creating a single variable (a vector of data) and introducing a little missingness (i.e., the ``NA'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toy }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{5}\NormalTok{, }\ConstantTok{NA}\NormalTok{)}
\NormalTok{toy }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(toy)}
\end{Highlighting}
\end{Shaded}

I can use the base R function \emph{mean()}. Inside the parentheses I point to the data. The function automatically sums the values. When there is missingness, adding \emph{na.rm=TRUE} tells the function to exclude the missing variables from the count (i.e., the denominator would still be 5).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.6
\end{verbatim}

Because of my simulation, we have no missing values, none-the-less, it is, perhaps a good habit to include the \emph{na.rm=TRUE} specification in our code. Becaue we have an entire dataframe, we just point to the dataframe and the specific variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.814666
\end{verbatim}

\hypertarget{median}{%
\subsubsection{Median}\label{median}}

The median of a set of values is the \textbf{median}. The easiest way to calculate the median is to sort the numbers:

\begin{longtable}[]{@{}lc@{}}
\toprule()
Unsorted & Sorted \\
\midrule()
\endhead
2, 3, 2, 1, 5, & 1, 2, 2, 3, 5 \\
\bottomrule()
\end{longtable}

And select the middle value. Because we have an odd number of values (\emph{N} = 5), our median is 2. If we had an even number of values, we would take the average of the middle two numbers.

We can use a base R function to calculate the median for us:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

Let's also calculate it for our negative affect variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.750954
\end{verbatim}

\hypertarget{mode}{%
\subsubsection{Mode}\label{mode}}

The \textbf{mode} is the score that occurs most frequently. When a histogram is available, spotting the mode is easy becaue it will have the tallest bar. Determining the mode can be made complicated if there are ties for high frequencies of values. A common occurence of this happens in the \textbf{bimodal} distribution.

Unfortunately, there is no base R function that will call a mode. In response, Navarro developed and included a function in the \emph{lsr} package that accompanies her \citeyearpar{navarro_book_2020} textbook. Once the package is installed, you can include two colons, the function name, and then the dataset to retrieve the mode.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{modeOf}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

From our toy data, we the \emph{modeOf()} function returns a 2.

Let's retrieve the mode from the negative affect variable in our research vignette.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{modeOf}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

The value is a 1.0 and is likely an artifact of how I simulated the data. Specifically, to ensure that the values fell within the 1-to-4 range, I rounded up to 1.0 any negative values and rounded down to 4.0 any values that were higher than 4.0.

\hypertarget{relationship-between-mean-median-and-mode}{%
\subsubsection{Relationship between mean, median, and mode}\label{relationship-between-mean-median-and-mode}}

Many inferential statistics rely on manipulations of the mean. The mean, though, can be misleading when it is influenced by outliers. Therefore, as we engage in preliminary exploration, it can be quite useful to calculate all three measures of central tendency, as well as exploring other distributional characteristics.

As a bit of an advanced cognitive organizer, it may be helpful to know that in a normal distribution, the mean, median, and mode are the same number (or quite close). In a positively skewed distribution, the mean is higher than the median which is higher than the mode. In a negatively skewed distribution, the mean is lower than the median, which is lower than the mode.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.814666
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.750954
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{modeOf}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

In our research vignette, the mean (1.81) is higher than the median (1.75) is higher than the mode (1.0). This would suggest a positive skew. Here is a reminder of our histogram:

\begin{figure}
\centering
\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-33-1.pdf}
\caption{\label{fig:unnamed-chunk-33}A histogram of the negative affect variable.}
\end{figure}

\hypertarget{variability}{%
\section{Variability}\label{variability}}

Researchers are equality interested in the spread or dispersion of the scores.

\hypertarget{range}{%
\subsection{Range}\label{range}}

The \textbf{range} is the simplest assessment of variability and is calculated by identifying the highest and lowest scores and subtracting the lowest from the highest. In our toy dataset, arranged from low-to-high (1, 2, 2, 3, 5 ) we see that the low is 1 and high is 5; 4 is the range. We can retrieve this data with three base R functions that ask for the minimum score, the maximum score, or both together -- the range:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{min}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 5
\end{verbatim}

The negative affect variable from our research vignette has the following range:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{min}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{max}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{range}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 4
\end{verbatim}

With a low of 1 and high of 4, the range of negative affect is 3. This is consistent with the description of the negative affect measure.

One limitation of the range is that it is easily influenced by extreme scores.

\hypertarget{percentiles-quantiles-interquartile-range}{%
\subsection{Percentiles, Quantiles, Interquartile Range}\label{percentiles-quantiles-interquartile-range}}

The \textbf{interquartile range} is middle 50\% of data, or the scores that fall between 25th and 75th percentiles. Before calculating that, let's first define \textbf{quantiles} and \textbf{percentiles}. \textbf{Quantiles} are values that split a data into equal portions. \textbf{Percentile} divide the data into 100 equal parts. Percentiles are commonly used in testing and assessment. You may have encountered them in standardized tests such as the SAT and GRE where both the score obtained and its associated percentile are reported. When graduate programs evaluate GRE scores, depending on their criteria and degree of competitiveness they may set a threshold based on percentiles (e.g., using a cut of of the 50th, 75th, or higher percentile for the verbal or quantitative GRE scores).

We have already learned the value of the median. The median is also the 50th percentile. We can now use the \emph{quantile()} function and indicate we want the value at the 50\% percentile.

Let's first examine the toy dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{probs =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
50% 
  2 
\end{verbatim}

As shown by our calculation, the value at the median and the 50th percentile is 2.0. Let's look at those values for the research vignette:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{median}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.750954
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{probs =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     50% 
1.750954 
\end{verbatim}

Again, we see the same result. Half of the values for negative affect are below 1.75; half are above.

The \emph{quantile()} function is extremely useful. We can retrieve the raw score at any percentile, and we could ask for as many as we desired. Here's an example.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     10%      20%      30%      40%      50%      60%      70%      80% 
1.000000 1.182154 1.402778 1.549423 1.750954 1.946013 2.142151 2.334597 
     90% 
2.688985 
\end{verbatim}

\textbf{Quartiles} divide the data into four equal parts. The \textbf{interquartile range} is the spread of data between the 25th and 75th percentiles (or quartiles). We calculate the interquartile range by first obtaining those values, and then subtracting the lower from the higher.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.75}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     25%      75% 
1.289916 2.241442 
\end{verbatim}

We see that a score of 1.29 is at the 25th percentile and a score of 2.24 is at the 75th percentile. If we subtract 1.29 from 2.24\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{2.24} \SpecialCharTok{{-}} \FloatTok{1.29}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.95
\end{verbatim}

\ldots we learn that the interquartile range is 0.95. We could also obtain this value by using the \emph{IQR()} function in base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{IQR}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9515256
\end{verbatim}

You may be asking, ``When would we use the interquartile range?'' When data are influenced by \textbf{outliers} (i.e., extreme scores), using a more truncated range (the middle 50\%, 75\%, 90\%) may be an option (if the dataset it large enough). At this point, though, the goal of this lesson is simply to introduce different ways of examining the variability in a dataset. Ultimately, we are working our way to the \textbf{standard deviation}. The next logical step is the \textbf{mean deviation.}

\hypertarget{deviations-around-the-mean}{%
\subsection{Deviations around the Mean}\label{deviations-around-the-mean}}

Nearly all statistics include assessments of variability in their calculation and most are based on deviations around the mean. In fact it might be good to pause for a moment and consider as the lessons in this OER (and those that follow) continue, we will be engaged in \emph{mathematical and statistical modeling}. In a featured article in the \emph{American Psychologist}, Rodgers \citeyearpar{rodgers_epistemology_2010} described models as a representation of reality that has two features:

\begin{itemize}
\tightlist
\item
  the model describes reality in some important ways, and
\item
  the model is simpler than reality.
\end{itemize}

Albeit one of the simplest, the mean is a statistical model. Rodgers noted this when he wrote, ``The mean and variance have done yeoman service to psychology and other behavioral sciences,'' \citeyearpar[p.~4]{rodgers_epistemology_2010}. These next statistical operations will walk through the use of the mean, particularly in its role in understanding variance. In later lessons, means and variances are used in understanding relations and differences.

A first step in understanding mean deviation is to ask, ``How far does each individual score deviates from the mean of scores?'' We can demonstrate this with our toy dataset. I am taking more steps than necessary to (a) make clear how the mean deviation (abbreviated, mdev) is calcuated and (b) practice using R.

First, I will create a variable representing the mean:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dissecting the script, each variable is referenced by}
\CommentTok{\# df\_name$variable\_name}
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{mean }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{head}\NormalTok{(toy)  }\CommentTok{\#displays the first 6 rows of the data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  toy mean
1   2  2.6
2   3  2.6
3   2  2.6
4   1  2.6
5   5  2.6
6  NA  2.6
\end{verbatim}

Next, I will subtract the mean from each individual score. The result

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{mdev }\OtherTok{\textless{}{-}}\NormalTok{ toy}\SpecialCharTok{$}\NormalTok{toy }\SpecialCharTok{{-}}\NormalTok{ toy}\SpecialCharTok{$}\NormalTok{mean}
\FunctionTok{head}\NormalTok{(toy)  }\CommentTok{\#displays the first 6 rows of the data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  toy mean mdev
1   2  2.6 -0.6
2   3  2.6  0.4
3   2  2.6 -0.6
4   1  2.6 -1.6
5   5  2.6  2.4
6  NA  2.6   NA
\end{verbatim}

The variable, \emph{mdev} (short for ``mean deviation'') lets us know how far the individual score is from the mean. Unfortunately, it does not provide an overall estimate of variation. Further, summing and averaging these values all result in zero. Take a look:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Dissecting the script, Wrapping the sum and mean script in \textquotesingle{}round\textquotesingle{}}
\CommentTok{\# and following with the desired decimal places, provides a rounde}
\CommentTok{\# result.}
\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

One solution is to create the \emph{mean absolute deviation}. We first transform the mean deviation score to their absolute values, and then sum them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{abslt\_m }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev)}
\FunctionTok{head}\NormalTok{(toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  toy mean mdev abslt_m
1   2  2.6 -0.6     0.6
2   3  2.6  0.4     0.4
3   2  2.6 -0.6     0.6
4   1  2.6 -1.6     1.6
5   5  2.6  2.4     2.4
6  NA  2.6   NA      NA
\end{verbatim}

And now to average them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{abslt\_m, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.12
\end{verbatim}

This value tells how far individual observations are from the mean, ``on average.'' In our toy dataset, the average distance from the mean is 1.12.

So that we can keep statistical notation in our mind, this is the formula calculating the absolute mean deviation:

\[\sum_{i=1}^{n}X_{i} - \bar{X}\]
Let's quickly repeat the process with the negative affect variable in our research vignette. So that we can more clearly see the relationship of the new variables to negative affect, let me create a df containing only nAff:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df\_nAff }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(nAff)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA }\OtherTok{\textless{}{-}}\NormalTok{ df\_nAff}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{df\_nAff}\SpecialCharTok{$}\NormalTok{abNAmdev }\OtherTok{\textless{}{-}} \FunctionTok{abs}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA)}
\FunctionTok{head}\NormalTok{(df\_nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      nAff     mdevNA  abNAmdev
1 2.316454  0.5017878 0.5017878
2 2.585344  0.7706780 0.7706780
3 2.274760  0.4600937 0.4600937
4 2.281637  0.4669707 0.4669707
5 2.005462  0.1907964 0.1907964
6 1.174359 -0.6403072 0.6403072
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{abNAmdev, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.521
\end{verbatim}

Thus, the absolute mean deviation for the negative affect variable in our research vignette is 0.521

Although relatively intuitive, the absolute mean deviation is not all that useful. Most statistics texts include it because it is one of the steps toward variance, and ultimately, the standard deviation.

\hypertarget{variance}{%
\subsection{Variance}\label{variance}}

Variance is considered to be an \emph{average} dispersion calculated by summing the squared deviations and dividing by the number of observations (less 1; more on that in later lessons).

Our next step is to square the mean deviations. This value is also called the \emph{sum of squared errors}, \emph{sum of squared deviations around the mean}, or \emph{sums of squares} and is abbreviated as \emph{SS}. Below are common statistical representations:

\[s^{2}=SS = \sum_{i=1}^{n}(X_{i} - \bar{X})^{^{2}}\]
Let's do it with our toy data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{mdev2 }\OtherTok{\textless{}{-}}\NormalTok{ (toy}\SpecialCharTok{$}\NormalTok{mdev) }\SpecialCharTok{*}\NormalTok{ (toy}\SpecialCharTok{$}\NormalTok{mdev)}
\FunctionTok{sum}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev2, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\#sum of squared deviations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  toy mean mdev abslt_m mdev2
1   2  2.6 -0.6     0.6  0.36
2   3  2.6  0.4     0.4  0.16
3   2  2.6 -0.6     0.6  0.36
4   1  2.6 -1.6     1.6  2.56
5   5  2.6  2.4     2.4  5.76
6  NA  2.6   NA      NA    NA
\end{verbatim}

Thus, our \emph{SS} (sums of squares or sums of squared errors) is 9.2.

To obtain the variance we divide by \emph{N} (or \emph{N} - 1; described in later lessons). Here are the updated formulas:

\[s^{2}=\frac{SS}{N-1}=\frac{\sum_{i=1}^{n}(X_{i} - \bar{X})^{^{2}}}{N-1}\]
Let's do this with the toy data:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.2}\SpecialCharTok{/}\NormalTok{(}\DecValTok{5} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)  }\CommentTok{\#calculated with the previously obtained values}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to obtain the \textquotesingle{}correct\textquotesingle{} calculation by using each of these}
\CommentTok{\# individual R commands, we need to have non{-}missing data}
\NormalTok{toy }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(toy)}
\FunctionTok{sum}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev2, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{/}\NormalTok{((}\FunctionTok{nrow}\NormalTok{(toy) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))  }\CommentTok{\#variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.3
\end{verbatim}

Of course R also has a function that will do all the steps for us:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.3
\end{verbatim}

The variance around the mean (2.6) of our toy data is 2.3.

Let's quickly repeat this process with the negative affect variable from the research vignette. In prior steps we had calculated the mean deviations by subtracting the mean from each individual score. Next we square the mean deviations\ldots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df\_nAff}\SpecialCharTok{$}\NormalTok{NAmd2 }\OtherTok{\textless{}{-}}\NormalTok{ (df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA) }\SpecialCharTok{*}\NormalTok{ (df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA)}
\FunctionTok{head}\NormalTok{(df\_nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      nAff     mdevNA  abNAmdev      NAmd2
1 2.316454  0.5017878 0.5017878 0.25179095
2 2.585344  0.7706780 0.7706780 0.59394458
3 2.274760  0.4600937 0.4600937 0.21168625
4 2.281637  0.4669707 0.4669707 0.21806160
5 2.005462  0.1907964 0.1907964 0.03640325
6 1.174359 -0.6403072 0.6403072 0.40999325
\end{verbatim}

\ldots{} and sum them.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{NAmd2, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)  }\CommentTok{\#sum of squared deviations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 283.4418
\end{verbatim}

Our sums of squared deviations around the mean is 283.44. When we divide it by \emph{N} - 1, we obtain the variance. We can check our work with (a) the values we calculated at each step, (b) the steps written in separate R code, and (c) the \emph{var()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{283.44}\SpecialCharTok{/}\NormalTok{(}\DecValTok{713} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3980899
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{NAmd2, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{/}\NormalTok{((}\FunctionTok{nrow}\NormalTok{(df\_nAff) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))  }\CommentTok{\#variance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3980924
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{var}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3980924
\end{verbatim}

Unfortunately, because the mean deviations were squared, this doesn't interpret well. Hence, we move to the \emph{standard deviation}.

\hypertarget{standard-deviation}{%
\subsection{Standard Deviation}\label{standard-deviation}}

The standard deviation is simply the square root of the variance. Stated another way, it is an estimate of the average spread of data, presented in the same metric as the data.

Calculating the standard deviation requires earlier steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Calculating the mean.
\item
  Calculating mean deviations by subtracting the mean from each individual score.
\item
  Squaring the mean deviations.
\item
  Summing the mean deviations to create the \emph{SS}, or sums of squares.
\item
  Dividing the \emph{SS} by \emph{N} - 1; this results in the \emph{variance} around teh mean.
\end{enumerate}

The 6th step is to take the square root of variance. It is represented in the formulae, below:

\[s=\sqrt{\frac{SS}{N-1}}=\sqrt{\frac{\sum_{i=1}^{n}(X_{i} - \bar{X})^{^{2}}}{N-1}}\]
Repeated below are each of the six steps for the toy data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# six steps wrapped into 1}
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{mdev }\OtherTok{\textless{}{-}}\NormalTok{ toy}\SpecialCharTok{$}\NormalTok{toy }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{toy}\SpecialCharTok{$}\NormalTok{mdev2 }\OtherTok{\textless{}{-}}\NormalTok{ (toy}\SpecialCharTok{$}\NormalTok{mdev) }\SpecialCharTok{*}\NormalTok{ (toy}\SpecialCharTok{$}\NormalTok{mdev)}
\CommentTok{\# I can save the variance calculation as an object for later use}
\NormalTok{toy\_var }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{mdev2)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(toy) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\CommentTok{\# checking work with the variance function}
\FunctionTok{var}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.3
\end{verbatim}

The seventh step is to take the square root of variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grabbing the mean for quick reference}
\FunctionTok{mean}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# below the \textquotesingle{}toy\_var\textquotesingle{} object was created in the prior step}
\FunctionTok{sqrt}\NormalTok{(toy\_var)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.516575
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# checking work with the R function to calculate standard deviation}
\FunctionTok{sd}\NormalTok{(toy}\SpecialCharTok{$}\NormalTok{toy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.516575
\end{verbatim}

It is common to report means and standard deviations for continuous variables in our datasets. For the toy data our mean is 2.6 with a standard deviation of 1.51.

Let's repeat the process for the negative affect variable in the research vignette. First the six steps to calculate variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# six steps wrapped into 1}
\NormalTok{df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA }\OtherTok{\textless{}{-}}\NormalTok{ df\_nAff}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{df\_nAff}\SpecialCharTok{$}\NormalTok{NAmd2 }\OtherTok{\textless{}{-}}\NormalTok{ (df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA) }\SpecialCharTok{*}\NormalTok{ (df\_nAff}\SpecialCharTok{$}\NormalTok{mdevNA)}
\CommentTok{\# I can save the variance calculation as an object for later use}
\NormalTok{nAff\_var }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{NAmd2)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\CommentTok{\# checking work with the variance function}
\FunctionTok{var}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3980924
\end{verbatim}

The seventh step is to take the square root of variance.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# grabbing the mean for quick reference}
\FunctionTok{mean}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.814666
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# below the \textquotesingle{}toy\_var\textquotesingle{} object was created in the prior step}
\FunctionTok{sqrt}\NormalTok{(nAff\_var)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6309456
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# checking work with the R function to calculate standard deviation}
\FunctionTok{sd}\NormalTok{(df\_nAff}\SpecialCharTok{$}\NormalTok{nAff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6309456
\end{verbatim}

In APA Style we use \emph{M} and \emph{SD} as abbreviations for mean and standard deviation, respectively. In APA Style, non-Greek statistical symbols such as these are italicized. Thus we could include \emph{M} = 1.81(\emph{SD} = 0.63) in a statistical string of results.

We can examine the standard deviation in relation to its mean to understand how narrowly or broadly the data is distributed. Relative to a same-sized mean, a small standard deviation means that the mean represents the data well. A larger standard deviation, means there is more variability and the mean, alone, is a less valid representation of the score.

In later lessons we will explore the standard deviation in more detail -- learning how we can use it in the determination of the significance and magnitude of relations and differences.

\hypertarget{are-the-variables-normally-distributed}{%
\section{Are the Variables Normally Distributed?}\label{are-the-variables-normally-distributed}}

Statistics that we use are accompanied by assumptions about the nature of variables in the dataset. A common assumption is that the data are \emph{normally distributed}. That is, the data presumes a standard normal curve.

Skew and kurtosis are indicators of non-normality. Skew refers to the degree to which the data is symmetrical. In the figure below, the symmetrical distribution in the center (the black line) would have no skewness. In contrast, the red figure whose curve (representing a majority of observations) in the left-most part of the graph (with the tail pulling to the right) would be positively skewed; the blue figure whose curve (representing a majority of cases) is in the right-most part of the graph (with the tail pulling to the left) would be negatively skewed.

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-60-1.pdf}
Kurtosis refers to the degree to which the distribution of data is flat or peaked. Mesokurtic distributions are considered to be closest to normal. Leptokurtic distributions are peaked and platykurtic distributions are flat.

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-61-1.pdf}
The \emph{describe()} function in the \emph{psych} package is one of the most common ways to obtain information on skew and kurtosis, as well as other descriptive information.

For just a quick view, call the psych package, add two colons, use the \emph{describe()} function, and indicate the df.

For a simplified presentation, let me create a df with three variables of interest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# I have opened the tidyverse library so that I can use the pipe}
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{df\_3vars }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(nAff, mAggr, drProb)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(df\_3vars)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       vars   n mean   sd median trimmed  mad min   max range skew kurtosis
nAff      1 713 1.81 0.63   1.75    1.76 0.71   1  4.00  3.00 0.58    -0.18
mAggr     2 713 2.49 2.20   2.17    2.22 2.42   0 10.42 10.42 0.93     0.58
drProb    3 713 2.92 2.77   2.40    2.58 3.49   0 11.69 11.69 0.78    -0.17
         se
nAff   0.02
mAggr  0.08
drProb 0.10
\end{verbatim}

If you will be needing to create a table of information in another application such as Word or Excel, save your descriptives as an object and then write them to a .csv file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LuiDescripts }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(df\_3vars)}
\FunctionTok{write.csv}\NormalTok{(LuiDescripts, }\AttributeTok{file =} \StringTok{"LuiDescripts.csv"}\NormalTok{)}
\CommentTok{\# Once you create an object, the result won\textquotesingle{}t automatically display;}
\CommentTok{\# to see it simply type the name of the object}
\NormalTok{LuiDescripts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       vars   n mean   sd median trimmed  mad min   max range skew kurtosis
nAff      1 713 1.81 0.63   1.75    1.76 0.71   1  4.00  3.00 0.58    -0.18
mAggr     2 713 2.49 2.20   2.17    2.22 2.42   0 10.42 10.42 0.93     0.58
drProb    3 713 2.92 2.77   2.40    2.58 3.49   0 11.69 11.69 0.78    -0.17
         se
nAff   0.02
mAggr  0.08
drProb 0.10
\end{verbatim}

To understand whether our data is normally distributed, we can look at skew and kurtosis.The skew and kurtosis indices in the \emph{psych} package are reported as \emph{z} scores. Positive skew values indicate positive skew; negative skew values represent negative skew. Skew values greater than 3.0 are generally considered to be ``severely skewed'' \citep{kline_principles_2016}.

Regarding kurtosis, positive values indicate the distribution is somewhat more peaked than a normal distribution (i.e., more leptokurtic); negative values indicate the distribution is flatter than a normal distribution (i.e., more platykurtic). Regarding kurtosis, ``severely kurtotic'' is argued anywhere \textgreater{} 8 to 20 \citep{kline_principles_2016}.

The values for all the variables in the research vignette are well below the regions of concern indicated by Kline \citeyearpar{kline_principles_2016}

\hypertarget{relations-between-variables}{%
\section{Relations between Variables}\label{relations-between-variables}}

Preliminary investigation of data almost always includes a report of their bivariate relations, or correlations. Correlation coefficients express the magnitude of relationships on a scale ranging from -1 to +1. A correlation coefficient of

\begin{itemize}
\tightlist
\item
  -1.0 implies a 1:1 inverse relationship, such that for every unit of increase in variable A, there is a similar decrease in variable B,
\item
  0.0 implies no correspondence between two variables,
\item
  1.0 implies that as A increases by one unit, so does B.
\end{itemize}

Correlation coefficients are commonly represented in two formulas. In a manner that echoes the calculation of \emph{variance}, the first part of the calculation estimates the covariation (i.e., \emph{covariance}) of the two variables of interest. The problem is that the result is unstandardized and difficult to interpret.

\[
\mbox{Cov}(X,Y) = \frac{1}{N-1} \sum_{i=1}^N \left( X_i - \bar{X} \right) \left( Y_i - \bar{Y} \right)
\]

The second significant calculation results in the standardization of the metric in the -1 to +1 scale.
\[
r_{XY}  = \frac{\mbox{Cov}(X,Y)}{ \hat{\sigma}_X \ \hat{\sigma}_Y}
\]
Covariation and correlation matrices are central to many of our statistics therefore, those of who teach statistics believe that it is important to take a look ``under the hood.'' From our research vignette, let's calculate the relationship between negative affect and psychological distress.

Examining the first formula, some parts should look familiar:

\begin{itemize}
\tightlist
\item
  \((X_i - \bar{X})\): We can see that we need to subtract the mean from the first(X) variable in involved in the correlation; we saw this when we calculated \emph{mean deviations}.
\item
  \((Y_i - \bar{Y})\): We repeat the \emph{mean deviation} process for the second (Y) variable.
\end{itemize}

Let's work this much of the problem. So that we can more easily see what we are doing with the variables, I will create a super tiny dataframe with the two variables of interest (negative affect and microaggressions):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# just in case it turned off, I\textquotesingle{}m reopening tidyverse so that I can}
\CommentTok{\# use the pipe ($\textgreater{}$)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# using the dplyr package to select the two variables in this tiny df}
\NormalTok{df4corr }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(nAff, mAggr)}
\CommentTok{\# displaying the first 6 rows of df4corr (\textquotesingle{}dataframe for}
\CommentTok{\# correlations\textquotesingle{} {-}{-} I made this up)}
\FunctionTok{head}\NormalTok{(df4corr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      nAff     mAggr
1 2.316454 0.6822586
2 2.585344 4.3834353
3 2.274760 0.2251289
4 2.281637 2.2351541
5 2.005462 1.9765313
6 1.174359 0.0000000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating the mean deviation for negative affect}
\NormalTok{df4corr}\SpecialCharTok{$}\NormalTok{MDnAff }\OtherTok{\textless{}{-}}\NormalTok{ df4corr}\SpecialCharTok{$}\NormalTok{nAff }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{nAff)}
\CommentTok{\# calculating the mean deviation for microaggressions}
\NormalTok{df4corr}\SpecialCharTok{$}\NormalTok{MDmAggr }\OtherTok{\textless{}{-}}\NormalTok{ df4corr}\SpecialCharTok{$}\NormalTok{mAggr }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{mAggr)}
\CommentTok{\# displaying the first 6 rows of df4corr}
\FunctionTok{head}\NormalTok{(df4corr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      nAff     mAggr     MDnAff    MDmAggr
1 2.316454 0.6822586  0.5017878 -1.8033045
2 2.585344 4.3834353  0.7706780  1.8978722
3 2.274760 0.2251289  0.4600937 -2.2604342
4 2.281637 2.2351541  0.4669707 -0.2504090
5 2.005462 1.9765313  0.1907964 -0.5090318
6 1.174359 0.0000000 -0.6403072 -2.4855631
\end{verbatim}

The next part of the formula \(\sum_{i=1}^N \left( X_i - \bar{X} \right) \left( Y_i - \bar{Y} \right)\) suggests that we sum the cross-products of these mean deviations. That is, first we multiply the mean deviations and then sum them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating a crossproduct variabl by multiplying negative affect by}
\CommentTok{\# psych distress}
\NormalTok{df4corr}\SpecialCharTok{$}\NormalTok{crossproductXY }\OtherTok{\textless{}{-}}\NormalTok{ df4corr}\SpecialCharTok{$}\NormalTok{MDnAff }\SpecialCharTok{*}\NormalTok{ df4corr}\SpecialCharTok{$}\NormalTok{MDmAggr}
\CommentTok{\# displaying the first 6 rows of df4corr}
\FunctionTok{head}\NormalTok{(df4corr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      nAff     mAggr     MDnAff    MDmAggr crossproductXY
1 2.316454 0.6822586  0.5017878 -1.8033045    -0.90487609
2 2.585344 4.3834353  0.7706780  1.8978722     1.46264835
3 2.274760 0.2251289  0.4600937 -2.2604342    -1.04001163
4 2.281637 2.2351541  0.4669707 -0.2504090    -0.11693366
5 2.005462 1.9765313  0.1907964 -0.5090318    -0.09712141
6 1.174359 0.0000000 -0.6403072 -2.4855631     1.59152382
\end{verbatim}

Next, we sum the column of cross-products.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{crossproductXY)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 265.9915
\end{verbatim}

To obtain the covariance, adding the next part of the formula suggests that we multiply the sum of cross-products \(\frac{1}{N-1}\). I will do this in one step.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# I have created the object \textquotesingle{}cov\textquotesingle{} so I can use it in a calculation,}
\CommentTok{\# later}
\NormalTok{cov }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df4corr) }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{sum}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{crossproductXY)}
\CommentTok{\# Because I created an object, R markdown won\textquotesingle{}t automatically display}
\CommentTok{\# it; I have to request it by listing it}
\NormalTok{cov}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3735836
\end{verbatim}

The covariance between negative affect and psychological distress is 0.353.

We now engage the second part of the formula to standardize it.

\[
r_{XY}  = \frac{\mbox{Cov}(X,Y)}{ \hat{\sigma}_X \ \hat{\sigma}_Y}
\]
We will use our covariance value in the numerator. The denominator involves the multiplication of the standard deviations of X and Y. Because we have already learned how to calculate standard deviation in a step-by-step manner, I will use code to simplify that process:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cov}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sd}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{nAff) }\SpecialCharTok{*} \FunctionTok{sd}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{mAggr))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2690291
\end{verbatim}

Our results suggest that the relationship between negative affect and psychological distress is positive, as one increases so does the other. Is it strong? That really depends on what you are studying. The traditional values of .10, .30, and .50 are used as small, medium, and large \citep{cohen_applied_2003}. Hence, when \emph{r} = 0.27, we can say that it is (more-or-less) medium.

Is it statistically significant? Because this is an introductory chapter, we will not calculate this in a stepwise manner, but use the \emph{cor()} function in base R to check our math and retrieve the \emph{p} value associated with the correlation coefficient.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(df4corr}\SpecialCharTok{$}\NormalTok{nAff, df4corr}\SpecialCharTok{$}\NormalTok{mAggr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  df4corr$nAff and df4corr$mAggr
t = 7.4481, df = 711, p-value = 0.0000000000002749
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.1995470 0.3358194
sample estimates:
      cor 
0.2690291 
\end{verbatim}

In a statistical string we would report the result of this Pearson correlation coefficient as: \emph{r} = 0.27 (\emph{p} \textless{} .001).

\hypertarget{shortcuts-to-preliminary-analyses}{%
\section{Shortcuts to Preliminary Analyses}\label{shortcuts-to-preliminary-analyses}}

Unless you teach statistics (or take another statistics class), you may never need to work through all those individual steps again. Rather, a number of R packages make retrieval of these values simple and efficient.

\hypertarget{splom}{%
\subsection{SPLOM}\label{splom}}

The \emph{pairs.panels()} function in the \emph{psych} package produces a SPLOM (i.e., scatterplot matrix) which includes:

\begin{itemize}
\tightlist
\item
  histograms of each individual variable within the dataframe with a curve superimposed (located on the diagonal),
\item
  scatterplots of each bivariate combination of variables (located below the diagonal), and
\item
  corrrelation coefficients of each bivariate combination of variables (located above the diagonal).
\end{itemize}

To provide a simple demonstration this, I will use our df with three variables of interest:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# in the code below, psych points to the package pairs.panels points}
\CommentTok{\# to the function we simply add the name of the df; if you want fewer}
\CommentTok{\# variables than that are in the df, you may wish to create a smaller}
\CommentTok{\# df adding the pch command is optional and produces a finer}
\CommentTok{\# resolution}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{pairs.panels}\NormalTok{(df\_3vars, }\AttributeTok{pch =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-72-1.pdf}
What do we observe?

\begin{itemize}
\tightlist
\item
  There is a more-or-less moderate correlation between negative affect and microaggressions (\emph{r} = 0.27)
\item
  There is a small-to-moderate correlation between negative affect and drinking problems (\emph{r} = 0.18)
\item
  There is a small correlation between microaggressions and drinking problems (\emph{r} = 0.09)
\item
  All variables have a positive skew (with pile-up of scores on the lower end and tail pulling to the right)
\item
  The scatterplots can provide clues to relations that are not necessarily linear.

  \begin{itemize}
  \tightlist
  \item
    Look at the relationship between negative affect and drinking problems. As negative affect hits around 2.75, there is a change in the relationship, such that drinking problems increase.
  \item
    Taking time to look at plots such as these can inform subsequent analyses.
  \end{itemize}
\end{itemize}

\hypertarget{apatables}{%
\subsection{apaTables}\label{apatables}}

Writing up an APA style results section frequently involves tables. A helpful package for doing this is \emph{apaTables}. An instructional article notes the contributions of tools like this contributing to the \emph{reproducibility} of science by reducing errors made when the author or analyst retypes or copies text from output to the manuscript. When the R script is shared through an open science framework, reproducibility is further enhanced \citep{stanley_reproducible_2018}.

We pass the desired df to the \emph{apa.cor.table()} function of the \emph{apaTables} package. Commands allow us to specify what is included in the table and whether it should be displayed in the console or saved as a document to the project's folder.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the apa.cor.table function removes any categorical variables that}
\CommentTok{\# might be in the df}
\NormalTok{Table1\_Cor }\OtherTok{\textless{}{-}}\NormalTok{ apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(df\_3vars, }\AttributeTok{filename =} \StringTok{"Table1\_Cor.doc"}\NormalTok{,}
    \AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{show.conf.interval =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
The ability to suppress reporting of reporting confidence intervals has been deprecated in this version.
The function argument show.conf.interval will be removed in a later version.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# swap in this command to see it in the R Markdown file}
\FunctionTok{print}\NormalTok{(Table1\_Cor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable  M    SD   1          2         
  1. nAff   1.81 0.63                      
                                           
  2. mAggr  2.49 2.20 .27**                
                      [.20, .34]           
                                           
  3. drProb 2.92 2.77 .18**      .09*      
                      [.11, .25] [.01, .16]
                                           

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

Because I added: \emph{filename = ``Table1\_Cor.doc''}, a word version of the table will appear in the same file folder as the .rmd file and data. It is easily manipulated with tools in your word processing package.

\hypertarget{an-apa-style-writeup}{%
\section{An APA Style Writeup}\label{an-apa-style-writeup}}

The statistics used in this lesson are often presented in the preliminary results portion of an empirical manuscript. Some of the results are written in text and some are presented in tables. APA Style recommends that the narration of results not duplicate what is presented in the tables. Rather, the write-up only highlights and clarifies what is presented in the table(s).

At the outset, let me note that a primary purpose of the Lui \citeyearpar{lui_racial_2020} article was to compare the relations of variables between three racial/ethic groups in the U.S. identified as Asian American, Black, an Latinx. Because we did not run separate analyses for each of the groups, my write-up does not make these distinctions. I highly recommend that you examine the write-up of results and the accompanying tables in Lui's article. The presentation is clear and efficient (i.e., takes up as little space as possible).

Below is an example of how I might write up these preliminary results:

\textbf{Preliminary Results}

\begin{quote}
Our sample included 713 participants who self-identified as Asian American, Black/African American, and Latinx American. Visual inspection of the three variables of interest (negative affect, microaggressions, drinking problems) combined with formal evaluation of skewness and kurtosis suggested that their distributions did not violate the assumption of univariate normality. Means, standard deviations, and a correlation matrix are presented in Table 1. We noted that he correlation between negative affect and microaggressions was moderate (\emph{r} - 0.29); correlations between remaining variables were smaller.
\end{quote}

\hypertarget{practice-problems}{%
\section{Practice Problems}\label{practice-problems}}

The three exercises described below are designed to ``meet you where you are'' and allow you to challenge your skills depending on your goals as well as your comfort with statistics and R.

Regardless which you choose, you should:

\begin{itemize}
\tightlist
\item
  Create a smaller df from a larger df selecting only continuously scaled variables
\item
  Calculate and interpret descriptives
\item
  Create the SPLOM (pairs.panels)
\item
  Use the \emph{apaTables} package to make an APA style table with means, standard deviations, and correlations
\item
  Write up an APA Style results section for these preliminary analyses
\end{itemize}

\hypertarget{problem-1-change-the-random-seed}{%
\subsection{Problem \#1: Change the Random Seed}\label{problem-1-change-the-random-seed}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation (at the very top), then rework the lesson exactly as written. This should provide minor changes to the data (maybe in the second or third decimal point), but the results will likely be very similar.

\hypertarget{problem-2-swap-variables-in-the-simulation}{%
\subsection{Problem \#2: Swap Variables in the Simulation}\label{problem-2-swap-variables-in-the-simulation}}

Use the simulated data from the Lui \citeyearpar{lui_racial_2020} study. However, select three continuous variables (2 must be different from mine) and then conduct the analyses. Be sure to select from the variables that are considered to be \emph{continuous} (and not \emph{categorical}).

\hypertarget{problem-3-use-or-simulate-your-own-data}{%
\subsection{Problem \#3: Use (or Simulate) Your Own Data}\label{problem-3-use-or-simulate-your-own-data}}

Use data for which you have permission and access. This could be IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; or data from other chapters in this OER.

\hypertarget{grading-rubric}{%
\subsection{Grading Rubric}\label{grading-rubric}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5775}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2113}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Poss
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Create a df with 3 continuously scaled variables of interest & 3 & \\
2. Produce descriptive statistics & 3 & \\
3. Produce SPLOM/pairs.panels & 3 & \\
4. Produce an apaTables matrix & 3 & \\
5. Produce an APA Style write-up of the preliminary analyses & 5 & \\
6. Explanation/discussion with a grader & 5 & \\
**Totals & 22 & \\
\bottomrule()
\end{longtable}

\hypertarget{t-tests}{%
\chapter*{\texorpdfstring{\emph{t}-tests}{t-tests}}\label{t-tests}}
\addcontentsline{toc}{chapter}{\emph{t}-tests}

The lessons offered in the \emph{t}-tests section introduce \emph{inferential statistics}. In the prior chapters, our use of measures of central tendency (i.e., mean, median, mode) and variance (i.e., range, variance, standard deviation) were merely about to \emph{describe} a sample.

As we move into \emph{inferential} statistics we evaluate data from a sample and try to determine whether or not we can use it to draw conclusions (i.e, predict or make inferences) about a larger, defined, population.at

The \emph{t}-test lessons begin with an explanation of the \emph{z}-score and progress through one sample, independent samples, and paired samples \emph{t}-tests. Each lesson is centered around a research vignette that was focused on physicians' communication with patients who were critically and terminally ill and in the intensive care unit at a hospial \citep{elliott_differences_2016}.

In addition to a conceptual presentation of of each statistic, each lesson includes:

\begin{itemize}
\tightlist
\item
  a workflow that guides researchers through decision-points in each statistic,
\item
  the presentation of formulas and R code for ``hand-calculating'' each component of the formula,
\item
  script for efficiently computing the statistic with R packages,
\item
  an ``recipe'' for an APA style presentation of the results,
\item
  a discussion of \emph{power} in that particular statistic with R script for calculating sample sizes sufficient to reject the null hypothesis, if in fact, it is appropriate to do so, and
\item
  suggestions for practice that vary in degree of challenge.
\end{itemize}

\hypertarget{tOneSample}{%
\chapter{\texorpdfstring{One Sample \emph{t}-tests}{One Sample t-tests}}\label{tOneSample}}

\href{link\%20here}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{)  }\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

Researchers, and perhaps especially those engaged in program evaluation, may wish to know if their data differs from an external standard. In today's research vignette, we will ask if the time physicians spent with their patients differed from what was reported nationally. The one-sample \emph{t}-test is an appropriate tool for this type of analysis.

\hypertarget{navigating-this-lesson-2}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-2}}

There is about \# hour and \#\# minutes of lecture. If you work through the materials with me it would be plan for an additional TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-2}{%
\subsection{Learning Objectives}\label{learning-objectives-2}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Recognize the research questions for which utilization of a one sample \emph{t}-test would be appropriate.
\item
  Narrate the steps in conducting a one-sample \emph{t}-test, beginning with testing the statistical assumptions through writing up an APA style results section.
\item
  Calculate a one-sample \emph{t}-test in R (including effect sizes).
\item
  Interpret a 95\% confidence interval around a mean difference score.
\item
  Produce an APA style results for a one-sample \emph{t}-test .
\item
  Determine a sample size that (given a set of parameters) would likely result in a statistically significant effect, if there was one.
\end{itemize}

\hypertarget{planning-for-practice-1}{%
\subsection{Planning for Practice}\label{planning-for-practice-1}}

The suggestions for homework are graded in complexity. The more complete descriptions at the end of the chapter follow these suggestions.

\begin{itemize}
\tightlist
\item
  Rework the one-sample \emph{t}-test in the lesson by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar.
\item
  Rework the one-sample \emph{t}-test in the lesson by changing something else about the simulation. For example, if you are interested in power, consider changing the sample size.
\item
  Conduct a one sample \emph{t}-test with data to which you have access and permission to use. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources-1}{%
\subsection{Readings \& Resources}\label{readings-resources-1}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Navarro, D. (2020). Chapter 13: Comparing two means. In \href{https://learningstatisticswithr.com/}{Learning Statistics with R - A tutorial for Psychology Students and other Beginners}. Retrieved from \url{https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book\%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)}

  \begin{itemize}
  \tightlist
  \item
    Navarro's OER includes a good mix of conceptual information about \emph{t} tests as well as R code. My lesson integrates her approach as well as considering information from Field's \citeyearpar{field_discovering_2012} and Green and Salkind's \citeyearpar{green_using_2014} texts (as well as searching around on the internet).
  \end{itemize}
\item
  Elliott, A. M., Alexander, S. C., Mescher, C. A., Mohan, D., \& Barnato, A. E. (2016). Differences in Physicians' Verbal and Nonverbal Communication With Black and White Patients at the End of Life. \emph{Journal of Pain and Symptom Management, 51}(1), 1--8. \url{https://doi.org/10.1016/j.jpainsymman.2015.07.008}

  \begin{itemize}
  \tightlist
  \item
    The source of our research vignette.
  \end{itemize}
\end{itemize}

\hypertarget{packages}{%
\subsection{Packages}\label{packages}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them. Remove the hashtags for the code to work.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(faux))\{install.packages(\textquotesingle{}faux\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(dplyr))\{install.packages(\textquotesingle{}dplyr\textquotesingle{})\}}
\CommentTok{\# if(!require(lsr))\{install.packages(\textquotesingle{}lsr\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{z-before-t}{%
\section{\texorpdfstring{\emph{z} before \emph{t}}{z before t}}\label{z-before-t}}

\textbf{Probability density functions} are mathematical formula that specifies idealized versions of known distributions. The equations that define these distributions allow us to calculate the probability of obtaining a given score. This is a powerful tool.

As students progress through statistics, they become familiar with a variety of these distributions including the \emph{t}-distribution (commonly used in \emph{t}-tests), \emph{F}-distribution (commonly used in analysis of variance {[}ANOVA{]}), and Chi-square (\(X^2\)) distributions (used in a variety of statistics, including structural equation modeling). The \emph{z} distribution is the most well-known of these distributions.

The \emph{z} distribution is also known as the normal distribution, the bell curve, or the standard normal distribution. Its mean is always 0.00 and its standard deviation is always 1.00. Regardless of what the actual mean and standard deviation are

\begin{itemize}
\tightlist
\item
  68.3\% of the area falls within 1 standard deviation of the mean
\item
  95.4\% of the distribution falls within 2 standard deviations of the mean
\item
  99.7\% of the distribution falls within 3 standard deviations of the mean
\end{itemize}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-78-1.pdf}
\emph{z}-scores are transformations of raw scores, in standard deviation units. Using the following formula, so long as the mean and standard deviation are known, any set of continuously scaled scores can be transformed to a \emph{z}-scores equivalent:

\[z=\frac{X-\bar{X}}{s}\]
We can rearrange the formula to find what raw score corresponds with the \emph{z}-score.

\[X = \bar{X} + z(s)\]

These powerful tools allow us to make inferences about the data.

\hypertarget{simulating-a-mini-research-vignette}{%
\subsection{Simulating a Mini Research Vignette}\label{simulating-a-mini-research-vignette}}

Later in this larger section on \emph{t}-tests we introduce a research vignette that focuses on time physicians spend with patients. Because working with the \emph{z}-test requires a minimum sample size of 120 (and the research vignette has a sample size of 33), I will quickly create normally distributed sample of 200 with a mean of 10 minutes and a standard deviation of 2 minutes per patient. This will allow us to ask some important questions of the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# https://r{-}charts.com/distribution/histogram{-}curves/}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220821}\NormalTok{)}
\NormalTok{PhysTime }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{minutes =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{200}\NormalTok{, }\AttributeTok{mean =} \DecValTok{10}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Using the \emph{describe()} function from the \emph{psych} package, we can see the resulting descriptive statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(PhysTime}\SpecialCharTok{$}\NormalTok{minutes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars   n mean sd median trimmed mad  min   max range skew kurtosis   se
X1    1 200  9.9  2   9.98    9.93   2 3.68 15.15 11.47 -0.2     0.03 0.14
\end{verbatim}

Specifically, in this sample size of 200, our mean is 9.9 with a standard deviation of 2.0.

\hypertarget{raw-scores-z-scores-and-proportions}{%
\subsection{\texorpdfstring{Raw Scores, \emph{z}-scores, and Proportions}{Raw Scores, z-scores, and Proportions}}\label{raw-scores-z-scores-and-proportions}}

With data in hand, let's ask, ``What is the range of time that physicians spend with patients that fall within 1 standard deviation of the mean?'' We would answer this question by applying th raw score formula (\(X = \bar{X} + z(s)\)) to +1 and -1 standard deviation.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{{-}} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 11.9
\end{verbatim}

Because \(\pm 1SD\) covers 68\% of the distribution, we now know that 68\% of patients have physician visits that are between 7.9 and 11.9 minutes long.

What about \(\pm 2SDs\)?

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{+} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 13.9
\end{verbatim}

Two standard deviations around the mean captures 94.5\% of patients; patients in this range receive between visits that range between 5.9 and 13.9 minutes.

And what about \(\pm 3SDs\)?

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{{-}} \DecValTok{3} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.9
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{9.9} \SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15.9
\end{verbatim}

Three standard deviations around the mean captures 99.7\% of patients; patients in this range receive between visits that range between 3.9 and 15.9 minutes.

\hypertarget{determining-probabilities}{%
\subsection{Determining Probabilities}\label{determining-probabilities}}

We can also ask questions of \textbf{probability}. For example, what is the probability that a physician spends at least 9.9 minutes with a patient? To answer this question we first calculate the \emph{z}-score associated with 9.9 minutes.

\[z=\frac{X-\bar{X}}{s}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{9.9} \SpecialCharTok{{-}} \FloatTok{9.9}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}  \CommentTok{\#for 9.9 minutes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

We learn that 9.9 minutes (the mean of the distribution of raw scores) corresponds with 0 (the mean of the distribution of \emph{z}-scores).

Next, we examine a \href{https://www.statology.org/z-table/}{table of critical \emph{z} values} where we see that a score of 0.0 corresponds to an area (probability) of .50. The directionality of our table is such that fewer minutes spent with patients are represented on the left (the shaded portion) and more minutes spent with patients are represented on the right (the unshaded portion). Our question asks, what is the probability that a physician spends \emph{at least} 9.9 minutes with a patient (i.e., 9.9 or more minutes) means that we should use the area on the right. Thus, the probability that a physician spends \emph{at least} 9.9 minutes with a patient is 50\%. In this case it is also true that the probability that a physician spends 9.9 minutes or less is also 50\%. This 50/50 result helps make the point that the area under the curve is equal to 1.0.

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-85-1.pdf}
We can also obtain the probability value with the \emph{pnorm()} function. We enter the score, the mean, and the sd. As shown below, we can enter them in \emph{z} score formula or from the raw scores.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{0}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\FloatTok{9.9}\NormalTok{, }\AttributeTok{mean =} \FloatTok{9.9}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5
\end{verbatim}

Let's ask a question that requires careful inspection of the asymmetry of the curve. What is the probability that a physician spends less than 5 minutes with a patient? First, we calculate the corresponding \emph{z}-score:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating the z{-}score}
\NormalTok{(}\DecValTok{5} \SpecialCharTok{{-}} \FloatTok{9.9}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}  \CommentTok{\#for 5 minutes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2.45
\end{verbatim}

Second we locate the corresponding area under the normal curve. Examining the table of critical \emph{z}-values we see that a \emph{z}-score of -2.95 corresponds with an area of 0.0071. We can check this with the \emph{pnorm()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.45}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{, }\AttributeTok{sd =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007142811
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{mean =} \FloatTok{9.9}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007142811
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-89-1.pdf}
There is a .7\% (that is less than 1\%) probability that physicians spend less than 5 minutes with a patient. The inverse (1 - .7) indicates that we can be 99\% confident that patients receive 5 or more minutes with the ICU physician.

What about operations at the other end of the curve? What is the probability that a patient receives less than 12 minutes with a physician? Again, we start with the calculation of the \emph{z}-score.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{12} \SpecialCharTok{{-}} \FloatTok{9.9}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}  \CommentTok{\#for 12 minutes}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.05
\end{verbatim}

The 12 minute mark is 1.05 \emph{SD} above the mean. Checking the \emph{z} table lets us know that an area of 0.8531 corresponds with a \emph{z}-score of 1.05.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\FloatTok{{-}.8531}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1469
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-92-1.pdf}
The probability of a physician spending 12 minutes \emph{or less} with a patient is 85\%; the probability of a physician spending 12 minutes or more with a patient is 15\%.

\hypertarget{percentiles}{%
\subsection{Percentiles}\label{percentiles}}

The same values that we just collected are often interpreted as percentiles. Our prior calculations taught us that a physician/patient visit that lasted 9.9 minutes (\emph{z} = 0), is ranked at the 50th percentile. That is, a 9.9 minute visit is longer than 50\% of patient/physician visits.

A visit lasting 5 minutes (\emph{z} = -2.45) is ranked at the .07th percentile. That is fewer than 1\% of patient/physicizn visits are shorter than that.

Finally, a visit lasting 12 minutes (\emph{z} = 1.05) is ranked at the 85th percentile. That is, it is longer than 85\% of patient visits.

While this seems redundant, this something of a prelude to the importance of \emph{z} scores and the standard normal curve in assessment, evaluation, and psychometrics.

\hypertarget{transforming-variables-to-standard-scores}{%
\subsection{Transforming Variables to Standard Scores}\label{transforming-variables-to-standard-scores}}

We could create a column of \emph{z}-scores, this way:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PhysTime}\SpecialCharTok{$}\NormalTok{zMinutes }\OtherTok{\textless{}{-}}\NormalTok{ (PhysTime}\SpecialCharTok{$}\NormalTok{minutes }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(PhysTime}\SpecialCharTok{$}\NormalTok{minutes))}\SpecialCharTok{/}\FunctionTok{sd}\NormalTok{(PhysTime}\SpecialCharTok{$}\NormalTok{minutes)}
\FunctionTok{head}\NormalTok{(PhysTime)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    minutes    zMinutes
1 10.300602  0.20226980
2 10.143081  0.12370440
3  9.785452 -0.05466684
4 13.162710  1.62977447
5  6.120944 -1.88237678
6 11.793346  0.94679063
\end{verbatim}

The transformation of scores is considered to be \emph{linear}. That is, this 1:1 relationship would result in a correlation of 1.00. Further, the z*version of the variable could be used in analyses, just as the original raw score. Choices to do this are made carefully and usually done to optimize interpretation.

\hypertarget{the-one-sample-z-test}{%
\subsection{\texorpdfstring{The One-Sample \emph{z} test}{The One-Sample z test}}\label{the-one-sample-z-test}}

The one-sample \emph{z} test is a common entry point to hypothesis testing. Let's imagine that we have reason to believe that an optimal physician/patient interaction in the ICU is 10.5 minutes. We want to use this value as a contrast to our own data and ask if the physician/patient interactions in our ICU are statistically significantly different. To test this hypothesis, we first set up null (\(H_0\)) and alternative (\(H_A\)) hypotheses. Our null hypothesis states that the population mean for physician/patient visits is equal to 12; the alternative hypothesis states that it is unequal to 12.

As written, this question is \emph{two-tailed.} That is, the external mean could be larger or smaller, we are just curious to see if it is different.

\[
\begin{array}{ll}
H_0: & \mu = 10.5 \\
H_A: & \mu \neq 10.5
\end{array}
\]
Alternatively, we could ask a \emph{one-sided} question. That is, we might hypothesize that our sample mean is smaller than the external mean.

\[
\begin{array}{ll}
H_0: & \mu = 10.5 \\
H_A: & \mu < 10.5
\end{array}
\]
Whether the test is one- or two- sided makes a difference in the strictness with which we interpret the results. We will reject the \(H_0\) in favor of the alternative (\(H_A\)) if the resulting test statistic (a \emph{z} score) falls into the region of rejection. Sir Ronald Fisher was the scientist who popularized 5\% as the rejection of rejection. In a directionless (two-tailed test), if a test value is greater than the hypothesized value of \(\pm 1.64\) there is said to be a statistically significant difference in means. In contrast, in a two-tailed test, if a test value is greater than \(\pm 1.96\) (where the 5\% is split between the two tails), then the \(H_0\) is rejected.

Inspecting a table of \emph{z} values shows that \(\pm 1.96\) corresponds to areas of 0.025 and \emph{z} values of \(\pm 1.64\) correspond to areas of 0.05.
\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-94-1.pdf}

The formula for a one-sample \emph{z}-test is as follows:

\[
z_{\bar{X}} =  \frac{\bar{X} - \mu_0}{\sigma / \sqrt{N}}
\]
We have already calculated these values. But let's grab some of them again as a reminder:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(PhysTime}\SpecialCharTok{$}\NormalTok{minutes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars   n mean sd median trimmed mad  min   max range skew kurtosis   se
X1    1 200  9.9  2   9.98    9.93   2 3.68 15.15 11.47 -0.2     0.03 0.14
\end{verbatim}

\begin{itemize}
\tightlist
\item
  Sample mean is 9.9
\item
  Population mean (the one we're comparing to) is 12
\item
  Standard deviation is 2
\item
  \emph{N} is 200
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{9.9} \SpecialCharTok{{-}} \FloatTok{10.5}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{2}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{200}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -4.242641
\end{verbatim}

The resulting value, \(z = -4.242\) is our test value. Because this faaaaaar exceeds \(\pm 1.96\) we know that there is a statistically significant effect. Just to be sure, let's use the \emph{pnorm()} function to obtain the \emph{p} value.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{4.24}\NormalTok{, }\AttributeTok{mean =} \FloatTok{9.9}\NormalTok{, }\AttributeTok{sd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0000000000007746685
\end{verbatim}

Without a fancy statistics package, we can claim that there was a statistically significant difference between the physician/patient visit times in our sample and the externa criteria: \(z(200) = -4.24, p < .001\).

The one sample \emph{z}-test is rarely seen in the published literature. However, a close inspection of tables that contain the critical values for \emph{t}-tests reveals that the very bottom row (i.e., when sample sizes are 120 or greater) is, in fact, the \emph{z} criteria. That must mean it's time to learn about the one sample \emph{t}-test.

\hypertarget{introducing-the-one-sample-t-test}{%
\section{\texorpdfstring{Introducing the One Sample \emph{t}-test}{Introducing the One Sample t-test}}\label{introducing-the-one-sample-t-test}}

The one-sample \emph{t} test is used to evaluate whether the mean of a sample differs from another value that, symbolically, is represented as the population mean. Green and Salkind \citep{green_using_2014} noted that this value is often the midpoint of set of scores, the average value of the test variable based on past research, or a test value as the chance level of performance.

\includegraphics{images/ttests/onesample.jpg}
This comparison is evident in the numerator of the formula for the \emph{t} test that shows the population mean \(\mu\) being subtracted from the sample mean\(\bar{X}\).

\[
t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{N} }
\]
Although this statistic is straightforward, it is quite limited. If the researcher wants to compare an outcome variable across two groups of people, they should consider the \protect\hyperlink{tIndSample}{independent samples \emph{t}-test}. If the participant wants to evaluate an outcome variable with two observations from the same group of people, they should consider the \protect\hyperlink{tPaired}{paired samples \emph{t}test}

\hypertarget{workflow-for-the-one-sample-t-test}{%
\subsection{\texorpdfstring{Workflow for the One Sample \emph{t}-test}{Workflow for the One Sample t-test}}\label{workflow-for-the-one-sample-t-test}}

The following is a proposed workflow for conducting a one-sample \emph{t}-test.

\begin{figure}
\centering
\includegraphics{images/ttests/OneSampleWrkFlw.jpg}
\caption{A colorful image of a workflow for the one sample \emph{t}-test}
\end{figure}

If the data meets the assumptions associated with the research design (e.g., independence of observations and a continuously scaled metric), these are the steps for the analysis of a one sample \emph{t} test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare (upload) data.
\item
  Explore data with

  \begin{itemize}
  \tightlist
  \item
    graphs
  \item
    descriptive statistics
  \end{itemize}
\item
  Assess normality via skew and kurtosis
\item
  Select the comparison (i.e., test) value
\item
  Compute the one sample \emph{t}-test
\item
  Compute an effect size (frequently the \emph{d} statistic)
\item
  Manage Type I error
\item
  Sample size/power analysis (which you should think about first, but in the context of teaching statistics, it's more pedagogically sensible, here).
\end{enumerate}

\hypertarget{research-vignette-1}{%
\section{Research Vignette}\label{research-vignette-1}}

Empirically published articles where \emph{t} tests are the primary statistic are difficult to locate. Having exhausted the psychology archives, I located this article in an interdisciplinary journal focused on palliative medicine. The research vignette for this lesson examined differences in physician's verbal and nonverbal communication with Black and White patients at the end of life \citep{elliott_differences_2016}.

Elliott and colleagues \citeyearpar{elliott_differences_2016} were curious to know if hospital-based physicians (56\% White, 26\% Asian, 7.4\% each Black and Hispanic) engaged in verbal and nonverbal communication differently with Black and White patients. Black and White patient participants were matched on characteristics deemed important to the researchers (e.g., critically and terminally ill, prognostically similar, expressed similar treatment preferences). Interactions in the intensive care unit were audio and video recorded and then coded on dimensions of verbal and nonverbal communication.

Because each physician saw a pair of patients (i.e., one Black patient and one White patient), the researchers utilized a paired samples, or dependent \emph{t}-test. This statistical choice was consistent with the element of the research design that controlled for physician effects through matching (and one we will work in a later lesson). Below are the primary findings of the study.

\begin{longtable}[]{@{}llll@{}}
\toprule()
& Black Patients & White Patients & \\
\midrule()
\endhead
Category & \emph{Mean}(\emph{SD}) & \emph{Mean}(\emph{SD}) & \emph{p}-value \\
Verbal skill score (range 0 - 27) & 8.37(3.36) & 8.41(3.21) & 0.958 \\
Nonverbal skill score (range 0 - 5) & 2.68(.84) & 2.93(.77) & 0.014 \\
\bottomrule()
\end{longtable}

In the research vignette Elliott et al. \citeyearpar{elliott_differences_2016} indicated that physician/patient visits lasted between 3 minutes and 40 seconds (220 seconds) to 20 minutes and 13 seconds (1213 seconds). For the purpose of demonstrating the one sample \emph{t}-test, we might want to ask whether the length of patient visits in this research study were statistically significantly different than patient in the ICU or in palliative care, more broadly. Elliott et al.\citeyearpar{elliott_differences_2016} did not indicate a measure of central tendency (i.e., mean, mode, median) therefore, I will simulate the data by randomly generating 33 numbers between 220 and 1213. I will use \emph{random selection with replacement}, which allows the same number to be selected more than once.

A warning: this particularly analysis is ``more simulated than usual'' and does not represent reality. However, this research vignette lends itself for this type of question.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Setting the \textquotesingle{}random\textquotesingle{} seed ensures that everyone gets the same}
\CommentTok{\# result, every time they rerun the analysis. My personal practice is}
\CommentTok{\# to create a random seed that represents the day I write up the}
\CommentTok{\# problem (in this case August, 15, 2022) When the Suggestions for}
\CommentTok{\# Practice invite you to \textquotesingle{}change the random seed,\textquotesingle{} simply change this}
\CommentTok{\# number to anything you like (maybe your birthday or today\textquotesingle{}s date)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220815}\NormalTok{)}

\CommentTok{\# Assigns as physician ID number to each row Simulates 33 numbers}
\CommentTok{\# between 220 and 1213}
\NormalTok{dfOneSample }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{33}\NormalTok{)), }\AttributeTok{PhysicianSeconds =} \FunctionTok{sample}\NormalTok{(}\DecValTok{220}\SpecialCharTok{:}\DecValTok{1213}\NormalTok{,}
    \DecValTok{33}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{))}

\CommentTok{\# Displays the first 6 rows of the df}
\FunctionTok{head}\NormalTok{(dfOneSample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID PhysicianSeconds
1  1              264
2  2              356
3  3             1078
4  4              767
5  5              815
6  6             1107
\end{verbatim}

With our data in hand, let's examine its structure. The variable representing physician seconds represents the ratio scale of measurement and therefore should be noted as \emph{num} (numerical) or \emph{int} (integer; whole numbers) in R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dfOneSample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   33 obs. of  2 variables:
 $ ID              : Factor w/ 33 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ PhysicianSeconds: int  264 356 1078 767 815 1107 583 892 698 1068 ...
\end{verbatim}

Below is code for saving (and then importing) the data in .csv or .rds files. I make choices about saving data based on what I wish to do with the data. If I want to manipulate the data outside of R, I will save it as a .csv file. It is easy to open .csv files in Excel. A limitation of the .csv format is that it does not save any restructuring or reformatting of variables. For this lesson, this is not an issue.

Here is code for saving the data as a .csv and then reading it back into R. I have hashtagged these out, so you will need to remove the hashtags if you wish to run any of these operations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# writing the simulated data as a .csv write.table(dfOneSample, file}
\CommentTok{\# = \textquotesingle{}dfOneSample.csv\textquotesingle{}, sep = \textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) at}
\CommentTok{\# this point you could clear your environment and then bring the data}
\CommentTok{\# back in as a .csv reading the data back in as a .csv file}
\CommentTok{\# dfOneSample\textless{}{-} read.csv (\textquotesingle{}dfOneSample.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

The .rds form of saving variables preserves any formatting (e.g., creating ordered factors) of the data. A limitation is that these files are not easily opened in Excel. Here is the hashtagged code (remove hashtags if you wish to do this) for writing (and then reading) this data as an .rds file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# saveRDS(dfOneSample, \textquotesingle{}dfOneSample.rds\textquotesingle{}) dfOneSample \textless{}{-}}
\CommentTok{\# readRDS(\textquotesingle{}dfOneSample.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-problem}{%
\section{Working the Problem}\label{working-the-problem}}

\hypertarget{stating-the-hypothesis}{%
\subsection{Stating the Hypothesis}\label{stating-the-hypothesis}}

A quick scan of the literature suggests that health care workers' visits to patients in the ICU are typically quite brief. Specifically, the average duration of a physician visit in a 2018 study was 73.5 seconds \citep{butler_estimating_2018}. A one-sample \emph{t} test is appropriate for comparing the visit lengths from our sample to this external metric.

As noted in the symbolic presentation below, our null hypothesis (\(H_0\)) states that our data will be equal to the test value of 73.5 seconds. In contrast, the alternative hypothesis (\(H_A\)) states that these values will not be equal.

\[
\begin{array}{ll}
H_0: & \mu = 73.5 \\
H_A: & \mu \neq 73.5
\end{array}
\]

\hypertarget{preliminary-exploration}{%
\subsection{Preliminary Exploration}\label{preliminary-exploration}}

Plotting the data is best practice to any data analysis. The \emph{ggpubr} package is one of my go-to-tools for quick and easy plots of data. Below, I have plotted the time-with-patient (Physician Seconds) variable and added the mean. As with most plotting packages, ggpubr will ``bin'' (or cluster) the data for plotting; this is especially true for data with a large number of units (a range from 220 to 1213 is quite large). The ``rug = TRUE'' command added a lower row of the table to identify where each of the datapoint follows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{gghistogram}\NormalTok{(dfOneSample, }\AttributeTok{x =} \StringTok{"PhysicianSeconds"}\NormalTok{, }\AttributeTok{add =} \StringTok{"mean"}\NormalTok{,}
    \AttributeTok{rug =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Using `bins = 30` by default. Pick better value with the argument
`bins`.
\end{verbatim}

\begin{verbatim}
Warning: geom_vline(): Ignoring `mapping` because `xintercept` was provided.
\end{verbatim}

\begin{verbatim}
Warning: geom_vline(): Ignoring `data` because `xintercept` was provided.
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-102-1.pdf}

The histogram makes it clear that our data does not reflect a standard normal curve.

Another view of our data is with a boxplot. The box captures the middle 50\% of data with the horizontal bar at the median. The whiskers extend three standard deviations around the mean with dots beyond the whiskers representing outliers. I personally like the \emph{add=``jitter''} statement because it shows where each case falls.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(dfOneSample}\SpecialCharTok{$}\NormalTok{PhysicianSeconds, }\AttributeTok{ylab =} \StringTok{"Seconds with Patient"}\NormalTok{,}
    \AttributeTok{xlab =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{add =} \StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-103-1.pdf}
We can further evaluate normality by obtaining the descriptive statistics with the \emph{describe()} function from the \emph{psych} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(dfOneSample}\SpecialCharTok{$}\NormalTok{PhysicianSeconds)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars  n   mean     sd median trimmed    mad min  max range  skew kurtosis
X1    1 33 717.12 312.22    762  720.07 403.27 225 1197   972 -0.14    -1.36
      se
X1 54.35
\end{verbatim}

Here we see that our scores range from 225 to 1197 with a mean of 717.12 and a standard deviation of 312.22. We're ready to calculate the one sample \emph{t}-test.

\hypertarget{hand-calculations}{%
\subsection{Hand-Calculations}\label{hand-calculations}}

In learning the statistic, hand-calculations can help understand what the statistic is doing. Here's the formula again:

\[
t = \frac{\bar{X} - \mu}{\hat{\sigma}/\sqrt{N} }
\]

The denominator of the formula below subtracts the test value from the sample mean. The denominator involves multiplying the standard deviation by the square root of the sample size. The descriptive statistics provided the values we need to complete the analysis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{717.12} \SpecialCharTok{{-}} \FloatTok{73.5}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FloatTok{312.22}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 11.84202
\end{verbatim}

\hypertarget{statistical-significance}{%
\subsubsection{Statistical Significance}\label{statistical-significance}}

If we ask about \emph{statistical significance} then we are likely engaged in \emph{null hypothesis significance testing} (NHST). In the case of a one sample test, we construct our hypothesis with a null and an alternative that are relatively straightforward. Specifically, we are interested in knowing if our sample mean (717.12) is statistically, significantly different from the test value of 73.5. We can write the hypotheses in this way:

\[
\begin{array}{ll}
H_0: & \mu = 73.5 \\
H_1: & \mu \neq 73.5
\end{array}
\]
In two parts, our null hypothesis (\(H_0\)) states that the population mean (\(H_0\)) for physician visits with palliative care patients is 73.5; the alternative \(\mu \neq\) states that it is not 73.5.

When we calculated the \emph{t} test, we obtained a \emph{t} value. We can check the statistical significance by determining the test critical value from a \href{https://www.statology.org/t-distribution-table/}{table of critical values} for the \emph{t} distribution. There are many freely available on the internet. If our \emph{t} value exceeds the value(s) in the table of critical values, then we can claim that our sample mean is statistically significantly different from the hypothesized value.

Heading to the table of critical values we do the following:

\begin{itemize}
\tightlist
\item
  For the one-sample \emph{t} test, the degrees of freedom (DF) is equal to the sample size (33). The closest value in our table is 30, so we will use that row.
\item
  A priorily, we did not specify if we thought the difference would be greater, or lower. Therefore, we will use a column that indicates \emph{two-tails}.
\item
  A \emph{p} value of .05 is customary.
\item
  Thus, if our \emph{t} value is lower than -2.042 or higher than 2.042 we know we have a statistically significant difference.
\end{itemize}

In our case, the \emph{t} value of 11.03 exceeded the test critical value of 2.042. We would write the statistical string this way: \emph{t}(33) = 11.84, \emph{p} \textless{} .05.

In base R, the \emph{qt()} function will look up a test critical value. For the one-sample \emph{t} test, degrees of freedom (df) is equal to the sample size. We ``divide the \emph{p} value by 2'' when we want a two-tailed test. Finally, the ``lower.tail'' command results in positive or negative values in the tail.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\AttributeTok{p =} \FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{df =} \DecValTok{33}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.034515
\end{verbatim}

Not surprisingly, this value is quite similar to the value we saw in the table. The \emph{qt()} function is more accurate because it used df of 33 (not rounded down to 30).

\hypertarget{confidence-intervals}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals}}

How confident are we in our result? With the one sample \emph{t}-test, it is common to report an interval in which we are 95\% confident that our true mean difference exists. Below is the formula, which involves:

\begin{itemize}
\tightlist
\item
  \(\bar{X}\) is the sample mean; in our case this is 717.12
\item
  \(t_{cv}\) the test critical value for a two-tailed model (even if the hypothesis was one-tailed) where \(\alpha = .05\) and the degrees of freedom are \(N-1\)
\item
  \(\frac{s}{\sqrt{n}}\) was the denominator of the test statistic it involves the standard deviation of our sample (312.22) and the square root of our sample size (33)
\end{itemize}

\[\bar{X} \pm t_{cv}(\frac{s}{\sqrt{n}})\]
Let's calculate it:

First, let's get the proper \emph{t} critical value. Even though these are identical to the one above, I am including them again. Why? Because if the original hypothesis had been one-tailed, we would need to calculate a two-tailed confidence interval; this is a placeholder to remind us.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\AttributeTok{p =} \FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\AttributeTok{df =} \DecValTok{32}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.036933
\end{verbatim}

Using the values from above, we can specify both the lower and upper bound of our confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{717.12}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ ((}\FloatTok{2.0369}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FloatTok{312.22}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 606.4134
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{717.12}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ ((}\FloatTok{2.0369}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FloatTok{312.22}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 827.8266
\end{verbatim}

The resulting interval is the 95\% confidence interval around our sample mean. Stated another way, we are 95\% certain that the true mean of time with patients in our sample ranges between 606.41 and 827.83 seconds.

\hypertarget{effect-size}{%
\subsubsection{Effect size}\label{effect-size}}

If you have heard someone say something like, ``I see there is statistical significance, but it the difference \emph{clinically significant},'' the person is probably asking about \emph{effect sizes.} Effect sizes provide an indication of the magnitude of the difference.

The \emph{d} statistic is commonly used with \emph{t} tests; \emph{d} assesses the degree that the mean on the test variable differs from the test value. Conveniently, \emph{d} represents standard deviation units. A \emph{d} value of 0 indicates that the mean of the sample equals the mean of the test value. As \emph{d} moves away from 0 (in either direction), we can interpret the effect size to be stronger. Conventionally, the absolute values of .2, .5, and .8, represent small, medium, and large effect sizes, respectfully.

Calculating the \emph{d} statistic is easy. Here are two equivalent formulas:

\[d = \frac{Mean Difference}{SD}=\frac{t}{\sqrt{N}}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First formula}
\NormalTok{(}\FloatTok{717.12} \SpecialCharTok{{-}} \FloatTok{73.5}\NormalTok{)}\SpecialCharTok{/}\FloatTok{312.22}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.061431
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Second formula}
\FloatTok{11.842}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.061428
\end{verbatim}

The value of 2.06 indicates that the test value is approximately two standard deviations away from the sample mean. This is a very large difference.

\hypertarget{computation-in-r}{%
\section{Computation in R}\label{computation-in-r}}

Calculating a one sample \emph{t}-test is possible through base R and a number of packages. Navarro's \citeyearpar{navarro_book_2020} \emph{lsr} package provides output that is commonly used in psychology.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{oneSampleTTest}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dfOneSample}\SpecialCharTok{$}\NormalTok{PhysicianSeconds, }\AttributeTok{mu =} \FloatTok{73.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   One sample t-test 

Data variable:   dfOneSample$PhysicianSeconds 

Descriptive statistics: 
            PhysicianSeconds
   mean              717.121
   std dev.          312.216

Hypotheses: 
   null:        population mean equals 73.5 
   alternative: population mean not equal to 73.5 

Test results: 
   t-statistic:  11.842 
   degrees of freedom:  32 
   p-value:  <.001 

Other information: 
   two-sided 95% confidence interval:  [606.414, 827.828] 
   estimated effect size (Cohen's d):  2.061 
\end{verbatim}

This well-organized output has everything we need for an APA style presentation of results. Identical to all the information we hand-calculated, we would write the \emph{t} string this way: \(t(32) = 11.842, p < .001, d = 2.061\). The \emph{lsr} output also includes confidence intervals. These represent the 95\% confidence interval of the true difference between the means. That is, we are 95\% confident that the true mean of the physicians in our sample falls between 606.414 and 827.828.

\hypertarget{apa-style-results}{%
\section{APA Style Results}\label{apa-style-results}}

Let's write up the results. I would probably choose to include the boxplot produced in the initial exploration of the data.

\begin{quote}
A one-sample \emph{t}-test was used to evaluate whether average amount of time that a sample of physicians (palliative care physicians in the ICU) enrolled in a research study on patient communication was statistically significantly different from the amount of time that ICU physicians spend with their patients, in general. The sample mean 717.121 (312.216) was significantly different from 73.5, \(t(33) = 11.84, p < .001., d = 2.061\). The effect size, (\emph{d}) indicates a very large effect. Figure 1 illustrates the distribution of time physicians in the research study spent with their patients. The results support the conclusion that physicians in the research study spent more time with their patients than ICU physicians in general.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(dfOneSample}\SpecialCharTok{$}\NormalTok{PhysicianSeconds, }\AttributeTok{ylab =} \StringTok{"Seconds with Patient"}\NormalTok{,}
    \AttributeTok{xlab =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{add =} \StringTok{"jitter"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Figure 1. Physician Time with Patients (in seconds)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-111-1.pdf}

Reflecting on these results, I must remind readers that this simulated data that is even further extrapolated. Although ``data'' informed both the amount of time spent by the physicians in the research study and data used as the test value, there are probably many reasons that the test value was not a good choice. For example, even though both contexts were ICU, palliative physicians may have a different standard of care than ICU physicians ``in general.''

\hypertarget{power-in-independent-samples-t-tests}{%
\section{\texorpdfstring{Power in Independent Samples \emph{t} tests}{Power in Independent Samples t tests}}\label{power-in-independent-samples-t-tests}}

Researchers often use power analysis packages to estimate the sample size needed to detect a statistically significant effect, if, in fact, there is one. Utilized another way, these tools allows us to determine the probability of detecting an effect of a given size with a given level of confidence. If the probability is unacceptably low, we may want to revise or stop. A helpful overview of power as well as guidelines for how to use the \emph{pwr} package can be found at a \href{https://www.statmethods.net/stats/power.html}{Quick-R website} \citep{kabacoff_power_2017}.

In Champely's \emph{pwr} package, we can conduct a power analysis for a variety of designs, including the one sample \emph{t} test that we worked in this lesson. There are a number of interrelating elements of power:

\begin{itemize}
\tightlist
\item
  Sample size, \emph{n} refers to the number of observations; our vignette had 33
\item
  \emph{d} refers to the difference between means divided by the pooled standard deviation; ours was (
\item
  \emph{power} refers to the power of a statistical test; conventionally it is set at .80
\item
  \emph{sig.level} refers to our desired alpha level; conventionally it is set at .05
\item
  \emph{type} indicates the type of test we ran; this was ``one.sample''
\item
  \emph{alternative} refers to whether the hypothesis is non-directional/two-tailed (``two.sided'') or directional/one-tailed(``less'' or ``greater'')
\end{itemize}

In this script, we must specify \emph{all-but-one} parameter; the remaining parameter must be defined as NULL. R will calculate the value for the missing parameter.

When we conduct a ``power analysis'' (i.e., the likelihood of a hypothesis test detecting an effect if there is one), we specify, ``power=NULL''. Using the data from our results, we learn from this first run, that our statistical power was 1.00. That is, given the value of the mean difference relative to the pooled standard deviation we had a 100\% chance of detecting a statistically significant effect if there was one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ (}\FloatTok{717.121} \SpecialCharTok{{-}} \FloatTok{73.5}\NormalTok{)}\SpecialCharTok{/}\FloatTok{312.216}\NormalTok{, }\AttributeTok{n =} \DecValTok{33}\NormalTok{, }\AttributeTok{power =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"one.sample"}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     One-sample t test power calculation 

              n = 33
              d = 2.061461
      sig.level = 0.05
          power = 1
    alternative = two.sided
\end{verbatim}

Researchers frequently use these tools to estimate the sample size required to obtain a statistically significant effect. In these scenarios we set \emph{n} to \emph{NULL}. Using the results from the simulation of our research vignette, you can see that we would have needed 21,022 individuals for the \emph{p} value to be \textless{} .05.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ (}\FloatTok{717.121} \SpecialCharTok{{-}} \FloatTok{73.5}\NormalTok{)}\SpecialCharTok{/}\FloatTok{312.216}\NormalTok{, }\AttributeTok{n =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"one.sample"}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     One-sample t test power calculation 

              n = 4.109718
              d = 2.061461
      sig.level = 0.05
          power = 0.8
    alternative = two.sided
\end{verbatim}

Shockingly, this suggests that a sample size of four could result in a statistically significant result. Let's see if this is true. Below I will re-simulate the data for the verbal scores, changing only the sample size:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Setting the \textquotesingle{}random\textquotesingle{} seed ensures that everyone gets the same}
\CommentTok{\# result, every time they rerun the analysis. My personal practice is}
\CommentTok{\# to create a random seed that represents the day I write up the}
\CommentTok{\# problem (in this case August, 15, 2022) When the Suggestions for}
\CommentTok{\# Practice invite you to \textquotesingle{}change the random seed,\textquotesingle{} simply change this}
\CommentTok{\# number to anything you like (maybe your birthday or today\textquotesingle{}s date)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220820}\NormalTok{)}

\CommentTok{\# Assigns as physician ID number to each row Simulates 33 numbers}
\CommentTok{\# between 220 and 1213}
\NormalTok{rdfOneSample }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{ID =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{)), }\AttributeTok{PhysicianSeconds =} \FunctionTok{sample}\NormalTok{(}\DecValTok{220}\SpecialCharTok{:}\DecValTok{1213}\NormalTok{,}
    \DecValTok{4}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{))}

\CommentTok{\# Displays the first 6 rows of the df}
\FunctionTok{head}\NormalTok{(rdfOneSample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID PhysicianSeconds
1  1             1071
2  2              517
3  3             1193
4  4              877
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{oneSampleTTest}\NormalTok{(}\AttributeTok{x =}\NormalTok{ rdfOneSample}\SpecialCharTok{$}\NormalTok{PhysicianSeconds, }\AttributeTok{mu =} \FloatTok{73.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   One sample t-test 

Data variable:   rdfOneSample$PhysicianSeconds 

Descriptive statistics: 
            PhysicianSeconds
   mean              914.500
   std dev.          295.221

Hypotheses: 
   null:        population mean equals 73.5 
   alternative: population mean not equal to 73.5 

Test results: 
   t-statistic:  5.697 
   degrees of freedom:  3 
   p-value:  0.011 

Other information: 
   two-sided 95% confidence interval:  [444.737, 1384.263] 
   estimated effect size (Cohen's d):  2.849 
\end{verbatim}

In this case our difference between the sample data and the external data is so huge, that a sample of four still nets a statistically significant result. This is unusual. Here's the \emph{t} string: \(t(3) = 914.50, p = 0.001, d = 2.85, 95%CI[444.74, 1384.26]
\).

\hypertarget{practice-problems-1}{%
\section{Practice Problems}\label{practice-problems-1}}

The suggestions for homework are graded in complexity and I encourage you to select one or more that meets you where you are (e.g., in terms of your self-efficacy for statistics, your learning goals, and competing life demands).

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation of the research vignette, then rework the problem. This should provide minor changes to the data but the results will likely be very similar. That said, don't be alarmed if what was non-significant in my working of the problem becomes significant. Our selection of \emph{p} \textless{} .05 (and the corresponding 95\% confidence interval) means that 5\% of the time there could be a difference in statistical significance.

\hypertarget{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation}{%
\subsection{Problem \#2: Rework the research vignette, but change something about the simulation}\label{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation}}

Rework the one sample \emph{t}-test in the lesson by changing something else about the simulation. Perhaps estimate another comparative number. The 73.5 seconds was a dramatic difference from the mean of the research participants. Perhaps suggest (and, ideally, support with a reference) another number. Alternatively, if you are interested in issues of power, specify a different sample size.

\hypertarget{problem-3-use-other-data-that-is-available-to-you}{%
\subsection{Problem \#3: Use other data that is available to you}\label{problem-3-use-other-data-that-is-available-to-you}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete an independent samples \emph{t} test.

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the variables and their role in the analysis & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct a one sample \emph{t} test (with an effect size) & 5 & \_\_\_\_\_ \\
5. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
6 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 30 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{tIndSample}{%
\chapter{\texorpdfstring{Independent Samples \emph{t} test}{Independent Samples t test}}\label{tIndSample}}

\href{link\%20here}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{)  }\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

Researchers may wish to know if there are differences on a given outcome variable as a result of a dichotomous grouping variable. For example, during the COVID-19 pandemic, my research team asked if there were differences in the percentage of time that individuals wore facemasks as a result of 2020 Presidential voting trends (Republican or Democratic) of their county of residence. In these simple designs, the independent samples \emph{t} test could be used to test the researchers' hypotheses.

\hypertarget{navigating-this-lesson-3}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-3}}

There is about \# hour and \#\# minutes of lecture. If you work through the materials with me it would be plan for an additional TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-3}{%
\subsection{Learning Objectives}\label{learning-objectives-3}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Recognize the research questions for which utilization of the independent samples \emph{t} test would be appropriate.
\item
  Narrate the steps in conducting an independent samples \emph{t} test, beginning with testing the statistical assumptions through writing up an APA style results section.
\item
  Calculate an independent samples \emph{t} test in R (including effect sizes).
\item
  Interpret a 95\% confidence interval around a mean difference score.
\item
  Produce an APA style results for an independent samples \emph{t} test.
\item
  Determine a sample size that (given a set of parameters) would likely result in a statistically significant effect, if there was one.
\end{itemize}

\hypertarget{planning-for-practice-2}{%
\subsection{Planning for Practice}\label{planning-for-practice-2}}

The suggestions for homework are graded in complexity. The more complete descriptions at the end of the chapter follow these suggestions.

\begin{itemize}
\tightlist
\item
  Rework the independent samples \emph{t} test in the lesson by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar.
\item
  Rework the independent samples \emph{t} test in the lesson by changing something else about the simulation. For example, if you are interested in power, consider changing the sample size.
\item
  Use the simulated data that is provided, but use the nonverbal variable, instead.
\item
  Conduct an independent samples \emph{t} test with data to which you have access and permission to use. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources-2}{%
\subsection{Readings \& Resources}\label{readings-resources-2}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Navarro, D. (2020). Chapter 13: Comparing two means. In \href{https://learningstatisticswithr.com/}{Learning Statistics with R - A tutorial for Psychology Students and other Beginners}. Retrieved from \url{https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book\%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)}

  \begin{itemize}
  \tightlist
  \item
    Navarro's OER includes a good mix of conceptual information about \emph{t} tests as well as R code. My lesson integrates her approach as well as considering information from Field's \citeyearpar{field_discovering_2012} and Green and Salkind's \citeyearpar{green_using_2014} texts (as well as searching around on the internet).
  \end{itemize}
\item
  Elliott, A. M., Alexander, S. C., Mescher, C. A., Mohan, D., \& Barnato, A. E. (2016). Differences in Physicians' Verbal and Nonverbal Communication With Black and White Patients at the End of Life. \emph{Journal of Pain and Symptom Management, 51}(1), 1--8. \url{https://doi.org/10.1016/j.jpainsymman.2015.07.008}

  \begin{itemize}
  \tightlist
  \item
    The source of our research vignette.
  \end{itemize}
\end{itemize}

\hypertarget{packages-1}{%
\subsection{Packages}\label{packages-1}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(dplyr))\{install.packages(\textquotesingle{}dplyr\textquotesingle{})\}}
\CommentTok{\# if(!require(lsr))\{install.packages(\textquotesingle{}lsr\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\}}
\CommentTok{\# if(!require(pwr))\{install.packages(\textquotesingle{}pwr\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-the-independent-samples-t-test}{%
\section{\texorpdfstring{Introducing the Independent Samples \emph{t} Test}{Introducing the Independent Samples t Test}}\label{introducing-the-independent-samples-t-test}}

The independent samples \emph{t}-test assesses whether the population mean of the test variable for one group differs from the population mean of the test variable for a second group. This \emph{t} test can only accommodate two levels of a grouping variable (e.g., teachers/students, volunteers/employees, treatment/control) and the participants must be different in each group.

\begin{figure}
\centering
\includegraphics{images/ttests/conditions_paired.jpg}
\caption{An image of a row with two boxes labeled Condition A (in light blue) and Condition B (in dark blue). This represents the use of an independent samples \emph{t}-test to compare across conditions.}
\end{figure}

The comparison of two means is especially evident in the numerator of the formula. In the denominator we can see that the mean difference is adjusted by the standard error. At the outset, you should know that the formula in the denominator gets messy, but the formula, alone, provides an important conceptual map.

\[t = \frac{\bar{X}_1 - \bar{X}_2}{\mbox{SE}}\]
If the researcher is interested in comparing the same participants' experiences across time or in different groups, they should consider using a \protect\hyperlink{tPaired}{paired samples \emph{t}-test}. Further, the independent samples \emph{t}-test is limited to a grouping variable with only two levels. If the researcher is interested in three or more levels, they should consider using a \protect\hyperlink{oneway}{one-way ANOVA}.

\hypertarget{workflow-for-independent-samples-t-test}{%
\subsection{\texorpdfstring{Workflow for Independent Samples \emph{t} test}{Workflow for Independent Samples t test}}\label{workflow-for-independent-samples-t-test}}

The following is a proposed workflow for conducting a independent samples \emph{t}-test.

\includegraphics{images/ttests/IndSampleWrkFlw.jpg}
If the data meets the assumptions associated with the research design (e.g., independence of observations and a continuously scaled metric), these are the steps for the analysis of an independent samples \emph{t} test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare (upload) data.
\item
  Explore data with

  \begin{itemize}
  \tightlist
  \item
    graphs
  \item
    descriptive statistics
  \end{itemize}
\item
  Assess normality via skew and kurtosis
\item
  Consider the homogeneity of variance assumption and decide whether to use the Student's or Welch's formulation.
\item
  Compute the independent samples \emph{t}-test
\item
  Compute an effect size (frequently the \emph{d} or \emph{eta} statistic)
\item
  Manage Type I error
\item
  Sample size/power analysis (which you should think about first, but in the context of teaching statistics, it's more pedagogically sensible, here).
\end{enumerate}

\hypertarget{research-vignette-2}{%
\section{Research Vignette}\label{research-vignette-2}}

Empirically published articles where \emph{t} tests are the primary statistic are difficult to locate. Having exhausted the psychology archives, I located this article in an interdisciplinary journal focused on palliative medicine. The research vignette for this lesson examined differences in physician's verbal and nonverbal communication with Black and White patients at the end of life \citep{elliott_differences_2016}.

Elliott and colleagues \citeyearpar{elliott_differences_2016} were curious to know if hospital-based physicians (56\% White, 26\% Asian, 7.4\% each Black and Hispanic) engaged in verbal and nonverbal communication differently with Black and White patients. Black and White patient participants were matched on characteristics deemed important to the researchers (e.g., critically and terminally ill, prognostically similar, expressed similar treatment preferences). Interactions in the intensive care unit were audio and video recorded and then coded on dimensions of verbal and nonverbal communication.

Because each physician saw a pair of patients (i.e., one Black patient and one White patient), the researchers utilized a paired samples, or dependent \emph{t}-test. This statistical choice was consistent with the element of the research design that controlled for physician effects through matching. Below are the primary findings of the study.

\begin{longtable}[]{@{}llll@{}}
\toprule()
& Black Patients & White Patients & \\
\midrule()
\endhead
Category & \emph{Mean}(\emph{SD}) & \emph{Mean}(\emph{SD}) & \emph{p}-value \\
Verbal skill score (range 0 - 27) & 8.37(3.36) & 8.41(3.21) & 0.958 \\
Nonverbal skill score (range 0 - 5) & 2.68(.84) & 2.93(.77) & 0.014 \\
\bottomrule()
\end{longtable}

Although their design was more sophisticated (and, therefore, required the paired samples \emph{t}-test), Elliott et al. \citeyearpar{elliott_differences_2016} could have simply compared the outcome variables (e.g., verbal and nonverbal communication) as a function of their dichotomous variable, patient race (Black, White).

In the data below, I have simulated the verbal and non-verbal communication variables using the means and standard deviations listed in the article. Further, I truncated them to fit within the assigned range. I created 33 sets each and assigned them to the Black or White level of the grouping variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220815}\NormalTok{)}
\CommentTok{\# sample size, M, and SD for Black then White patients}
\NormalTok{Verbal }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{33}\NormalTok{, }\AttributeTok{mean =} \FloatTok{8.37}\NormalTok{, }\AttributeTok{sd =} \FloatTok{3.36}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{33}\NormalTok{, }\AttributeTok{mean =} \FloatTok{8.41}\NormalTok{, }\AttributeTok{sd =} \FloatTok{3.21}\NormalTok{))}
\CommentTok{\# set upper bound}
\NormalTok{Verbal[Verbal }\SpecialCharTok{\textgreater{}} \DecValTok{27}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{3}
\CommentTok{\# set lower bound}
\NormalTok{Verbal[Verbal }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\CommentTok{\# sample size, M, and SD for Black then White patients}
\NormalTok{Nonverbal }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{33}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.68}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.84}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{33}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.93}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.77}\NormalTok{))}
\CommentTok{\# set upper bound}
\NormalTok{Nonverbal[Nonverbal }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\CommentTok{\# set lower bound}
\NormalTok{Nonverbal[Nonverbal }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{66}\NormalTok{))}
\CommentTok{\# name factors and identify how many in each group; should be in same}
\CommentTok{\# order as first row of script}
\NormalTok{PatientRace }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\DecValTok{33}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"White"}\NormalTok{, }\DecValTok{33}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{dfIndSamples }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, PatientRace, Verbal, Nonverbal)}
\end{Highlighting}
\end{Shaded}

With our data in hand, let's inspect its structure (i.e., the measurement scales for the variables) to see if they are appropriate.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dfIndSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   66 obs. of  4 variables:
 $ ID         : Factor w/ 66 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ PatientRace: chr  "Black" "Black" "Black" "Black" ...
 $ Verbal     : num  2.76 5.73 6.81 8.68 9.1 ...
 $ Nonverbal  : num  3.41 4.02 1.62 2.52 2.11 ...
\end{verbatim}

The verbal and nonverbal variables are quasi-interval scale variables. Therefore, the numerical scale is correctly assigned by R. In contrast, patient race is a nominal variable and should be a factor. In their article, Elliot et al. \citeyearpar{elliott_differences_2016} assigned Black as the baseline variable and White as the comparison variable. Because R orders factors alphabetically, and ``Black'' precedes ``White'', this would happen automatically. Because creating ordered factors is a useful skill, I will write out the full code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dfIndSamples}\SpecialCharTok{$}\NormalTok{PatientRace }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(dfIndSamples}\SpecialCharTok{$}\NormalTok{PatientRace, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"White"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's again check the formatting of the variables:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dfIndSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   66 obs. of  4 variables:
 $ ID         : Factor w/ 66 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ PatientRace: Factor w/ 2 levels "Black","White": 1 1 1 1 1 1 1 1 1 1 ...
 $ Verbal     : num  2.76 5.73 6.81 8.68 9.1 ...
 $ Nonverbal  : num  3.41 4.02 1.62 2.52 2.11 ...
\end{verbatim}

The four variables of interest are now correctly formatted as \emph{num} and \emph{factor}.

Below is code for saving (and then importing) the data in .csv or .rds files. I make choices about saving data based on what I wish to do with the data. If I want to manipulate the data outside of R, I will save it as a .csv file. It is easy to open .csv files in Excel. A limitation of the .csv format is that it does not save any restructuring or reformatting of variables. For this lesson, this is not an issue.

Here is code for saving the data as a .csv and then reading it back into R. I have hashtagged these out, so you will need to remove the hashtags if you wish to run any of these operations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# writing the simulated data as a .csv write.table(dfIndSamples, file}
\CommentTok{\# = \textquotesingle{}dfIndSamples.csv\textquotesingle{}, sep = \textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# at this point you could clear your environment and then bring the}
\CommentTok{\# data back in as a .csv reading the data back in as a .csv file}
\CommentTok{\# dfIndSamples\textless{}{-} read.csv (\textquotesingle{}dfIndSamples.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

The .rds form of saving variables preserves any formatting (e.g., creating ordered factors) of the data. A limitation is that these files are not easily opened in Excel. Here is the hashtagged code (remove hashtags if you wish to do this) for writing (and then reading) this data as an .rds file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# saveRDS(dfIndSamples, \textquotesingle{}dfIndSamples.rds\textquotesingle{}) dfIndSamples \textless{}{-}}
\CommentTok{\# readRDS(\textquotesingle{}dfIndSamples.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-problem-1}{%
\section{Working the Problem}\label{working-the-problem-1}}

\hypertarget{stating-the-hypothesis-1}{%
\subsection{Stating the Hypothesis}\label{stating-the-hypothesis-1}}

In this lesson, I will focus on differences in the verbal communication variable. Specifically, I hypothesize that physician verbal communication scores for Black and White patients will differ. In the hypotheses below, the null hypothesis (\(H_0\)) states that the two means are equal; the alternative hypothesis (\(H_A\)) states that the two means are not equal.

\[
\begin{array}{ll}
H_0: & \mu_1 = \mu_2  \\
H_A: & \mu_1 \neq \mu_2
\end{array}
\]

\hypertarget{preliminary-exploration-1}{%
\subsection{Preliminary Exploration}\label{preliminary-exploration-1}}

Plotting the data is a helpful early step in any data analysis. The \emph{ggpubr} package is one of my go-to-tools for quick and easy plots of data. Boxplots are terrific for data that is grouped. A helpful \href{https://rpkgs.datanovia.com/ggpubr/}{tutorial} for boxplots (and related plots) can be found at datanovia.

In the code below I introduced the colors by identifying the grouping variable and assigning colors. Those color codes are the ``Hex'' codes you find in the custom color palette in your word processing program.

I am also fond of plotting each case with the command, \emph{add = ``jitter''}. To increase your comfort and confidence in creating figures (and with other tools) try deleting and adding back in different commands. This is how to distinguish between the essential and the elective.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(dfIndSamples, }\AttributeTok{x =} \StringTok{"PatientRace"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Verbal"}\NormalTok{, }\AttributeTok{color =} \StringTok{"PatientRace"}\NormalTok{,}
    \AttributeTok{palette =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#00AFBB"}\NormalTok{, }\StringTok{"\#FC4E07"}\NormalTok{), }\AttributeTok{add =} \StringTok{"jitter"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-126-1.pdf}
The box of the boxplot covers the middle 50\% (the interquartile range). The horizontal line is the median. The whiskers represent three standard deviations above and below the mean. Any dots are outliers.

We can begin to evaluate the assumption of normality by obtaining the descriptive statistics with the \emph{describe()} function from the \emph{psych} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(dfIndSamples}\SpecialCharTok{$}\NormalTok{Verbal)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars  n mean   sd median trimmed  mad  min   max range skew kurtosis   se
X1    1 66 8.25 3.14   7.93     8.2 3.08 0.35 19.31 18.96 0.43     1.21 0.39
\end{verbatim}

From this, we learn that the overall verbal mean is 8.25 with a standard deviation of 3.14. The values for skew (0.43) and kurtosis (1.21) fall below the areas of concern (below the absolute value of 3 for skew; below the absolute values of 8 for kurtosis) identified by Kline \citeyearpar{kline_principles_2016}.

Recall that one of the assumptions for independent samples \emph{t}-test is that the variable of interest is normally distributed within each level of the grouping variable. The \emph{describeBy()} function in the \emph{psych} package allows us to obtain these values for each level of the grouping variable.

If we feed the function the entire df, it will give us results for each level of PatientRace for each variable, including variables for which such disaggregation is nonsensible (i.e., physID, PatientRace). If we had a large df, we might want to create a tiny df that only includes our variable(s) of interest. For now, it is not problematic to include all the variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(dfIndSamples, }\AttributeTok{group =}\NormalTok{ PatientRace, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              item group1 vars  n      mean        sd    median   trimmed
ID*1             1  Black    1 33 17.000000 9.6695398 17.000000 17.000000
ID*2             2  White    1 33 50.000000 9.6695398 50.000000 50.000000
PatientRace*1    3  Black    2 33  1.000000 0.0000000  1.000000  1.000000
PatientRace*2    4  White    2 33  2.000000 0.0000000  2.000000  2.000000
Verbal1          5  Black    3 33  7.614884 2.9854116  7.693516  7.733412
Verbal2          6  White    3 33  8.891483 3.2032222  7.979546  8.606615
Nonverbal1       7  Black    4 33  2.943125 0.9251164  2.885724  2.931841
Nonverbal2       8  White    4 33  2.965472 0.7001442  2.936787  2.995131
                     mad        min       max     range       skew    kurtosis
ID*1          11.8608000  1.0000000 33.000000 32.000000  0.0000000 -1.30951223
ID*2          11.8608000 34.0000000 66.000000 32.000000  0.0000000 -1.30951223
PatientRace*1  0.0000000  1.0000000  1.000000  0.000000        NaN         NaN
PatientRace*2  0.0000000  2.0000000  2.000000  0.000000        NaN         NaN
Verbal1        2.9075794  0.3507447 13.011100 12.660355 -0.3537887 -0.30860583
Verbal2        3.2861809  4.5891699 19.311207 14.722037  1.0170842  1.26737896
Nonverbal1     0.9185825  0.8333731  5.000000  4.166627  0.1150450 -0.04929788
Nonverbal2     0.5560620  1.1311619  4.350886  3.219724 -0.4143090  0.19115265
                     se
ID*1          1.6832508
ID*2          1.6832508
PatientRace*1 0.0000000
PatientRace*2 0.0000000
Verbal1       0.5196935
Verbal2       0.5576094
Nonverbal1    0.1610421
Nonverbal2    0.1218795
\end{verbatim}

In this analysis we are interested in the verbal variable. We see that patients who are Black received verbal interactions from physicians that were quantified by a mean score of 7.61 (\emph{SD} = 2.99); physicians' scores for White patients were 8.89 (\emph{SD} = 3.20). Skew and kurtosis values for the verbal ratings with Black patients were -.35 and -.31, respectively. They were 1.02 and 1.27 for White patients. As before, these fall well below the absolute values of 3 (skew) and 8 (kurtosis) that are considered to be concerning.

One of the assumptions of the independent samples \emph{t}-test is that the variances of the dependent variable are similar for both levels of the grouping factor. We can use the Levene's test to do this. We want this value to be non-significant (\(p\) \textgreater{} .05). If violated, we we can use the Welch's test because it is ``robust to the violation of the homogeneity of variance.''

In R, Levene's test is found in the \emph{car} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{leveneTest}\NormalTok{(Verbal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PatientRace, dfIndSamples, }\AttributeTok{center =}\NormalTok{ mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Levene's Test for Homogeneity of Variance (center = mean)
      Df F value Pr(>F)
group  1  0.1422 0.7074
      64               
\end{verbatim}

The results of the Levene's test are presented as an \emph{F} statistic. We'll get to \emph{F} distributions in the next chapter. For now, it is just important to know how to report and interpret them:

\begin{itemize}
\tightlist
\item
  Degrees of freedom are 1 and 74
\item
  The value of the \emph{F} statistic is 0.142
\item
  We want our \emph{p} value to be \textless{} .05
\end{itemize}

Happily, our Levene's result is (\emph{F}{[}1, 64{]} = 0.142, \emph{p} = .707) not significant. Because \emph{p} is less than 05, we have not violated the homogeneity of variance assumption. That is to say, the variance in each of the patient race groups is not statistically significantly different. we can use the regular (Student's) formulation of the \emph{t} test for independent samples.

\hypertarget{hand-calculations-1}{%
\subsection{Hand-Calculations}\label{hand-calculations-1}}

Earlier I presented a formula for the independent samples \emph{t}-test.

\[t = \frac{\bar{X}_1 - \bar{X}_2}{\mbox{SE}}\]
There are actually two formulations of the \emph{t}-test. Student's version can be used when there is no violation of the homogeneity of variance assumption; Welch's can be used when the homogeneity of variance assumption is violated. For the hand-calculation demonstration, I will only demonstrate the formula in the most ideal of circumstances, that is: there is no violation of the homogeneity of variance assumption and sample sizes are equal.

Even so, while the formula seems straightforward enough, calculating the SE in the denominator gets a little spicy:

\[t = \frac{\bar{X_{1}} -\bar{X_{2}}}{\sqrt{\frac{s_{1}^{2}}{N_{1}}+\frac{s_{2}^{2}}{N_{2}}}}\]
Let's first calculate the SE -- the value of the denominator. For this, we need the standard deviations for the dependent variable (verbal) for both levels of patient race. We obtained these earlier when we used the \emph{describeBy()} function in the \emph{psych} package.

The standard deviation of the verbal variable for the levels in the patient race group were 2.99 for Black patients and 3.20 for White patients; the \emph{N} in both our groups is 33. We can do the denominator math right in an R chunk:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{((}\FloatTok{2.985}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\DecValTok{33}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{3.203}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\DecValTok{33}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7621627
\end{verbatim}

Our \emph{SE} = 0.762

With the simplification of the denominator, we can easily calculate the independent sample \emph{t}-test.

\[t = \frac{\bar{X_{1}} -\bar{X_{2}}}{SE}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{7.615} \SpecialCharTok{{-}} \FloatTok{8.891}\NormalTok{)}\SpecialCharTok{/}\FloatTok{0.762}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -1.674541
\end{verbatim}

Hopefully, this hand-calculation provided an indication of how the means, standard deviation, and sample sizes contribute to the estimate of this \emph{t} test value. Now we ask, ``But it is statistically significant?''

\hypertarget{statistical-significance-1}{%
\subsubsection{Statistical Significance}\label{statistical-significance-1}}

The question of statistical significance testing invokes NHST (null hypothesis significance testing). In the case of the independent samples \emph{t}-test, the null hypothesis is that the two means are equal; the alternative is that they are not equal. Our test is of the null hypothesis. When the probability (\emph{p}) is less than the value we specify (usually .05), we are 95\% certain that the two means are not equal. Thus, we reject the null hypothesis (the one we tested) in favor of the alternative (that the means are not equal).

\[
\begin{array}{ll}
H_0: & \mu_1 = \mu_2  \\
H_A: & \mu_1 \neq \mu_2
\end{array}
\]
Although still used, NHST has its critiques. Among the critiques are the layers of logic and confusing language as we interpret the results.

Our \emph{t}-value was -1.675. We compare this value to the test critical value in a table of \emph{t} critical values. In-so-doing we must know our degrees of freedom. In the test that involves two levels of a grouping value, we will use \emph{N} -1 as the value for degrees of freedom. We must also specify the \emph{p} value (in our case .05) and whether-or-not our hypothesis is unidirectional or bi-directional. Our question only asked, ``Are the verbal communication levels different?'' In this case, the test is two-tailed, or bi-directional.

Let's return to the \href{https://www.statology.org/t-distribution-table/}{table of critical values} for the \emph{t} distribution to compare our \emph{t}-value (-1.675) to the column that is appropriate for our:

\begin{itemize}
\tightlist
\item
  Degrees of freedom (in this case \(N-2\) or 64)

  \begin{itemize}
  \tightlist
  \item
    We have two levels of a grouping value; for each our df is \(N-1\)
  \end{itemize}
\item
  Alpha, as represented by \(p < .05\)
\item
  Specification as a one-tailed or two-tailed test

  \begin{itemize}
  \tightlist
  \item
    Our alternative hypothesis made no prediction about the direction of the difference; therefore we will use a two-tailed test
  \end{itemize}
\end{itemize}

In the linked table, when the degrees of freedom reaches 30, there larger intervals. We will use the row representing degrees of freedom of 60. If our \emph{t} test value is lower than an absolute value of -1.675 or greater than the absolute value of 1.675, then our means are statistically significantly different from each other. In our case, we have not achieved statistical significance and we cannot say that the means are different. The \emph{t} string would look like this: \(t(64) = -1.675, p > .05\)

We can also use the \emph{qt()} function in base R. In the script below, I have indicated an alpha of .05. The ``2'' that follows indicates I want a two-tailed test. The 64 represents my degrees of freedom (\(N-2\)). In a two-tailed test, the regions of rejection will be below the lowerbound (lower.tail=TRUE) and above the upperbound (lower.tail=FALSE).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{64}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -1.99773
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{64}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.99773
\end{verbatim}

Given the large intervals, it makes sense that this test critical value is slightly different than the one fro the table.

\hypertarget{confidence-intervals-1}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals-1}}

How confident are we in our result? With independent samples \emph{t}-tests, it is common to report an interval in which we are 95\% confident that our true mean difference exists. Below is the formula, which involves:

\begin{itemize}
\tightlist
\item
  \(\bar{X_{1}}-\bar{X_{2}}\) the difference in the means
\item
  \(t_{cv}\) the test critical value for a two-tailed model (even if the hypothesis was one-tailed) where \(\alpha = .05\) and the degrees of freedom are \(N-2\)
\item
  \(SE\) the standard error used in the denominator of the test statistic
\end{itemize}

\[(\bar{X_{1}} -\bar{X_{2})} \pm  t_{cv}(SE)\]
Let's calculate it:

First, let's get the proper \emph{t} critical value. Even though these are identical to the one above, I am including them again. Why? Because if the original hypothesis had been one-tailed, we would need to calculate a two-tailed confidence interval; this is a placeholder to remind us.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{64}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -1.99773
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{64}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.99773
\end{verbatim}

With this in hand, let's calculate the confidence intervals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{7.614} \SpecialCharTok{{-}} \FloatTok{8.891}\NormalTok{) }\SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{1.99773} \SpecialCharTok{*} \FloatTok{0.762}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2.79927
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{7.614} \SpecialCharTok{{-}} \FloatTok{8.891}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{1.99773} \SpecialCharTok{*} \FloatTok{0.762}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2452703
\end{verbatim}

These values indicate the range of scores in which we are 95\% confident that our true mean difference (\(\bar{X_{1}}-\bar{X_{2}}\)) lies. Stated another way, we are 95\% confident that the true mean difference lies between -2.80 and 0.25 Because this interval crosses zero, we cannot rule out that the true mean difference is 0.00. This result is consistent with our non-significant \emph{p} value. For these types of statistics, the 95\% confidence interval and \emph{p} value will always be yoked together.

\hypertarget{effect-size-1}{%
\subsubsection{Effect Size}\label{effect-size-1}}

Whereas \emph{p} values address statistical significance, effect sizes address the magnitude of difference. There are two common effect sizes that are used with the independent samples \emph{t}-test. The first is the \emph{d} statistic, which measures, in standard deviation units, the distance between the two means. The simplest formula involves the \emph{t} value and sample sizes:

\[d = t\sqrt{\frac{N_{1}+N_{2}}{N_{1}N_{2}}}\]

With a \emph{t} value of -1.675 and sample sizes at 33 each, we can easily calculate this. Small, medium, and large sizes for the \emph{d} statistic are .2, .5, and .8, respectively (irrespective of sign).

\begin{Shaded}
\begin{Highlighting}[]
\SpecialCharTok{{-}}\FloatTok{1.675} \SpecialCharTok{*}\NormalTok{ (}\FunctionTok{sqrt}\NormalTok{((}\DecValTok{33} \SpecialCharTok{+} \DecValTok{33}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\DecValTok{33} \SpecialCharTok{*} \DecValTok{33}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.4123565
\end{verbatim}

Our value, -0.412 suggests a small-to-medium effect size. We might wonder why it wasn't statistically significant? Later we will discuss power and the relationship between sample size, one vs.~two-tailed hypotheses, and effect sizes.

Eta square, \(\eta^2\) is the proportion of variance of a test variable that is a function of the grouping variable. A value of 0 indicates that the difference in the mean scores is equal to 0, where a value of 1 indicates that the sample means differ, and the test scores do not differ within each group. The following equation can be used to compute \(\eta^2\). Conventionally, values of .01, .06, and .14 are considered to be small, medium, and large effect sizes, respectively.

\[\eta^{2} =\frac{t^{2}}{{t^{2}+(N_{1}+N_{2}-2)}}\]
Let's calculate it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{1.6745} \SpecialCharTok{*} \SpecialCharTok{{-}}\FloatTok{1.6745}\NormalTok{)}\SpecialCharTok{/}\NormalTok{((}\SpecialCharTok{{-}}\FloatTok{1.6745} \SpecialCharTok{*} \SpecialCharTok{{-}}\FloatTok{1.6745}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{33} \SpecialCharTok{+} \DecValTok{33} \SpecialCharTok{{-}} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.04197282
\end{verbatim}

Similarly, the \(\eta^2\) is small-to-medium.

\hypertarget{computation-in-r-1}{%
\section{Computation in R}\label{computation-in-r-1}}

Navarro's \emph{lsr} package makes the computation of the independent \emph{t}-test easy and produces output that is commonly used in psychological science.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{independentSamplesTTest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Verbal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PatientRace, }\AttributeTok{data =}\NormalTok{ dfIndSamples,}
    \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Student's independent samples t-test 

Outcome variable:   Verbal 
Grouping variable:  PatientRace 

Descriptive statistics: 
            Black White
   mean     7.615 8.891
   std dev. 2.985 3.203

Hypotheses: 
   null:        population means equal for both groups
   alternative: different population means in each group

Test results: 
   t-statistic:  -1.675 
   degrees of freedom:  64 
   p-value:  0.099 

Other information: 
   two-sided 95% confidence interval:  [-2.799, 0.246] 
   estimated effect size (Cohen's d):  0.412 
\end{verbatim}

This well-organized output has everything we need for an APA style presentation of results. Identical to all the information we hand-calculated, we would write the \emph{t} string this way: \emph{t}(64) = -1.675, \emph{p} = .099, \emph{d} = 0.412. The \emph{lsr} output also includes confidence intervals. These represent the 95\% confidence interval of the true difference between the means. That is, we are 95\% confident that the true difference between means could be as large as -2.799 or as (small/medium/large) as 0.246. What is critically important is that this confidence interval crosses zero. There is an important link between the CI95\% and statistical significance. When the CI95\% includes zero, \emph{p} will not be lower than 0.05.

\hypertarget{what-if-we-had-violated-the-homogeneity-of-variance-assumption}{%
\subsection{What if we had violated the homogeneity of variance assumption?}\label{what-if-we-had-violated-the-homogeneity-of-variance-assumption}}

Earlier we used the Levene's test to examine the homogeneity of variance assumption. If we had violated it, the Welch's formulation of the independent sample \emph{t} test is available to us. Navarro's \emph{lsr} package makes this easy. We simply change the \emph{var.equal} to \emph{FALSE}. This will produce the Welch's alternative, which takes into consideration violations of the homogeneity of variance assumption. Conveniently, ``Student's'' or ``Welch's'' will serve as the first row of the output.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{independentSamplesTTest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Verbal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ PatientRace, }\AttributeTok{data =}\NormalTok{ dfIndSamples,}
    \AttributeTok{var.equal =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Welch's independent samples t-test 

Outcome variable:   Verbal 
Grouping variable:  PatientRace 

Descriptive statistics: 
            Black White
   mean     7.615 8.891
   std dev. 2.985 3.203

Hypotheses: 
   null:        population means equal for both groups
   alternative: different population means in each group

Test results: 
   t-statistic:  -1.675 
   degrees of freedom:  63.685 
   p-value:  0.099 

Other information: 
   two-sided 95% confidence interval:  [-2.799, 0.246] 
   estimated effect size (Cohen's d):  0.412 
\end{verbatim}

Likely because of the similarity of the standard deviations associated with each level of patient race and our equal cell sizes, this changes nothing about our conclusion. Note that the degrees of freedom in the Student's \emph{t}-test analysis (the first one) was 64; in the Welch's version, the degrees of freedom is 63.685. It is this change that, when the homogeneity of variance assumption is violated, can make the Welch's results more conservative (i.e., less likely to have a statistically significant result).

\hypertarget{apa-style-results-1}{%
\section{APA Style Results}\label{apa-style-results-1}}

\begin{quote}
An independent samples \emph{t}-test was conducted to evaluate the hypothesis that there would be differences between the quality of physicians' verbal communication depending on whether the patient's race (Black, White). Although the independent samples \emph{t}-test was nonsignificant, \emph{t}(64) = -1.675, \emph{p} = .099, the effect size (\emph{d} = 0.412) was somewhat moderate in size. The 95\% confidence interval for the difference in means ranged from -2.799 to 0.246. Means and standard deviations are presented in Table 1; the results are illustrated in Figure 1.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.1way.table}\NormalTok{(PatientRace, Verbal, dfIndSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Descriptive statistics for Verbal as a function of PatientRace.  

 PatientRace    M   SD
       Black 7.61 2.99
       White 8.89 3.20

Note. M and SD represent mean and standard deviation, respectively.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(dfIndSamples, }\AttributeTok{x =} \StringTok{"PatientRace"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Verbal"}\NormalTok{, }\AttributeTok{color =} \StringTok{"PatientRace"}\NormalTok{,}
    \AttributeTok{palette =} \FunctionTok{c}\NormalTok{(}\StringTok{"\#00AFBB"}\NormalTok{, }\StringTok{"\#FC4E07"}\NormalTok{), }\AttributeTok{add =} \StringTok{"jitter"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Figure 1. Physician Verbal Engagement as a Function of Patient Race"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-140-1.pdf}

\hypertarget{power-in-independent-samples-t-tests-1}{%
\section{\texorpdfstring{Power in Independent Samples \emph{t} tests}{Power in Independent Samples t tests}}\label{power-in-independent-samples-t-tests-1}}

Researchers often use power analysis packages to estimate the sample size needed to detect a statistically significant effect, if, in fact, there is one. Utilized another way, these tools allows us to determine the probability of detecting an effect of a given size with a given level of confidence. If the probability is unacceptably low, we may want to revise or stop. A helpful overview of power as well as guidelines for how to use the \emph{pwr} package can be found at a \href{https://www.statmethods.net/stats/power.html}{Quick-R website} \citep{kabacoff_power_2017}.

In Champely's \emph{pwr} package, we can conduct a power analysis for a variety of designs, including the independent samples \emph{t} test that we worked in this lesson. There are a number of interrelating elements of power:

\begin{itemize}
\tightlist
\item
  Sample size, \emph{n} refers to the number of observations in each group; our vignette had 30
\item
  \emph{d} refers to the difference between means divided by the pooled standard deviation; ours was (7.615 - 8.891) = -1.276
\item
  \emph{power} refers to the power of a statistical test; conventionally it is set at .80
\item
  \emph{sig.level} refers to our desired alpha level; conventionally it is set at .05
\item
  \emph{type} indicates the type of test we ran; this was ``two.sample''
\item
  \emph{alternative} refers to whether the hypothesis is non-directional/two-tailed (``two.sided'') or directional/one-tailed(``less'' or ``greater'')
\end{itemize}

In this script, we must specify \emph{all-but-one} parameter; the remaining parameter must be defined as NULL. R will calculate the value for the missing parameter.

When we conduct a ``power analysis'' (i.e., the likelihood of a hypothesis test detecting an effect if there is one), we specify, ``power=NULL''. Using the data from our results, we learn from this first run, that our statistical power was 0.99. That is, given the value of the mean difference (1.276) we had a 99\% chance of detecting a statistically significant effect if there was one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =} \SpecialCharTok{{-}}\FloatTok{1.276}\NormalTok{, }\AttributeTok{n =} \DecValTok{33}\NormalTok{, }\AttributeTok{power =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{type =} \StringTok{"two.sample"}\NormalTok{,}
    \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Two-sample t test power calculation 

              n = 33
              d = 1.276
      sig.level = 0.05
          power = 0.9991669
    alternative = two.sided

NOTE: n is number in *each* group
\end{verbatim}

Researchers frequently use these tools to estimate the sample size required to obtain a statistically significant effect. In these scenarios we set \emph{n} to \emph{NULL}. Using the results from the simulation of our research vignette, you can see that we would have needed 11 individuals (per group; 22 total) for the \emph{p} value to be \textless{} .05.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =} \SpecialCharTok{{-}}\FloatTok{1.276}\NormalTok{, }\AttributeTok{n =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{type =} \StringTok{"two.sample"}\NormalTok{,}
    \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Two-sample t test power calculation 

              n = 10.69247
              d = 1.276
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number in *each* group
\end{verbatim}

Let's see if this is true. Below I will re-simulate the data for the verbal scores and change only the sample size:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220821}\NormalTok{)}
\CommentTok{\# sample size, M, and SD for Black then White patients}
\NormalTok{rVerbal }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{11}\NormalTok{, }\AttributeTok{mean =} \FloatTok{8.37}\NormalTok{, }\AttributeTok{sd =} \FloatTok{3.36}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{11}\NormalTok{, }\AttributeTok{mean =} \FloatTok{8.41}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{3.21}\NormalTok{))}
\CommentTok{\# set upper bound}
\NormalTok{rVerbal[rVerbal }\SpecialCharTok{\textgreater{}} \DecValTok{27}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{3}
\CommentTok{\# set lower bound}
\NormalTok{rVerbal[rVerbal }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\CommentTok{\# sample size, M, and SD for Black then White patients}
\NormalTok{rNonverbal }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{11}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.68}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.84}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{11}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.93}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.77}\NormalTok{))}
\CommentTok{\# set upper bound}
\NormalTok{rNonverbal[rNonverbal }\SpecialCharTok{\textgreater{}} \DecValTok{5}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{5}
\CommentTok{\# set lower bound}
\NormalTok{rNonverbal[rNonverbal }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}

\NormalTok{rID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{22}\NormalTok{))}
\CommentTok{\# name factors and identify how many in each group; should be in same}
\CommentTok{\# order as first row of script}
\NormalTok{rPatientRace }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Black"}\NormalTok{, }\DecValTok{11}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"White"}\NormalTok{, }\DecValTok{11}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{rdfIndSamples }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(rID, rPatientRace, rVerbal, rNonverbal)}

\NormalTok{rdfIndSamples}\SpecialCharTok{$}\NormalTok{rPatientRace }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(rdfIndSamples}\SpecialCharTok{$}\NormalTok{rPatientRace, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Black"}\NormalTok{,}
    \StringTok{"White"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{independentSamplesTTest}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rVerbal }\SpecialCharTok{\textasciitilde{}}\NormalTok{ rPatientRace, }\AttributeTok{data =}\NormalTok{ rdfIndSamples,}
    \AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Student's independent samples t-test 

Outcome variable:   rVerbal 
Grouping variable:  rPatientRace 

Descriptive statistics: 
            Black White
   mean     9.657 7.880
   std dev. 3.147 4.094

Hypotheses: 
   null:        population means equal for both groups
   alternative: different population means in each group

Test results: 
   t-statistic:  1.142 
   degrees of freedom:  20 
   p-value:  0.267 

Other information: 
   two-sided 95% confidence interval:  [-1.47, 5.025] 
   estimated effect size (Cohen's d):  0.487 
\end{verbatim}

Curiously, this did not result in a statistically significant result: \(t(20) = 1.142, p = 0.267, d = 0.487, CI95%[-1.47, 5.025]
\)

\hypertarget{practice-problems-2}{%
\section{Practice Problems}\label{practice-problems-2}}

The suggestions for homework are graded in complexity and I encourage you to select one or more that meets you where you are (e.g., in terms of your self-efficacy for statistics, your learning goals, and competing life demands).

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-1}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-1}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation of the research vignette, then rework the problem. This should provide minor changes to the data (maybe even in the second or third decimal point), but the results will likely be very similar. That said, don't be alarmed if what was non-significant in my working of the problem becomes significant. Our selection of \emph{p} \textless{} .05 (and the corresponding 95\% confidence interval) means that 5\% of the time there could be a difference in statistical significance.

\hypertarget{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation-1}{%
\subsection{Problem \#2: Rework the research vignette, but change something about the simulation}\label{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation-1}}

Rework the independent samples \emph{t} test in the lesson by changing something else about the simulation. You might have noticed that my re-simulation of a smaller sample size did not produce a statistically significant result. You may wish to pick a value in between the primary lecture \emph{N} and the re-simulation to see what it takes to achieve statistical significance. Alternatively, you could specify different means and/or standard deviations.

\hypertarget{problem-3-rework-the-research-vignette-but-swap-one-or-more-variables}{%
\subsection{Problem \#3: Rework the research vignette, but swap one or more variables}\label{problem-3-rework-the-research-vignette-but-swap-one-or-more-variables}}

Use the simulated data, but select the nonverbal communication variables that were evaluated in the Elliott et al. \citeyearpar{elliott_differences_2016}study. Compare your results to those reported in the mansucript.

\hypertarget{problem-4-use-other-data-that-is-available-to-you}{%
\subsection{Problem \#4: Use other data that is available to you}\label{problem-4-use-other-data-that-is-available-to-you}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete an independent samples \emph{t} test.

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the variables and their role in the analysis & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct an independent samples \emph{t} test (with an effect size) & 5 & \_\_\_\_\_ \\
5. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
6 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 30 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{tPaired}{%
\chapter{\texorpdfstring{Paired Samples \emph{t}-test}{Paired Samples t-test}}\label{tPaired}}

\href{link\%20here}{Screencasted Lecture Link}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{)  }\CommentTok{\#eliminates scientific notation}
\end{Highlighting}
\end{Shaded}

Researchers are often interested in knowing if participants score differently on some outcome variable (like affective well-being) across two conditions. These conditions could be before and after an intervention; they could also be interventionless exposures such as scary versus funny movies. In these simple designs, the paired \emph{t} test can be used to test the researchers' hypotheses.

\hypertarget{navigating-this-lesson-4}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-4}}

There is about \# hour and \#\# minutes of lecture. If you work through the materials with me it would be plan for an additional TIME.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-4}{%
\subsection{Learning Objectives}\label{learning-objectives-4}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Recognize the research questions for which utilization of paired sample \emph{t} tests would be appropriate.
\item
  Narrate the steps in conducting a paired samples \emph{t} test, beginning with testing the statistical assumptions through writing up an APA style results section.
\item
  Calculate a paired samples \emph{t} test in R (including effect sizes).
\item
  Interpret a 95\% confidence interval around a mean difference score.
\item
  Produce an APA style results for a paired-samples \emph{t} test.
\item
  Determine a sample size that (given a set of parameters) would likely result in a statistically significant effect, if there was one.
\end{itemize}

\hypertarget{planning-for-practice-3}{%
\subsection{Planning for Practice}\label{planning-for-practice-3}}

The suggestions for homework are graded in complexity. The more complete descriptions at the end of the chapter follow these suggestions.

\begin{itemize}
\tightlist
\item
  Rework the paired samples \emph{t} test in the lesson by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar.
\item
  Rework the paired samples \emph{t} test in the lesson by changing something else about the simulation. For example, if you are interested in power, consider changing the sample size.
\item
  Use the simulated data that is provided, but use the nonverbal variable, instead.
\item
  Conduct paired \emph{t} test with data to which you have access and permission to use. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources-3}{%
\subsection{Readings \& Resources}\label{readings-resources-3}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Navarro, D. (2020). Chapter 13: Comparing two means. In \href{https://learningstatisticswithr.com/}{Learning Statistics with R - A tutorial for Psychology Students and other Beginners}. Retrieved from \url{https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book\%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)}

  \begin{itemize}
  \tightlist
  \item
    Navarro's OER includes a good mix of conceptual information about \emph{t} tests as well as R code. My lesson integrates her approach as well as considering information from Field's \citeyearpar{field_discovering_2012} and Green and Salkind's \citeyearpar{green_using_2014} texts (as well as searching around on the internet).
  \end{itemize}
\item
  Elliott, A. M., Alexander, S. C., Mescher, C. A., Mohan, D., \& Barnato, A. E. (2016). Differences in Physicians' Verbal and Nonverbal Communication With Black and White Patients at the End of Life. \emph{Journal of Pain and Symptom Management, 51}(1), 1--8. \url{https://doi.org/10.1016/j.jpainsymman.2015.07.008}

  \begin{itemize}
  \tightlist
  \item
    The source of our research vignette.
  \end{itemize}
\end{itemize}

\hypertarget{packages-2}{%
\subsection{Packages}\label{packages-2}}

The script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(faux))\{install.packages(\textquotesingle{}faux\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(dplyr))\{install.packages(\textquotesingle{}dplyr\textquotesingle{})\}}
\CommentTok{\# if(!require(lsr))\{install.packages(\textquotesingle{}lsr\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\}}
\CommentTok{\# if(!require(pwr))\{install.packages(\textquotesingle{}pwr\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-the-paired-samples-t-test}{%
\section{\texorpdfstring{Introducing the Paired Samples \emph{t}-test}{Introducing the Paired Samples t-test}}\label{introducing-the-paired-samples-t-test}}

There are a couple of typical use cases for the paired samples \emph{t}-test. Repeated measures or change-over-time is a very common use. In this case, the research participant may take a pre-test, be exposed to an intervention or other type of stimulus, then take a post-test. Owing to the limitations of the statistics, all participants must be exposed to the same intervention/stimulus.

\begin{figure}
\centering
\includegraphics{images/ttests/prepost_paired.jpg}
\caption{An image of a row with three boxes: pre-test (in blue), intervention or exposure to stimulus (in light red), post-test (in blue) representing the use of a paired samples \emph{t}-test in a repeated measures design}
\end{figure}

A second common use is the assessment of a research participant in two competing conditions. An example might be the galvanic skin response ratings when a participant's hand is submerged in ice versus the GSR ratings when the hand is not exposed in ice. A strength of this design is the within-subjects' control of the participant.

\includegraphics{images/ttests/conditions_paired.jpg}
In the formula for the paired samples \emph{t} test we see a \(\bar{D}\) in the numerator. This represents the \emph{difference} between the continuously scaled scores in the two conditions. The denominator involves a standard deviation of the difference scores (\(\hat\sigma_D\)) and the square root of the sample size.

\[t = \frac{\bar{D}}{\hat\sigma_D / \sqrt{N}}\]
Although these types of research design and analyses are quite handy, they have some limitations. First, the paired samples \emph{t}-test cannot establish causality because it lacks elements such as comparing conditions (e.g., treatment vs.~control) and random assignment to those conditions. If a research wants to compare pre-post change as a result of participating in more-than-one condition, a \protect\hyperlink{Mixed}{mixed design ANOVA} would be a better option. Second, the paired samples \emph{t}-test cannot accommodate more than two comparison conditions. If the researcher wants to copare three or or more time periods or conditions, they will want to consider \protect\hyperlink{Repeated}{repeated measures ANOVA} or \href{https://lhbikos.github.io/MultilevelModeling/}{multilevel/hierarchical linear modeling}.

\hypertarget{workflow-for-paired-samples-t-test}{%
\section{\texorpdfstring{Workflow for Paired Samples \emph{t}-test}{Workflow for Paired Samples t-test}}\label{workflow-for-paired-samples-t-test}}

The following is a proposed workflow for conducting the paired samples \emph{t}-test.

\begin{figure}
\centering
\includegraphics{images/ttests/PairedSampleWrkFlw.jpg}
\caption{A colorful image of a workflow for the paired samples \emph{t} test}
\end{figure}

If the data meets the assumptions associated with the research design (e.g., independence of difference scores and a continuously scaled metric for that difference score), these are the steps for the analysis of an independent samples \emph{t} test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare (upload) data.
\item
  Explore data with

  \begin{itemize}
  \tightlist
  \item
    graphs
  \item
    descriptive statistics
  \end{itemize}
\item
  Assess normality of the difference scores via skew and kurtosis
\item
  Compute the paired samples \emph{t}-test
\item
  Compute an effect size (frequently the \emph{d} or \emph{eta} statistic)
\item
  Manage Type I error
\item
  Sample size/power analysis (which you should think about first, but in the context of teaching statistics, it's more pedagogically sensible, here).
\end{enumerate}

\hypertarget{research-vignette-3}{%
\section{Research Vignette}\label{research-vignette-3}}

Empirically published articles where \emph{t} tests are the primary statistic are difficult to locate. Having exhausted the psychology archives, I located this article in an interdisciplinary journal focused on palliative medicine. The research vignette for this lesson examined differences in physician's verbal and nonverbal communication with Black and White patients at the end of life \citep{elliott_differences_2016}.

Elliott and colleagues \citeyearpar{elliott_differences_2016} were curious to know if hospital-based physicians (56\% White, 26\% Asian, 7.4\% each Black and Hispanic) engaged in verbal and nonverbal communication differently with Black and White patients. Black and White patient participants were matched on characteristics deemed important to the researchers (e.g., critically and terminally ill, prognostically similar). Interactions in the intensive care unit were audio and video recorded and then coded on dimensions of verbal and nonverbal communication.

Because each physician saw a pair of patients (i.e., one Black patient and one White patient), the researchers utilized a paired samples, or dependent \emph{t}-test. This statistical choice was consistent with the element of the research design that controlled for physician effects through matching patients on critical characteristics. Below are the primary findings of the study.

\begin{longtable}[]{@{}llll@{}}
\toprule()
& Black Patients & White Patients & \\
\midrule()
\endhead
Category & \emph{Mean}(\emph{SD}) & \emph{Mean}(\emph{SD}) & \emph{p}-value \\
Verbal skill score (range 0 - 27) & 8.37(3.36) & 8.41(3.21) & 0.958 \\
Nonverbal skill score (range 0 - 5) & 2.68(.84) & 2.93(.77) & 0.014 \\
\bottomrule()
\end{longtable}

The primary analysis utilized by Elliott and colleagues \citeyearpar{elliott_differences_2016} was the paired samples \emph{t}-test. We will replicate that exact analysis with simulated data.

\hypertarget{simulating-data-for-the-paired-samples-t-test}{%
\subsection{\texorpdfstring{Simulating Data for the Paired Samples \emph{t} test}{Simulating Data for the Paired Samples t test}}\label{simulating-data-for-the-paired-samples-t-test}}

Below is the code I used to simulate the data. The following code assumes 33 physician participants who had separate interactions with critically ill, end-of-life stage patients, who were identified as Black and White. The Elliott et al. \citeyearpar{elliott_differences_2016} manuscript describe the process for coding verbal and nonverbal communication for video/audio recordings of the physician/patient interactions. Using that data, I simulate verbal and nonverbal communication scores for 33 physicians who rate patients who identify as Black and White, respectively. This creates four variables.

In the lesson, we will compare verbal communication scores. The nonverbal communication score is available as an option for practice.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# Setting the seed. If you choose this practice option, change the}
\CommentTok{\# number below to something different.}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220817}\NormalTok{)}
\CommentTok{\# These define the characteristics of the verbal variable. It is}
\CommentTok{\# essential that the object names (e.g., A\_mean) are not changed}
\CommentTok{\# because they will be fed to the function in the faux package.}
\NormalTok{sub\_n }\OtherTok{\textless{}{-}} \DecValTok{33}
\NormalTok{A\_mean }\OtherTok{\textless{}{-}} \FloatTok{8.37}
\NormalTok{B\_mean }\OtherTok{\textless{}{-}} \FloatTok{8.41}
\NormalTok{A\_sd }\OtherTok{\textless{}{-}} \FloatTok{3.36}
\NormalTok{B\_sd }\OtherTok{\textless{}{-}} \FloatTok{3.21}
\NormalTok{AB\_r }\OtherTok{\textless{}{-}} \FloatTok{0.3}

\CommentTok{\# the faux package can simulate a variety of data. This function}
\CommentTok{\# within the faux package will use the objects above to simulate}
\CommentTok{\# paired samples data}
\NormalTok{paired\_V }\OtherTok{\textless{}{-}}\NormalTok{ faux}\SpecialCharTok{::}\FunctionTok{rnorm\_multi}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sub\_n, }\AttributeTok{vars =} \DecValTok{2}\NormalTok{, }\AttributeTok{r =}\NormalTok{ AB\_r, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(A\_mean,}
\NormalTok{    B\_mean), }\AttributeTok{sd =} \FunctionTok{c}\NormalTok{(A\_sd, B\_sd), }\AttributeTok{varnames =} \FunctionTok{c}\NormalTok{(}\StringTok{"Verbal\_BL"}\NormalTok{, }\StringTok{"Verbal\_WH"}\NormalTok{))}

\NormalTok{paired\_V }\OtherTok{\textless{}{-}}\NormalTok{ paired\_V }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{PhysID =} \FunctionTok{row\_number}\NormalTok{())}

\CommentTok{\# Here, I repeated the process for the nonverbal variable.}
\NormalTok{sub\_n }\OtherTok{\textless{}{-}} \DecValTok{33}
\NormalTok{A\_mean }\OtherTok{\textless{}{-}} \FloatTok{2.68}
\NormalTok{B\_mean }\OtherTok{\textless{}{-}} \FloatTok{2.93}
\NormalTok{A\_sd }\OtherTok{\textless{}{-}} \FloatTok{0.84}
\NormalTok{B\_sd }\OtherTok{\textless{}{-}} \FloatTok{0.77}
\NormalTok{AB\_r }\OtherTok{\textless{}{-}} \FloatTok{0.9}

\NormalTok{paired\_NV }\OtherTok{\textless{}{-}}\NormalTok{ faux}\SpecialCharTok{::}\FunctionTok{rnorm\_multi}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sub\_n, }\AttributeTok{vars =} \DecValTok{2}\NormalTok{, }\AttributeTok{r =}\NormalTok{ AB\_r, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(A\_mean,}
\NormalTok{    B\_mean), }\AttributeTok{sd =} \FunctionTok{c}\NormalTok{(A\_sd, B\_sd), }\AttributeTok{varnames =} \FunctionTok{c}\NormalTok{(}\StringTok{"NVerb\_BL"}\NormalTok{, }\StringTok{"NVerb\_WH"}\NormalTok{))}

\CommentTok{\# This code produced an ID number for each physician}
\NormalTok{paired\_NV }\OtherTok{\textless{}{-}}\NormalTok{ paired\_NV }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{PhysID =} \FunctionTok{row\_number}\NormalTok{())}

\CommentTok{\# This data joined the two sets of data.  Note, I did not write any}
\CommentTok{\# code that assumed tha the verbal and nonverbal data came from the}
\CommentTok{\# same physician.  Full confession: I\textquotesingle{}m not quite sure how to do that}
\CommentTok{\# just yet.}
\NormalTok{dfPairedSamples }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(paired\_V, paired\_NV, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"PhysID"}\NormalTok{))}
\NormalTok{dfPairedSamples }\OtherTok{\textless{}{-}}\NormalTok{ dfPairedSamples }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(PhysID, }\FunctionTok{everything}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Before beginning our analysis, let's check the format of the variables to see if they are consistent with the scale of measurement of the variables. In our case, we expect to see four variables representing the verbal and nonverbal communication of the physicians with the patients who are identified as Black and White. Each of the variables should be continuously scaled and, therefore, should be formatted as \emph{num} (numerical).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(dfPairedSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   33 obs. of  5 variables:
 $ PhysID   : int  1 2 3 4 5 6 7 8 9 10 ...
 $ Verbal_BL: num  8.19 3.3 6.18 4.85 6.91 ...
 $ Verbal_WH: num  4.63 12.85 13.47 6.49 12.27 ...
 $ NVerb_BL : num  3.099 4.234 0.429 1.835 3.704 ...
 $ NVerb_WH : num  2.74 5.02 1.34 2.38 2.91 ...
\end{verbatim}

The four variables of interest are correctly formatted as \emph{num}. Because PhysID (physician ID) will not be used in our analysis, its structure is irrelevant.

Below is code for saving (and then importing) the data in .csv or .rds files. I make choices about saving data based on what I wish to do with the data. If I want to manipulate the data outside of R, I will save it as a .csv file. It is easy to open .csv files in Excel. A limitation of the .csv format is that it does not save any restructuring or reformatting of variables. For this lesson, this is not an issue.

Here is code for saving the data as a .csv and then reading it back into R. I have hashtagged these out, so you will need to remove the hashtags if you wish to run any of these operations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# writing the simulated data as a .csv write.table(dfPairedSamples,}
\CommentTok{\# file = \textquotesingle{}dfPairedSamples.csv\textquotesingle{}, sep = \textquotesingle{},\textquotesingle{}, col.names=TRUE,}
\CommentTok{\# row.names=FALSE) at this point you could clear your environment and}
\CommentTok{\# then bring the data back in as a .csv reading the data back in as a}
\CommentTok{\# .csv file dfPairedSamples\textless{}{-} read.csv (\textquotesingle{}dfPairedSamples.csv\textquotesingle{}, header}
\CommentTok{\# = TRUE)}
\end{Highlighting}
\end{Shaded}

The .rds form of saving variables preserves any formatting (e.g., creating ordered factors) of the data. A limitation is that these files are not easily opened in Excel. Here is the hashtagged code (remove hashtags if you wish to do this) for writing (and then reading) this data as an .rds file.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# saveRDS(dfPairedSamples, \textquotesingle{}dfPairedSamples.rds\textquotesingle{}) dfPairedSamples \textless{}{-}}
\CommentTok{\# readRDS(\textquotesingle{}dfPairedSamples.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-problem-2}{%
\section{Working the Problem}\label{working-the-problem-2}}

\hypertarget{stating-the-hypothesis-2}{%
\subsection{Stating the Hypothesis}\label{stating-the-hypothesis-2}}

In this lesson, I will focus on differences in the verbal communication variable. Specifically, I hypothesize that physician verbal communication scores for Black and White patients will differ. In the hypotheses below, the null hypothesis (\(\mu_D\)) states that the difference score is zero; the alternative hypothesis (\(\mu_D\)) states that the difference score is different from zero.

\[
\begin{array}{ll}
H_0: & \mu_D = 0  \\
H_A: & \mu_D \neq 0
\end{array}
\]
Notice the focus on a \emph{difference} score. Even thought the R package we will use does not require one for calculation, creating one in our df will be useful for preliminary exploration.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Creating the Verbal\_D variable within the dfPairedSamples df Doing}
\CommentTok{\# the \textquotesingle{}math\textquotesingle{} that informs that variable}
\NormalTok{dfPairedSamples}\SpecialCharTok{$}\NormalTok{Verbal\_D }\OtherTok{\textless{}{-}}\NormalTok{ (dfPairedSamples}\SpecialCharTok{$}\NormalTok{Verbal\_BL }\SpecialCharTok{{-}}\NormalTok{ dfPairedSamples}\SpecialCharTok{$}\NormalTok{Verbal\_WH)}
\CommentTok{\# Displaying the first six rows of the df to show that the difference}
\CommentTok{\# score now exists}
\FunctionTok{head}\NormalTok{(dfPairedSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  PhysID Verbal_BL Verbal_WH  NVerb_BL NVerb_WH  Verbal_D
1      1  8.190342  4.625680 3.0991101 2.742055  3.564663
2      2  3.297486 12.851362 4.2338398 5.024047 -9.553876
3      3  6.176386 13.466880 0.4288566 1.337259 -7.290495
4      4  4.851426  6.488762 1.8347393 2.379431 -1.637336
5      5  6.911155 12.266646 3.7035910 2.914445 -5.355491
6      6 11.965831  6.259292 1.5369696 1.598493  5.706540
\end{verbatim}

Examining this new variable, because we subtracted the verbal communication ratings of physicians with White patients from those of Black patients a negative score means that physicians had lower verbal engagement with Black patients; a positive score means that physicians had more verbal engagement with White patients.

\hypertarget{preliminary-exploration-2}{%
\subsection{Preliminary Exploration}\label{preliminary-exploration-2}}

Let's plot the data. The \emph{ggpubr} package is one of my go-to-tools for quick and easy plots of data. The \emph{ggpaired()} function is especially appropriate for paired data. A \href{https://rpkgs.datanovia.com/ggpubr/reference/ggpaired.html}{tutorial} is available at datanovia.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggpaired}\NormalTok{(dfPairedSamples, }\AttributeTok{cond1 =} \StringTok{"Verbal\_BL"}\NormalTok{, }\AttributeTok{cond2 =} \StringTok{"Verbal\_WH"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"condition"}\NormalTok{, }\AttributeTok{line.color =} \StringTok{"gray"}\NormalTok{, }\AttributeTok{palette =} \FunctionTok{c}\NormalTok{(}\StringTok{"npg"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Patient Race"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Verbal Communication Rating"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-154-1.pdf}
The box of the boxplot covers the middle 50\% (the interquartile range). The horizontal line is the median. The whiskers represent three standard deviations above and below the mean. Any dots beyond the whiskers are outliers.

We can begin to evaluate normality by obtaining the descriptive statistics with the \emph{describe()} function from the \emph{psych} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(dfPairedSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          vars  n  mean   sd median trimmed   mad   min   max range  skew
PhysID       1 33 17.00 9.67  17.00   17.00 11.86  1.00 33.00 32.00  0.00
Verbal_BL    2 33  8.70 2.80   9.09    8.80  3.10  1.94 13.34 11.40 -0.33
Verbal_WH    3 33  8.62 3.08   8.57    8.65  3.44  1.59 13.47 11.88 -0.15
NVerb_BL     4 33  2.73 1.00   2.63    2.78  1.26  0.43  4.23  3.80 -0.36
NVerb_WH     5 33  2.89 0.85   2.94    2.87  0.64  1.34  5.02  3.69  0.24
Verbal_D     6 33  0.08 4.14   0.61    0.27  4.11 -9.55  7.61 17.17 -0.41
          kurtosis   se
PhysID       -1.31 1.68
Verbal_BL    -0.47 0.49
Verbal_WH    -0.89 0.54
NVerb_BL     -0.85 0.17
NVerb_WH     -0.19 0.15
Verbal_D     -0.69 0.72
\end{verbatim}

From this, we can see the statistics for all of our variables, including the difference variable. Focused on the verbal communication scores, we see that the means are slightly higher for patients who are Black (\emph{M} = 8.70, \emph{SD} = 2.80), than patients who are White (\emph{M} = 8.62, \emph{SD} = 3.08). Further, the skew and kurtosis values are well below the areas of concern (below the absolute value of 3 for skew; below the absolute values of 8 for kurtosis) identified by Kline \citeyearpar{kline_principles_2016}.

Recall, though that the normality assumption for the paired samples \emph{t}-test concerns the difference score. We see that the mean difference is .08(\emph{SD} = 4.14). Its skew (-0.41) and kurtosis (-0.69) are also well-below the thresholds of concern.

\hypertarget{hand-calculations-2}{%
\subsection{Hand Calculations}\label{hand-calculations-2}}

Let's take another look at the formula for calculating paired samples \emph{t}-test.

\[t = \frac{\bar{D}}{\hat\sigma_D / \sqrt{N}}\]
We can use the data from our preliminary exploration in the calculation.

\begin{itemize}
\tightlist
\item
  The mean difference was .08
\item
  The standard deviation of that difference was 4.14
\item
  The sample size is 33
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.08}\SpecialCharTok{/}\NormalTok{(}\FloatTok{4.14}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.111006
\end{verbatim}

The resultant \emph{t} value is 0.111

Hopefully, this hand-calculation provided an indication of how the means, standard deviation, and sample sizes contribute to the estimate of this \emph{t} test value. Now we ask, ``But it is statistically significant?''

\hypertarget{statistical-significance-2}{%
\subsubsection{Statistical Significance}\label{statistical-significance-2}}

Our \emph{t}-value was 0.111. We compare this value to the test critical value in a table of \emph{t} critical values. In-so-doing we must know our degrees of freedom. Because the numerator in a paired samples \emph{t}-test is a single difference score \(\bar{D}\), the associated degrees of freedom is \(N-1\). We must also specify the \emph{p} value (in our case .05) and whether-or-not our hypothesis is unidirectional or bi-directional. Our question only asked, ``Are the verbal communication levels different?'' In this case, the test is two-tailed, or bi-directional.

Let's return to the \href{https://www.statology.org/t-distribution-table/}{table of critical values} for the \emph{t} distribution to compare our \emph{t}-value (0.111) to the column that is appropriate for our:

\begin{itemize}
\tightlist
\item
  Degrees of freedom (in this case \(N-1\) or 32)
\item
  Alpha, as represented by \(p < .05\)
\item
  Specification as a one-tailed or two-tailed test

  \begin{itemize}
  \tightlist
  \item
    Our alternative hypothesis made no prediction about the direction of the difference; therefore we will use a two-tailed test
  \end{itemize}
\end{itemize}

In the linked table, when the degrees of freedom reaches 30, there larger intervals. We will use the row representing degrees of freedom of 30. If our \emph{t} test value is lower than an absolute value of -2.042 or greater than the absolute value of 2.042, then our means are statistically significantly different from each other. In our case, we have not achieved statistical significance and we cannot say that the means are different. The \emph{t} string would look like this: \(t(32) = 0.111, p > .05\)

We can also use the \emph{qt()} function in base R. In the script below, I have indicated an alpha of .05. The ``2'' that follows indicates I want a two-tailed test. The 32 represents my degrees of freedom (\(N-1\)). In a two-tailed test, the regions of rejection will be below the lowerbound (lower.tail=TRUE) and above the upperbound (lower.tail=FALSE).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{32}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2.036933
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{32}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.036933
\end{verbatim}

If our \emph{t} value is below the lowerbound (-2.04) or above the upper bound (2.04), then we have rejected the null hypothesis in favor of the alternative. As we demonstrated in the hand-calculations, we have not. The ratings of physicians verbal engagement with patients who are racially identified as Black and White are not statistically significant.

\hypertarget{confidence-intervals-2}{%
\subsubsection{Confidence Intervals}\label{confidence-intervals-2}}

How confident are we in our result? With paired samples \emph{t}-tests, it is common to report an interval in which we are 95\% confident that our true mean difference exists. Below is the formula, which involves:

\begin{itemize}
\tightlist
\item
  \(\bar{D}\) the mean difference score
\item
  \(t_{cv}\) the test critical value for a two-tailed model (even if the hypothesis was one-tailed) where \(\alpha = .05\) and the degrees of freedom are \(N-1\)
\item
  \(s_{d}\) the standard deviation of \(\bar{D}\)
\item
  \(N\) sample size
\end{itemize}

\[\bar{D}\pm t_{cv}(s_{d}/\sqrt{n})\]
Let's calculate it:

First, let's get the proper \emph{t} critical value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{32}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2.036933
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qt}\NormalTok{(}\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{2}\NormalTok{, }\DecValTok{32}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.036933
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.08} \SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{2.037} \SpecialCharTok{*}\NormalTok{ ((}\FloatTok{4.14}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{)))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -1.388028
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.08} \SpecialCharTok{+}\NormalTok{ (}\FloatTok{2.037} \SpecialCharTok{*}\NormalTok{ ((}\FloatTok{4.14}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.548028
\end{verbatim}

These values indicate the range of scores in which we are 95\% confident that our true \(\bar{D}\) lies. Stated another way, we are 95\% confident that the true mean difference lies between -1.39 and 1.55. Because this interval crosses zero, we cannot rule out that the true mean difference is 0.00. This result is consistent with our non-significant \emph{p} value. For these types of statistics, the 95\% confidence interval and \emph{p} value will always be yoked together.

\hypertarget{effect-size-2}{%
\subsubsection{Effect Size}\label{effect-size-2}}

Effect sizes address the magnitude of difference. There are two common effect sizes that are used with the paired samples \emph{t}-test. The first is the \emph{d} statistic, which measures, in standard deviation units, the distance between the two means. Regardless of sign, values of .2, .5, and .8 are considered to be small, medium, and large, respectively.

Because the paired samples \emph{t} test used the difference score in the numerator, there are two easy options for calculating this effect:

\[d=\frac{\bar{D}}{\hat\sigma_D}=\frac{t}{\sqrt{N}}\]

The first is to use the mean and standard deviation associated with the difference score:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.08}\SpecialCharTok{/}\FloatTok{4.14}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01932367
\end{verbatim}

The formula uses the \emph{t} value and \emph{N}.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.111}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{33}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01932262
\end{verbatim}

Within rounding error, both calculations result in a value (\(d = 0.02\)) that is quite small.

Eta square, \(\eta^2\) is the proportion of variance of a test variable that is a function of the grouping variable. A value of 0 indicates that mean of the difference scores is equal to 0, where a value of 1 indicates that the difference scores in the sample are all the same nonzero value, and the test scores do not differ within each group. The following equation can be used to compute \(\eta^2\). Conventionally, values of .01, .06, and .14 are considered to be small, medium, and large effect sizes, respectively.

\[\eta^{2} =\frac{N(\bar{D}^{2})}{N(\bar{D}^{2}+(N-1)(\hat\sigma_D^{^{2}})}=\frac{t^{2}}{{t^{2}+(N_{1}-1)}}\]
The first calculation option uses the N and the mean difference score:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{33} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{0.08}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}\SpecialCharTok{/}\NormalTok{((}\DecValTok{33} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{0.08}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ ((}\DecValTok{33} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\FloatTok{4.14}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0003849249
\end{verbatim}

The second calculation option uses the \emph{t} values and sample size:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{0.111}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{((}\FloatTok{0.111}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{33} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0003848831
\end{verbatim}

Within rounding errors, and similar to our \emph{d} statistic, the \(\eta^2\) value (0.0004) is quite small.

\hypertarget{computation-in-r-2}{%
\section{Computation in R}\label{computation-in-r-2}}

Navarro's \emph{lsr} package makes the computation of the paired samples \emph{t}-test easy and produces output that is commonly used in psychological science.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{pairedSamplesTTest}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Verbal\_BL }\SpecialCharTok{+}\NormalTok{ Verbal\_WH, }\AttributeTok{data =}\NormalTok{ dfPairedSamples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Paired samples t-test 

Variables:  Verbal_BL , Verbal_WH 

Descriptive statistics: 
            Verbal_BL Verbal_WH difference
   mean         8.698     8.617      0.081
   std dev.     2.795     3.083      4.139

Hypotheses: 
   null:        population means equal for both measurements
   alternative: different population means for each measurement

Test results: 
   t-statistic:  0.113 
   degrees of freedom:  32 
   p-value:  0.911 

Other information: 
   two-sided 95% confidence interval:  [-1.386, 1.549] 
   estimated effect size (Cohen's d):  0.02 
\end{verbatim}

This well-organized output has everything we need for an APA style presentation of results. Identical to all the information we hand-calculated, we would write the \emph{t} string this way: \emph{t}(32) = 0.113, \emph{p} = .911, \emph{d} = 0.02. The \emph{lsr} output also includes confidence intervals. These represent the 95\% confidence interval of the true difference between the means. That is, we are 95\% confident that the true difference between means falls between the values of -1.386 and 1.549. What is critically important is that this confidence interval crosses zero. There is an important link between the CI95\% and statistical significance. When the CI95\% includes zero, \emph{p} will not be lower than 0.05.

\hypertarget{apa-style-results-2}{%
\section{APA Style Results}\label{apa-style-results-2}}

\begin{quote}
An paired samples \emph{t}-test was conducted to evaluate the hypothesis that there would be differences in the degree of physicians' verbal engagement as a function of the patient's race (Black, White). The paired samples \emph{t}-test was nonsignificant, \emph{t}(32) = 0.133, \emph{p} = .911. The small magnitude of the effect size (\emph{d} = 0.02) was consistent with the nonsignificant result. The 95\% confidence interval for the difference in means was quite wide and included the value of zero (95\%CI{[}-1.386, 1.549{]}). Means and standard deviations are presented in Table 1; the results are illustrated in Figure 1.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)  }\CommentTok{\#needed to use the pipe }
\CommentTok{\# Creating a smaller df to include only the variables I want in the}
\CommentTok{\# table}
\NormalTok{PairedDescripts }\OtherTok{\textless{}{-}}\NormalTok{ dfPairedSamples }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(Verbal\_BL, Verbal\_WH, Verbal\_D)}
\CommentTok{\# using the apa.cor.table function for means, standard deviations,}
\CommentTok{\# and correlations the filename command will write the table as a}
\CommentTok{\# word document to your file}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(PairedDescripts, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename =} \StringTok{"Tab1\_PairedV.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable     M    SD   1           2           
  1. Verbal_BL 8.70 2.80                         
                                                 
  2. Verbal_WH 8.62 3.08 .01                     
                         [-.33, .35]             
                                                 
  3. Verbal_D  0.08 4.14 .67**       -.74**      
                         [.42, .82]  [-.86, -.53]
                                                 

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggpaired}\NormalTok{(dfPairedSamples, }\AttributeTok{cond1 =} \StringTok{"Verbal\_BL"}\NormalTok{, }\AttributeTok{cond2 =} \StringTok{"Verbal\_WH"}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"condition"}\NormalTok{, }\AttributeTok{line.color =} \StringTok{"gray"}\NormalTok{, }\AttributeTok{palette =} \FunctionTok{c}\NormalTok{(}\StringTok{"npg"}\NormalTok{), }\AttributeTok{xlab =} \StringTok{"Patient Race"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Verbal Communication Rating"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Figure 1. Physician Verbal Engagement as a Function of Patient Race"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-166-1.pdf}

\hypertarget{power-in-paired-samples-t-tests}{%
\section{\texorpdfstring{Power in Paired Samples \emph{t} tests}{Power in Paired Samples t tests}}\label{power-in-paired-samples-t-tests}}

Researchers often use power analysis packages to estimate the sample size needed to detect a statistically significant effect, if, in fact, there is one. Utilized another way, these tools allows us to determine the probability of detecting an effect of a given size with a given level of confidence. If the probability is unacceptably low, we may want to revise or stop. A helpful overview of power as well as guidelines for how to use the \emph{pwr} package can be found at a \href{https://www.statmethods.net/stats/power.html}{Quick-R website} \citep{kabacoff_power_2017}.

In Champely's \emph{pwr} package, we can conduct a power analysis for a variety of designs, including the paired \emph{t} test that we worked in this chapter. There are a number of interrelating elements of power:

\begin{itemize}
\tightlist
\item
  Sample size, \emph{n} refers to the number of pairs; our vignette had 33
\item
  \emph{d} refers to the difference between means divided by the pooled standard deviation; using data from our vignette it would be (0-.08/4.14)
\item
  \emph{power} refers to the power of a statistical test; conventionally it is set at .80
\item
  \emph{sig.level} refers to our desired alpha level; conventionally it is set at .05
\item
  \emph{type} indicates the type of test we ran; ours was ``paired''
\item
  \emph{alternative} refers to whether the hypothesis is non-directional/two-tailed (``two.sided'') or directional/one-tailed(``less'' or ``greater'')
\end{itemize}

In this script, we must specify \emph{all-but-one} parameter; the remaining parameter must be defined as NULL. R will calculate the value for the missing parameter.

When we conduct a ``power analysis'' (i.e., the likelihood of a hypothesis test detecting an effect if there is one), we specify, ``power=NULL''. Using the data from our results, we learn from this first run, that our statistical power was at 5\%. That is, given the low value of the mean difference (.08) and the relatively large standard deviation (4.14), we had only a 5\% chance of detecting a statistically significant effect if there was one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{{-}} \FloatTok{0.08}\NormalTok{)}\SpecialCharTok{/}\FloatTok{4.14}\NormalTok{, }\AttributeTok{n =} \DecValTok{33}\NormalTok{, }\AttributeTok{power =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"paired"}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Paired t test power calculation 

              n = 33
              d = 0.01932367
      sig.level = 0.05
          power = 0.05133016
    alternative = two.sided

NOTE: n is number of *pairs*
\end{verbatim}

Researchers frequently use these tools to estimate the sample size required to obtain a statistically significant effect. In these scenarios we set \emph{n} to \emph{NULL}. Using the results from the simulation of our research vignette, you can see that we would have needed 21,022 individuals for the \emph{p} value to be \textless{} .05.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ (}\DecValTok{0} \SpecialCharTok{{-}} \FloatTok{0.08}\NormalTok{)}\SpecialCharTok{/}\FloatTok{4.14}\NormalTok{, }\AttributeTok{n =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{type =} \StringTok{"paired"}\NormalTok{, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Paired t test power calculation 

              n = 21021.66
              d = 0.01932367
      sig.level = 0.05
          power = 0.8
    alternative = two.sided

NOTE: n is number of *pairs*
\end{verbatim}

Let's see if this is true. Below I will re-simulate the data for the verbal scores, changing only the sample size:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{220820}\NormalTok{)}
\CommentTok{\# These define the characteristics of the verbal variable. It is}
\CommentTok{\# essential that the object names (e.g., A\_mean) are not changed}
\CommentTok{\# because they will be fed to the function in the faux package.}
\NormalTok{sub\_n }\OtherTok{\textless{}{-}} \DecValTok{21022}
\NormalTok{A\_mean }\OtherTok{\textless{}{-}} \FloatTok{8.37}
\NormalTok{B\_mean }\OtherTok{\textless{}{-}} \FloatTok{8.41}
\NormalTok{A\_sd }\OtherTok{\textless{}{-}} \FloatTok{3.36}
\NormalTok{B\_sd }\OtherTok{\textless{}{-}} \FloatTok{3.21}
\NormalTok{AB\_r }\OtherTok{\textless{}{-}} \FloatTok{0.3}

\CommentTok{\# the faux package can simulate a variety of data. This function}
\CommentTok{\# within the faux package will use the objects above to simulate}
\CommentTok{\# paired samples data}
\NormalTok{paired\_V2 }\OtherTok{\textless{}{-}}\NormalTok{ faux}\SpecialCharTok{::}\FunctionTok{rnorm\_multi}\NormalTok{(}\AttributeTok{n =}\NormalTok{ sub\_n, }\AttributeTok{vars =} \DecValTok{2}\NormalTok{, }\AttributeTok{r =}\NormalTok{ AB\_r, }\AttributeTok{mu =} \FunctionTok{c}\NormalTok{(A\_mean,}
\NormalTok{    B\_mean), }\AttributeTok{sd =} \FunctionTok{c}\NormalTok{(A\_sd, B\_sd), }\AttributeTok{varnames =} \FunctionTok{c}\NormalTok{(}\StringTok{"Verbal\_BL"}\NormalTok{, }\StringTok{"Verbal\_WH"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now I will conduct the paired samples \emph{t} test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{pairedSamplesTTest}\NormalTok{(}\AttributeTok{formula =} \SpecialCharTok{\textasciitilde{}}\NormalTok{Verbal\_BL }\SpecialCharTok{+}\NormalTok{ Verbal\_WH, }\AttributeTok{data =}\NormalTok{ paired\_V2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   Paired samples t-test 

Variables:  Verbal_BL , Verbal_WH 

Descriptive statistics: 
            Verbal_BL Verbal_WH difference
   mean         8.367     8.419     -0.052
   std dev.     3.359     3.213      3.867

Hypotheses: 
   null:        population means equal for both measurements
   alternative: different population means for each measurement

Test results: 
   t-statistic:  -1.936 
   degrees of freedom:  21021 
   p-value:  0.053 

Other information: 
   two-sided 95% confidence interval:  [-0.104, 0.001] 
   estimated effect size (Cohen's d):  0.013 
\end{verbatim}

The new results is much closer to statistical significance, but, wisely, remains non-significant: \(t(21021) = -1.936, p = .053, d = 0.013\).

Conducting power analyses requires that researchers speculate about their values. In this case, in order to estimate sample size, the researcher would need to make some guesses about the difference scores means and standard deviations. These values could be estimated from prior literature or a pilot study.

\hypertarget{practice-problems-3}{%
\section{Practice Problems}\label{practice-problems-3}}

The suggestions for homework are graded in complexity and I encourage you to select one or more that meets you where you are (e.g., in terms of your self-efficacy for statistics, your learning goals, and competing life demands).

\hypertarget{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-2}{%
\subsection{Problem \#1: Rework the research vignette as demonstrated, but change the random seed}\label{problem-1-rework-the-research-vignette-as-demonstrated-but-change-the-random-seed-2}}

If this topic feels a bit overwhelming, simply change the random seed in the data simulation of the research vignette, then rework the problem. This should provide minor changes to the data (maybe even in the second or third decimal point), but the results will likely be very similar. That said, don't be alarmed if what was non-significant in my working of the problem becomes significant. Our selection of \emph{p} \textless{} .05 (and the corresponding 95\% confidence interval) means that 5\% of the time there could be a difference in statistical significance.

\hypertarget{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation-2}{%
\subsection{Problem \#2: Rework the research vignette, but change something about the simulation}\label{problem-2-rework-the-research-vignette-but-change-something-about-the-simulation-2}}

Rework the paired samples \emph{t} test in the lesson by changing something else about the simulation. For example, if you are interested in understanding more about power, consider changing the sample size. Alternatively, you could specify different means and/or standard deviations.

\hypertarget{problem-3-rework-the-research-vignette-but-swap-one-or-more-variables-1}{%
\subsection{Problem \#3: Rework the research vignette, but swap one or more variables}\label{problem-3-rework-the-research-vignette-but-swap-one-or-more-variables-1}}

Use the simulated data, but select the nonverbal communication variables that were evaluated in the Elliott et al. \citeyearpar{elliott_differences_2016}study. Compare your results to those reported in the mansucript.

\hypertarget{problem-4-use-other-data-that-is-available-to-you-1}{%
\subsection{Problem \#4: Use other data that is available to you}\label{problem-4-use-other-data-that-is-available-to-you-1}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a paired samples \emph{t} test.

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the variables and their role in the analysis & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct a paired samples \emph{t} test (with an effect size) & 5 & \_\_\_\_\_ \\
5. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
6 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 30 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{analysis-of-variance}{%
\chapter*{Analysis of Variance}\label{analysis-of-variance}}
\addcontentsline{toc}{chapter}{Analysis of Variance}

\hypertarget{oneway}{%
\chapter{One-way ANOVA}\label{oneway}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=c88f8492-0599-462d-a471-ad8a01702156}{Screencasted Lecture Link}

One-way ANOVA allows the researcher to analyze mean differences between two or more groups on a between-subjects factor. For the one-way ANOVA, each case (i.e., individual, participant) must have scores on two variables: a factor and a dependent variable.

The factor must be categorical in nature, dividing the cases into two or more groups or levels. These levels could be ordered (e.g., placebo, low dose, high dose) or unordered (e.g., cognitive-behavioral, existential, psychodynamic). The dependent variable must be assessed on a quantitative, continuous dimension. The ANOVA \emph{F} test evaluates whether population means on the dependent variable differ across the levels of the factor.

One-way ANOVA can be used in experimental, quasi-experimental, and field studies. As we work through the chapter, we will examine some some of the requirements (assumptions) of the statistic in greater detail.

\hypertarget{navigating-this-lesson-5}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-5}}

There is about 2 hours of lecture. If you work through the materials with me, plan for another two hours of study.

\hypertarget{learning-objectives-5}{%
\subsection{Learning Objectives}\label{learning-objectives-5}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Evaluate the statistical assumptions associated with one-way analysis of variance (ANOVA).
\item
  Describe the relationship between model/between-subjects and residual/within-subjects variance.
\item
  Narrate the steps in conducting a formal one-way ANOVA beginning with testing the statistical assumptions through writing up an APA style results section.
\item
  Conduct a one-way ANOVA in R (including calculation of effect sizes and follow-up to the omnibus).
\item
  Conduct a power analysis for a one-way ANOVA.
\item
  Produce an APA style results section.
\end{itemize}

\hypertarget{planning-for-practice-4}{%
\subsection{Planning for Practice}\label{planning-for-practice-4}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option comes from the research vignette. The Tran et al. \citeyearpar{tran_you_2014} vignette has two variables where the authors have conducted one-way ANOVAs. I will demonstrate one (\emph{Accurate}) in this lecture; the second is available as one of the homework options.

As a third option, you are welcome to use data to which you have access and is suitable for two-way ANOVA. In either case the practice options suggest that you:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a one-way ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    follow-up (pairwise, planned comparisons, polynomial trends)
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{readings-resources-4}{%
\subsection{Readings \& Resources}\label{readings-resources-4}}

In preparing this chapter, I drew heavily from the following resource(s) that are freely available on the internet. Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Navarro, D. (2020). Chapter 14: Comparing Several Means (one-Way ANOVA). In \href{https://learningstatisticswithr.com/}{Learning Statistics with R - A tutorial for Psychology Students and other Beginners}. Retrieved from \url{https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book\%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)}

  \begin{itemize}
  \tightlist
  \item
    Navarro's OER includes a good mix of conceptual information about one-way ANOVA as well as R code. My code/approach is a mix of Green and Salkind's \citeyearpar{green_using_2014}, Field's \citeyearpar{field_discovering_2012}, Navarro's \citeyearpar{navarro_chapter_2020}, and other techniques I have found on the internet and learned from my students.
  \end{itemize}
\item
  Crump, M. J. C. (2018). Chapter 5.5.2, Simulating data for one-way between subjects design with 3 levels. In \href{https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html\#single-factor-anovas-data-simulation-and-analysis}{Programming for Psychologists: Data Creation and Analysis}. Retrieved from \url{https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html\#single-factor-anovas-data-simulation-and-analysis}

  \begin{itemize}
  \tightlist
  \item
    Although this reference is on simulating data, the process of simulation can provide another perspective on one-way ANOVA.
  \end{itemize}
\item
  Tran, A. G. T. T., \& Lee, R. M. (2014). You speak English well! Asian Americans' reactions to an exceptionalizing stereotype. \emph{Journal of Counseling Psychology, 61}(3), 484--490. \url{https://doi.org/10.1037/cou0000034}

  \begin{itemize}
  \tightlist
  \item
    The source of our research vignette.
  \end{itemize}
\end{itemize}

\hypertarget{packages-3}{%
\subsection{Packages}\label{packages-3}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed easy plotting for}
\CommentTok{\# simple ANOVA if(!require(gplots))\{install.packages(\textquotesingle{}gplots\textquotesingle{})\}}
\CommentTok{\# creating new variables and other handy functions}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\} a specific}
\CommentTok{\# part of the tidyverse with useful tools for manipulating data}
\CommentTok{\# if(!require(dplyr))\{install.packages(\textquotesingle{}dplyr\textquotesingle{})\} for descriptive}
\CommentTok{\# statistics and writing them as csv files}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\} a number of wrappers}
\CommentTok{\# for ANOVA models; today for evaluating the Shapiro}
\CommentTok{\# if(!require(rstatix))\{install.packages(\textquotesingle{}rstatix\textquotesingle{})\} produces effect}
\CommentTok{\# sizes if(!require(lsr))\{install.packages(\textquotesingle{}lsr\textquotesingle{})\} estimating sample}
\CommentTok{\# sizes and power analysis if(!require(pwr))\{install.packages(\textquotesingle{}pwr\textquotesingle{})\}}
\CommentTok{\# produces an APA style table for ANOVAs and other models}
\CommentTok{\# if(!require(apaTAbles))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\} helps with}
\CommentTok{\# formats like decimals and percentages for inline code}
\CommentTok{\# if(!require(formattable))\{install.packages(\textquotesingle{}formattable\textquotesingle{})\} more}
\CommentTok{\# effect size options}
\CommentTok{\# if(!require(effectsize))\{install.packages(\textquotesingle{}effectsize\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{workflow-for-one-way-anova}{%
\section{Workflow for One-Way ANOVA}\label{workflow-for-one-way-anova}}

The following is a proposed workflow for conducting a one-way ANOVA.

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw.jpg}
\caption{A colorful image of a workflow for the one-way ANOVA}
\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare (upload) data.
\item
  Explore data

  \begin{itemize}
  \tightlist
  \item
    graphs
  \item
    descriptive statistics
  \end{itemize}
\item
  Checking distributional assumptions

  \begin{itemize}
  \tightlist
  \item
    assessing normality via skew, kurtosis, Shapiro Wilks
  \item
    checking for violation of homogeneity of variance assumption with Levene's test; if we violate this we can use Welch's omnibus ANOVA
  \end{itemize}
\item
  Compute the omnibus ANOVA (remember to use Welch's if Levene's \emph{p} \textless{} .05)
\item
  Compute post-hoc comparisons, planned contrasts, or polynomial trends
\item
  Managing Type I error
\item
  Sample size/power analysis (which you should think about first -- but in the context of teaching ANOVA, it's more pedagogically sensible, here)
\end{enumerate}

\hypertarget{research-vignette-4}{%
\section{Research Vignette}\label{research-vignette-4}}

The \emph{exceptionalizing racial stereotype} is microaggression framed as interpersonally complimentary, but perpetuates negative stereotypical views of a racial/ethnic group. We are using data that is \emph{simulated} from a random clinical trial (RCT) conducted by Tran and Lee \citeyearpar{tran_you_2014}.

The one-way ANOVA examples we are simulating represent the post-only design which investigated three levels of the exceptionalizing stereotype in a sample of Asian American participants. This experimental design involved a confederate (posing as a peer) whose parting comment fell into the low racial loading, high racial loading, or control conditions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2388}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1791}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2687}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3134}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
COND
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Assignment
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Manipulation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Post-test Observation
\end{minipage} \\
\midrule()
\endhead
\textbf{Low} racial loading condition (\emph{n} = 22) & Random & Yes: ``Nice talking to you. You speak English well.'' & \textbf{Accurate} \\
\textbf{High} racial loading (\emph{n} = 23) & Random & Yes: ``Nice talking to you. You speak English well for an Asian.'' & \textbf{Accurate} \\
\textbf{Control} (\emph{n} = 23) & Random & No: ``Nice talking to you.'' & \textbf{Accurate} \\
\bottomrule()
\end{longtable}

Tran and Lee \citeyearpar{tran_you_2014} reported results from two ANOVAs and 4 ANCOVAs, using a pre-test as a covariate. A preprint of their article is available \href{https://pdfs.semanticscholar.org/4146/b528961c041de317c6a4c699f12fc5a4bc22.pdf?_ga=2.179078439.2028716028.1610939782-1660125104.1610939782}{here}.

\begin{itemize}
\tightlist
\item
  \textbf{Accurate} is the DV we will be exploring in this lesson. Participants rated how \emph{accurate} they believed their partner's impression of them was (0 = \emph{very inaccurate}, 3 = \emph{very accurate}).
\item
  \textbf{moreTalk} is the DV suggested as a practice problem. Participants rated how much longer they would continue the interaction with their partner compared to their interactions in general (-2 = \emph{much less than average}, 0 = \emph{average}, 2 = \emph{much more than average}).
\end{itemize}

\hypertarget{data-simulation}{%
\subsection{Data Simulation}\label{data-simulation}}

Simulating data for a one-way ANOVA requires the sample size (rnorm), mean (mean), and standard deviation (sd) for each of the groups \citep{crump_simulating_2018}. In creating this simulation, I used the data from Table 1 in the Tran and Lee \citeyearpar{tran_you_2014} article. Having worked the problem several times, I made one change. The group sizes in the original study were 23, 22, and 23. To increase the probability that we would have statistically significant results in our worked example, I increased the sample sizes to 30 for each group. In this way we have a perfectly \emph{balanced} (equal cell sizes) design.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note, this script results in a different simulation than is in the}
\CommentTok{\# ReadySetR lesson sets a random seed so that we get the same results}
\CommentTok{\# each time}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210820}\NormalTok{)}
\CommentTok{\# sample size, M and SD for each group}
\NormalTok{Accurate }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.18}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.8}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.83}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.58}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.76}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.56}\NormalTok{))}
\CommentTok{\# set upper bound for DV}
\NormalTok{Accurate[Accurate }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{3}
\CommentTok{\# set lower bound for DV}
\NormalTok{Accurate[Accurate }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\CommentTok{\# sample size, M and SD for each group}
\NormalTok{moreTalk }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{0.82}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.91}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{0.39}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.66}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{30}\NormalTok{, }\AttributeTok{mean =} \SpecialCharTok{{-}}\FloatTok{0.04}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.71}\NormalTok{))}
\CommentTok{\# set upper bound for DV}
\NormalTok{moreTalk[moreTalk }\SpecialCharTok{\textgreater{}} \DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{2}
\CommentTok{\# set lower bound for DV}
\NormalTok{moreTalk[moreTalk }\OtherTok{\textless{}{-}} \DecValTok{2}\NormalTok{] }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\DecValTok{2}
\CommentTok{\# IDs for participants}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{90}\NormalTok{))}
\CommentTok{\# name factors and identify how many in each group; should be in same}
\CommentTok{\# order as first row of script}
\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"High"}\NormalTok{, }\DecValTok{30}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\DecValTok{30}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\DecValTok{30}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{accSIM30 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, COND, Accurate, moreTalk)}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-problem-3}{%
\section{Working the Problem}\label{working-the-problem-3}}

\hypertarget{preparing-the-data}{%
\subsection{Preparing the Data}\label{preparing-the-data}}

Examining the data is important for several reasons. First, we can begin to inspect for any anomalies. Second, if we are confused about what statistic we wish to apply, understanding the characteristics of the data can provide clues.

In R markdown we can

\begin{itemize}
\tightlist
\item
  look at the data by clicking on it, and
\item
  examine its structure with the \emph{str()} function.
\end{itemize}

Let's do both.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   90 obs. of  4 variables:
 $ ID      : Factor w/ 90 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ COND    : chr  "High" "High" "High" "High" ...
 $ Accurate: num  0.42 1.123 0.885 1.569 1.831 ...
 $ moreTalk: num  -0.64 -2 -0.25 0.146 -0.996 ...
\end{verbatim}

If we look at this simple dataset, we see that we see that

\begin{itemize}
\tightlist
\item
  \textbf{COND} is a grouping variable) with 3 levels (high, low, control)

  \begin{itemize}
  \tightlist
  \item
    it is presently in ``chr'' (character) format, it needs to be changed to be a factor.
  \end{itemize}
\item
  \textbf{Accurate} is a continuous variable

  \begin{itemize}
  \tightlist
  \item
    it is presently in ``num'' (numerical) format, this is an appropriate format.
  \end{itemize}
\item
  \textbf{moreTalk} is a continuous variable

  \begin{itemize}
  \tightlist
  \item
    it is presently in ``num'' (numerical) format, this is an appropriate format
  \end{itemize}
\end{itemize}

There are many ways to convert variables to factors; here is one of the simplest.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#convert variable to factor}
\NormalTok{accSIM30}\SpecialCharTok{$}\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{COND)}
\end{Highlighting}
\end{Shaded}

Let's recheck the structure

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   90 obs. of  4 variables:
 $ ID      : Factor w/ 90 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ COND    : Factor w/ 3 levels "Control","High",..: 2 2 2 2 2 2 2 2 2 2 ...
 $ Accurate: num  0.42 1.123 0.885 1.569 1.831 ...
 $ moreTalk: num  -0.64 -2 -0.25 0.146 -0.996 ...
\end{verbatim}

By default, R orders factors alphabetically. This means, analyses will assume that ``Control'' (C) is the lowest condition, then ``High,'' then ``Low.'' Since these have theoretically ordered values, we want them in the order of ``Control,'' ``Low,'' ``High.''

Here is the script to create an ordered factor. The order in which the variables are entered in the concatenated list (``c'') establishes the order (e.g., levels).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ordering the factor}
\NormalTok{accSIM30}\SpecialCharTok{$}\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{COND, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Low"}\NormalTok{, }\StringTok{"High"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Again, we can check our work.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#another structure check}
\FunctionTok{str}\NormalTok{(accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   90 obs. of  4 variables:
 $ ID      : Factor w/ 90 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ COND    : Factor w/ 3 levels "Control","Low",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Accurate: num  0.42 1.123 0.885 1.569 1.831 ...
 $ moreTalk: num  -0.64 -2 -0.25 0.146 -0.996 ...
\end{verbatim}

Now our variables are suitable for analysis.

At this point, you may wish to export and/or import the data as a .csv (think ``Excel lite'') or .rds (R object that preserves the information about the variables -- such changing COND to an ordered factor), here is the code to do so. The data should save in the same folder as the .rmd file. Therefore, it is really important (think, ``good R hygiene'') to have organized your folders so that your .rmd and data files are co-located.

I have hashtagged out the code. If you wish to use it, delete the hashtags. Although I show the .csv code first, my personal preference is to save R data as .rds files. While they aren't easy to ``see'' as an independent file, they retain the formatting of the variables. For a demonstration, refer back to the \protect\hyperlink{Ready}{Ready\_Set\_R} lesson.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(accSIM30,}
\CommentTok{\# file=\textquotesingle{}accSIM.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file acc\_csv \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}accSIM.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# a quick demo to show that the .csv format loses the variable}
\CommentTok{\# formatting str(acc\_csv)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(accSIM30, \textquotesingle{}accSIM.rds\textquotesingle{}) bring back the simulated dat}
\CommentTok{\# from an .rds file acc\_RDS \textless{}{-} readRDS(\textquotesingle{}accSIM.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# a quick demo to show that the .rds format preserves the variable}
\CommentTok{\# formatting str(acc\_RDS)}
\end{Highlighting}
\end{Shaded}

Note that I renamed each of these data objects to reflect the form in whic I saved them (i.e., ``acc\_csv'', ``acc\_RDS''). If you have followed this step, you will want to rename the file before continuing with the rest of the chapter. Alternatively, you can start from scratch, re-run the code to simulate the data, and skip this portion on importing/exporting data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#accSIM30 \textless{}{-} acc\_RDS}
\CommentTok{\#or}
\CommentTok{\#accSIM30 \textless{}{-} acc\_csv}
\end{Highlighting}
\end{Shaded}

\hypertarget{exploring-the-distributional-characteristics-numerically}{%
\subsection{Exploring the Distributional Characteristics Numerically}\label{exploring-the-distributional-characteristics-numerically}}

We will explore the data such that you will have several tools for future exploration. In this first demonstration I will quickly produce a mean and standard deviation.

These functions are in base R. The \emph{aggregate()} function lets R know we want output by a grouping variable. We then list the variable of interest, a tilda (I think of the word ``by''), and then the grouping variable (I think ``Accurate by COND''). Finally we list the dataframe and the statistic (e.g., mean or standard deviation). R is case sensitive to check your capitalization if your code fails to execute.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aggregate}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     COND Accurate
1 Control 1.756195
2     Low 1.900116
3    High 1.152815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aggregate}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     COND  Accurate
1 Control 0.4603964
2     Low 0.6301138
3    High 0.6587486
\end{verbatim}

Before looking at graphs, we can see that racially loaded \emph{high} condition has the lowest accuracy score and the largest variability. Let's produce some helpful figures so that we can visualize this.

\hypertarget{exploring-the-distributional-characteristics-graphically}{%
\subsection{Exploring the Distributional Characteristics Graphically}\label{exploring-the-distributional-characteristics-graphically}}

The package \emph{gplots} produces a simple line graph and the script is fairly intuitive. The \emph{plotmeans()} function plots the means with error bars (95\% confidence intervals) around the mean. Regarding the confidence intervals, we can think, ``How confident are we that the mean is this particular value?'' Earlier we noted that the ``high racial loading condition'' had the lowest mean and the widest variability. Is this apparent from the graph?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# plots DV by IV}
\NormalTok{gplots}\SpecialCharTok{::}\FunctionTok{plotmeans}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30, }\AttributeTok{xlab =} \StringTok{"Racial Loading Condition"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Accuracy of Confederate\textquotesingle{}s Impression"}\NormalTok{, }\AttributeTok{n.label =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-185-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this code could be more elegantly written in one row plotmeans}
\CommentTok{\# (formula = Accurate \textasciitilde{} COND, data = accSIM30, xlab = \textquotesingle{}Racial Loading}
\CommentTok{\# Condition\textquotesingle{}, ylab = \textquotesingle{}Accuracy of Confederate\textquotesingle{}s Impression\textquotesingle{}, n.label}
\CommentTok{\# = TRUE)}
\end{Highlighting}
\end{Shaded}

Boxplots, with the \emph{boxplot2()} function provide another view of our data. In boxplots the center value is the median. The box spans the \emph{interquartile range} and ranges from the 25th to the 75th percentile. The whiskers cover 1.5 times the interquartile range. When this does not capture the entire range, outliers are represented with dots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gplots}\SpecialCharTok{::}\FunctionTok{boxplot2}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30, }\AttributeTok{xlab =} \StringTok{"Racial Loading Condition"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Accuracy of Confederate\textquotesingle{}s Impression"}\NormalTok{, }\AttributeTok{n.label =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-186-1.pdf}

From both the boxplot and the linegraph with error bars, we can see that participants in the low racial loading condition have the highest accuracy ratings. This is followed by the control and then high racial loading conditions. Are these differences statistically significant? This is why we need the one-way ANOVA.

\hypertarget{understanding-anova-with-hand-calculations}{%
\section{\texorpdfstring{Understanding ANOVA with \emph{Hand Calculations}}{Understanding ANOVA with Hand Calculations}}\label{understanding-anova-with-hand-calculations}}

ANOVA was developed by Sir Ronald Fisher in the early 20th century. The name is a bit of a misnomer -- rather than analyzing \emph{variances}, we are investigating differences in \emph{means} (but the formula does take variances into consideration\ldots stay tuned).

ANOVA falls squarely within the tradition of \textbf{null hypothesis significance testing} (NHST). As such, a formal, traditional, ANOVA begins with statements of the null and alternate hypotheses. \emph{Note. In their article, Tran and Lee \citeyearpar{tran_you_2014} do not list such.}

In our example, we would hypothesize that the population means (i.e., Asian or Asian American individuals in the U.S.) are equal:
\[H_{O}: \mu _{1} = \mu _{2} = \mu _{3}\]
There are an number of ways that the \(H_{O}\) could be false. Here are a few:
\[H_{a1}: \mu _{1} \neq \mu _{2} \neq \mu _{3}\]
\[H_{a2}: \mu _{1} = \mu _{2} > \mu _{3}\]
\[H_{a3}: \mu _{1} > \mu _{2} > \mu _{3}\]
The bottom line is that if we have a statistically significant omnibus ANOVA (i.e., the test of the overall significance of the model) and the \(H_{O}\) is false, somewhere between the three levels of the grouping factor, the means are statistically significantly different from each other.

In evaluating the differences between means, one-way ANOVA compares:

\begin{itemize}
\tightlist
\item
  systematic variance to unsystematic variance
\item
  explained to unexplained variation
\item
  experimental effect to the individual differences
\item
  model variance to residual variance
\item
  between group variance to within group variance
\end{itemize}

The ratio of these variances is the \emph{F}-ratio.

Navarro \citeyearpar{navarro_book_2020} offers a set of useful figures to compare between- and within-group variation.

\begin{figure}
\centering
\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-187-1.pdf}
\caption{\label{fig:unnamed-chunk-187}Graphical illustration of ``between groups'' variation}
\end{figure}

\begin{figure}
\centering
\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-188-1.pdf}
\caption{\label{fig:unnamed-chunk-188}Graphical illustration of ``within groups'' variation}
\end{figure}

When between-group variance (i.e,. model variance) is greater than within-group variance (i.e., residual variance) there may be support to suggest that there are statistically significant differences between groups.

Let's examine how variance is partitioned by hand-calculating sums of squares total, model, and residual. Along the way we will use some basic R skills to manipulate the data.

\hypertarget{sums-of-squares-total}{%
\subsection{Sums of Squares Total}\label{sums-of-squares-total}}

Sums of squares total represents the total amount of variance within our data. Examining the formula(s; there are variants of each) can help us gain a conceptual understanding of this.

In this first version of the formula we can see that the grand (or overall) mean is subtracted from each individual score, squared, and then summed. This makes sense: \emph{sums of squares, total}.

\[SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}\]
In the next version of the formula we see that the sums of square total is the addition of the sums of squares model and residual.

\[SS_{T}= SS_{M} + SS_{R}\]

``Between'' and ``within'' are another way to understand ``model'' and ``residual.'' This is reflected in the next formula.

\[SS_{T}= SS_{B} + SS_{W}\]
Finally, think of the sums of squares total as the grand variance multiplied by the overall degrees of freedom (\emph{N} - 1).

\[SS_{T}= s_{grand}^{2}(n-1)\]
Let's take a moment to \emph{hand-calculate} \(SS_{T}\). Not to worry -- we'll get R to do the math for us!

Our grand (i.e., overall) mean is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GrandMean }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{Accurate)}
\NormalTok{GrandMean}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.603042
\end{verbatim}

Subtracting the grand mean from each Accurate rating yields a mean difference. In the script below I have used the \emph{mutate()} function from the \emph{dplyr} package (a part of the \emph{tidyverse}) to created a new variable ('' m\_dev'') in the dataframe. The \emph{tidyverse} package is one of the few exceptions that I will open via the library. This is because we need it if we are going to use the pipe (\%\textgreater\%) to string parts of our script together.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{accSIM30 }\OtherTok{\textless{}{-}}\NormalTok{ accSIM30 }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_dev =}\NormalTok{ Accurate}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(Accurate))}

\FunctionTok{head}\NormalTok{(accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID COND  Accurate   moreTalk       m_dev
1  1 High 0.4203896 -0.6398265 -1.18265259
2  2 High 1.1226505 -2.0000000 -0.48039170
3  3 High 0.8852238 -0.2497750 -0.71781837
4  4 High 1.5689439  0.1455637 -0.03409829
5  5 High 1.8307196 -0.9960413  0.22767748
6  6 High 1.8874431 -1.0692978  0.28440098
\end{verbatim}

Pop quiz: What's the sum of our new \emph{m\_dev} variable? Let's check.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{m\_dev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.00000000000000003830065
\end{verbatim}

Unless you run the script at the top of this document (``options(scipen=999)''), R will (seemingly selectively) use \textbf{scientific e notation} to report your results. The proper value is one where the base number (before the ``e'') is multiplied by 10, raised to the power shown: \(3.830065 * 10^{17}\) Another way to think of it is to move the decimal 17 places to the left. In any case, this number is essentially zero.

Back to the point of sums of squares total, the sum of deviations around the grand mean will always be zero. To make them useful, we must square them:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accSIM30 }\OtherTok{\textless{}{-}}\NormalTok{ accSIM30 }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_devSQ =}\NormalTok{ m\_dev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\FunctionTok{head}\NormalTok{(accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID COND  Accurate   moreTalk       m_dev     m_devSQ
1  1 High 0.4203896 -0.6398265 -1.18265259 1.398667144
2  2 High 1.1226505 -2.0000000 -0.48039170 0.230776185
3  3 High 0.8852238 -0.2497750 -0.71781837 0.515263216
4  4 High 1.5689439  0.1455637 -0.03409829 0.001162694
5  5 High 1.8307196 -0.9960413  0.22767748 0.051837034
6  6 High 1.8874431 -1.0692978  0.28440098 0.080883915
\end{verbatim}

If we sum the squared mean deviations we will obtain the total variance (sums of squares total):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{m\_devSQ)}
\NormalTok{SST}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 39.67818
\end{verbatim}

This value, the sum of squared deviations around the grand mean, is our \(SS_T\). The associated \emph{degrees of freedom} is \(N - 1\); in our case this is 90-1 = 89.

In one-way ANOVA, we divide \(SS_T\) into \textbf{model/between sums of squares} and \textbf{residual/within sums of squares}.

The \emph{model} generally represents the notion that the means are different than each other. We want the variation between our means to be greater than the variation within each of the groups from which our means are calculated.

\hypertarget{sums-of-squares-for-the-model-or-between}{%
\subsection{Sums of Squares for the Model (or Between)}\label{sums-of-squares-for-the-model-or-between}}

We just determined that the total amount of variation within the data is 39.678 units. From this we can estimate how much of this variation our model can explain. \(SS_M\) tells us how much of the total variation can be explained by the fact that different data points come from different groups.

We see this reflected in the formula below, where

\begin{itemize}
\tightlist
\item
  the grand mean is subtracted from each group mean
\item
  this value is squared and multiplied by the number of cases in each group
\item
  these values are summed
\end{itemize}

\[SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}\]

To calculate this, we start with the grand mean (previously calculated): 1.603.

We also estimate the group means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{GroupMeans }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, mean)}
\NormalTok{GroupMeans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     COND Accurate
1 Control 1.756195
2     Low 1.900116
3    High 1.152815
\end{verbatim}

This formula occurs in three chunks, representing the control, low, and high racial loading conditions. In each of the chunks we have the \(n\), group mean, and grand mean.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculated by using object names from our calculations}
\NormalTok{SSM }\OtherTok{\textless{}{-}}\NormalTok{ nControl }\SpecialCharTok{*}\NormalTok{ (ControlMean }\SpecialCharTok{{-}}\NormalTok{ GrandMean)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ nLow }\SpecialCharTok{*}\NormalTok{ (LowMean }\SpecialCharTok{{-}}\NormalTok{ GrandMean)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}
\NormalTok{    nHigh }\SpecialCharTok{*}\NormalTok{ (HighMean }\SpecialCharTok{{-}}\NormalTok{ GrandMean)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{SSM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.432
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculated by specifying the actual values from our calculations}
\DecValTok{30} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{1.756} \SpecialCharTok{{-}} \FloatTok{1.603}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{1.9} \SpecialCharTok{{-}} \FloatTok{1.603}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{30} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{1.153} \SpecialCharTok{{-}} \FloatTok{1.603}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.42354
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Both result in the same}
\end{Highlighting}
\end{Shaded}

This value, \(SS_M\) is the amount of variance accounted for by the model; that is, the the amount of variance accounted for by the grouping variable/factor, COND. Degrees of freedom for \(SS_M\) is always one less than the number of elements (e.g., groups) used in its calculation (\(k-1\)). Because we have three groups, our degrees of freedom for the model is two.

\hypertarget{sums-of-squares-residual-or-within}{%
\subsection{Sums of Squares Residual (or within)}\label{sums-of-squares-residual-or-within}}

To recap, we know there are 39.678 units of variation to be explained in our data. Our model explains 9.432 of these units. Sums of squares residual tells us how much of the variation cannot be explained by the model. This value is influenced by extraneous factors; some will refer to it as ``noise.''

Looking at the formula can assist us in with a conceptual formula. In \(SS_R\) we subtract the group mean from each individual member of the group and then square it.

\[SS_{R}= \sum(x_{ik}-\bar{x}_{k})^{^{2}}\]
Below is another approach to calculating\(SS_R\). In this one the variance for each group is multiplied by its respective degrees of freedom, then summed.

\[SS_{R}= s_{group1}^{2}(n-1) + s_{group2}^{2}(n-1) + s_{group3}^{2}(n-1))\]
Again, the formula is in three chunks -- but this time the calculations are \emph{within-group}. We need the variance (the standard deviation squared) for the calculation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SDs }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{ (Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, sd)}
\NormalTok{SDs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     COND  Accurate
1 Control 0.4603964
2     Low 0.6301138
3    High 0.6587486
\end{verbatim}

\hypertarget{on-the-relationship-between-standard-deviation-and-variance}{%
\subsubsection{On the relationship between standard deviation and variance}\label{on-the-relationship-between-standard-deviation-and-variance}}

Early in statistics training the difference between standard deviation (\emph{s} or \(\sigma_{n-1}\)) and variance(\(s^{2}\) or \(\sigma^{2}\)) can be confusing. This calculation demonstrates the relationship between standard deviation and variance. Variance is the standard deviation, squared.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#when squared, the standard deviation of the control group, }
\CommentTok{\#hould equal the variance reported in the next chunk}
\NormalTok{sdControl}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.212
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{VARs }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{ (Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, var)}
\NormalTok{VARs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     COND  Accurate
1 Control 0.2119648
2     Low 0.3970434
3    High 0.4339497
\end{verbatim}

We will use the second formula to calculate \(SS_R\). For each of the groups, we multiply the variance by the respective degrees of freedom for the group (\emph{n} - 1).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculated by using object names from our calculations}
\NormalTok{SSR }\OtherTok{\textless{}{-}}\NormalTok{ varControl }\SpecialCharTok{*}\NormalTok{ (nControl }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ varLow }\SpecialCharTok{*}\NormalTok{ (nLow }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ varHigh }\SpecialCharTok{*}\NormalTok{ (nHigh }\SpecialCharTok{{-}}
    \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Re{-}calculated by specifying the actual values from our calculations}
\NormalTok{SSR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 30.246
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.212} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{30} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.397} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{30} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.434} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{30} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 30.247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Both result in the same}
\end{Highlighting}
\end{Shaded}

The value for our \(SS_R\) is 30.246. Degrees of freedom for the residual is \(df_T - df_M\).

\begin{itemize}
\tightlist
\item
  \(df_T\) was \(N-1\): 90 - 1 = 89
\item
  \(df_M\) was \(k - 1\): 3 - 1 = 2
\item
  Therefore, \(df_R\): is 89 - 2 = 87
\end{itemize}

\hypertarget{relationship-between-ss_t-ss_m-and-ss_r.}{%
\subsection{\texorpdfstring{Relationship between \(SS_T\), \(SS_M\), and \(SS_R\).}{Relationship between SS\_T, SS\_M, and SS\_R.}}\label{relationship-between-ss_t-ss_m-and-ss_r.}}

In case it is not clear:

\(SS_T\) = 9.432 + 30.246

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculated with object names }
\NormalTok{SSM }\SpecialCharTok{+}\NormalTok{ SSR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 39.678
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Re{-}calculated with the actual values}
\FloatTok{9.432} \SpecialCharTok{+} \FloatTok{30.247}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 39.679
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Both result in the same}
\end{Highlighting}
\end{Shaded}

Our SST, calculated from above was 39.678.

\hypertarget{mean-squares-model-residual}{%
\subsection{Mean Squares Model \& Residual}\label{mean-squares-model-residual}}

Our estimates of variation were \emph{sums of squares} and are influenced by the number of scores that were summed. We can correct this bias by calculating their average -- the \emph{mean squares} or \(MS\). We will use these in the calculation of the \(F\) ratio -- the statistic that tests if there are significant differences between groups.

Like the constellation of sums of squares, we calculate mean squares for the model (\(MS_M\)) and residual(\(MS_R\)). Each formula simply divides the corresponding sums of squares by their respective degrees of freedom.

\[MS_M = \frac{SS_{M}}{df{_{M}}}\]
Regarding the calculation of our model mean squares:

\begin{itemize}
\tightlist
\item
  \(SS_M\) was 9.432
\item
  \(df_M\) was 2
\item
  Therefore, \(MS_M=\)is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#mean squares for the model}
\CommentTok{\#calculated with object names}
\NormalTok{MSM }\OtherTok{\textless{}{-}}\NormalTok{ SSM}\SpecialCharTok{/}\NormalTok{dfM}
\NormalTok{MSM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.716
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Re{-}calculated with actual values }
\FloatTok{9.432}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.716
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Both result in the same}
\end{Highlighting}
\end{Shaded}

\[MS_R = \frac{SS_{R}}{df{_{R}}}\]
Regarding the calculation of our model residual squares:

\begin{itemize}
\tightlist
\item
  \(SS_R\) was 30.246
\item
  \(df_R\) was 87
\item
  Therefore, \(MS_R\) is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#mean squares for the residual}
\CommentTok{\#calculated with object names }
\NormalTok{MSR }\OtherTok{\textless{}{-}}\NormalTok{ SSR}\SpecialCharTok{/}\NormalTok{ dfR}
\NormalTok{MSR}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.348
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculated with actual values}
\FloatTok{30.247}\SpecialCharTok{/}\DecValTok{87}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3476667
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Both result in the same}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculating-the-f-statistic}{%
\subsection{\texorpdfstring{Calculating the \emph{F} Statistic}{Calculating the F Statistic}}\label{calculating-the-f-statistic}}

The \emph{F} statistic (or \emph{F} ratio) assesses the ratio (as its name implies) of variation explained by the model to unsystematic factors (i.e., the residual). Earlier we used ``between'' and ``within'' language. Especially when we think of our example -- where the model is composed of three groups, we can think of the \emph{F} statistic as assessing the ratio of variation explained by between-subjects differences to within-subjects differences. Navarro's \citeyearpar{navarro_chapter_2020} figures (earlier in the chapter) illustrate this well.

\[F = \frac{MS_{M}}{MS_{R}}\]
Regarding the calculation of our \emph{F}-ratio:

\begin{itemize}
\tightlist
\item
  \(MS_M\) was 4.716
\item
  \(MS_R\) was 0.348
\item
  Therefore, \(F\) is:
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculated with object names }
\NormalTok{Fratio }\OtherTok{\textless{}{-}}\NormalTok{ MSM }\SpecialCharTok{/}\NormalTok{ MSR}
\NormalTok{Fratio}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 13.566
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#calculated with actual values}
\CommentTok{\#Both result in the same}
\FloatTok{4.716}\SpecialCharTok{/}\FloatTok{0.348}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 13.55172
\end{verbatim}

\hypertarget{source-table-games}{%
\subsection{Source Table Games}\label{source-table-games}}

These last few calculations are actually less complicated than this presentation makes them seem. To better understand the relation between sums of squares, degrees of freedom, and mean squares, let's play a couple of rounds of \emph{Source Table Games}!

Rules of the game:

\begin{itemize}
\tightlist
\item
  In each case, mean squares are determined by dividing the sums of squares by its respective degrees of freedom.
\item
  The \emph{F} statistic is determined by dividing \(MS_M\) by \(MS_R\)
\end{itemize}

Knowing only two of the values, challenge yourself to complete the rest of the table. Before looking at the answers (below), try to the fill in the blanks based in the table based on what we have learned so far.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1184}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2632}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3684}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Game
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Total (df, \emph{N} - 1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Model (df, \emph{k} -1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Residual (df, \(df_T - df_M\))
\end{minipage} \\
\midrule()
\endhead
SS & 39.678(89) & 9.432(2) & \_\_\_\_\_\_ \\
MS & NA & \_\_\_\_\_\_ & \_\_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\(F = MS_{M}/MS_{R}\) = \_\_\_\_\_\_

\textbf{DON'T PEEK! TRY TO DO THE CALCULATIONS IN THE ``SOURCE TABLE GAMES'' EXERCISE BEFORE LOOKING AT THESE ANSWERS}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1184}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2632}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3684}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Answers
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Total (df, \emph{N} - 1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Model (df, \emph{k} -1)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Residual (df, \(df_T - df_M\))
\end{minipage} \\
\midrule()
\endhead
SS & 39.678(89) & 9.432(2) & 30.246(87) \\
MS & NA & 4.716 & 0.348 \\
\bottomrule()
\end{longtable}

\(F = MS_{M}/MS_{R}\) = 13.566

To determine whether or not it is statistically significant, we can check a \href{https://www.statology.org/how-to-read-the-f-distribution-table/}{table of critical values} \citep{zach_how_2019} for the \emph{F} test.

Our example has 2 (numerator) and 87 (denominator) degrees of freedom. Rolling down to the table where \(\alpha = .05\), we can see that any \(F\) value \textgreater{} 3.11 (a value somewhere between 3.07 and 3.15) will be statistically significant. Our \(F\) = 13.566, so we have clearly exceeded the threshold. This is our \emph{omnibus F test}.

We can also use a look-up function, which follows this general form: qf(p, df1, df2. lower.tail=FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qf}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{87}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.101296
\end{verbatim}

Significance at this level lets us know that there is at least 1 statistically significant difference between our control, low, and high racially loaded conditions. While it is important to follow-up to see where these significant differences lie, we will not do these by hand. Rather, let's rework the problem in R.

\hypertarget{working-the-one-way-anova-in-r}{%
\section{Working the One-Way ANOVA in R}\label{working-the-one-way-anova-in-r}}

Let's rework the problem in R. We start at the top of the flowchart, evaluating the statistical assumptions.

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw_Asmptns.jpg}
\caption{An image of the workflow for one-way ANOVA, showing that we are at the beginning: evaluating the potential violation of the assumptions.}
\end{figure}

\hypertarget{evaluating-the-statistical-assumptions}{%
\subsection{Evaluating the Statistical Assumptions}\label{evaluating-the-statistical-assumptions}}

All statistical tests have some assumptions about the data. The one-way ANOVA has four assumptions:

\begin{itemize}
\tightlist
\item
  The dependent variable is normally distributed for each of the populations as defined by the different levels of the factor. We will examine this by

  \begin{itemize}
  \tightlist
  \item
    evaluating skew and kurtosis
  \item
    visually inspecting the distribution
  \item
    conduct a Shapiro Wilks test
  \item
    examine a QQ plot
  \end{itemize}
\item
  The variances of the dependent variable are the same for all populations. This is often termed the \emph{homogeneity of variance} assumption. We will examine this with

  \begin{itemize}
  \tightlist
  \item
    Levene's Test
  \end{itemize}
\item
  The cases represent \emph{random} samples from the populations and scores on the test variable are \emph{independent} of each other. That is, comparing related cases (e.g., parent/child, manager/employee, time1/time2) violates this assumption and this question would need to be evaluated by a different statistic such as repeated measures ANOVA or dyadic data analysis.

  \begin{itemize}
  \tightlist
  \item
    \emph{Independence} in observations is a research design issue. ANOVA is not robust to violating this assumption. When observations are correlated/dependent there is a dramatic increase in Type I error.
  \end{itemize}
\item
  The dependent variable is measured on an interval scale.

  \begin{itemize}
  \tightlist
  \item
    If the dependent variable is categorical, another statistic (such as logistic regression) should be chosen.
  \end{itemize}
\end{itemize}

\hypertarget{is-the-dependent-variable-normally-distributed-across-levels-of-the-factor}{%
\subsubsection{Is the dependent variable normally distributed across levels of the factor?}\label{is-the-dependent-variable-normally-distributed-across-levels-of-the-factor}}

From the \emph{psych} package, the \emph{describe()} function can be used to provide descriptive statistics (or, ``descriptives'') of continuously scaled variables (i.e., variables measured on the interval or ratio scale). In this simple example, we can specify the specific continuous, DV.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#we name the function}
\CommentTok{\#in parentheses we list data source}
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{Accurate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   vars  n mean   sd median trimmed  mad min max range  skew kurtosis   se
X1    1 90  1.6 0.67   1.73    1.62 0.68   0   3     3 -0.28    -0.48 0.07
\end{verbatim}

If we want descriptives for each level of the grouping variable (factor), we can use the \emph{describeBy()} function of the \emph{psych} package. The order of entry within the script is the DV followed by the grouping variable (IV).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# It is unnecessary to create an object, but an object allows you to}
\CommentTok{\# do cool stuff, like write it to a .csv file and use that as a basis}
\CommentTok{\# for APA style tables In this script we can think \textquotesingle{}Accurate by COND\textquotesingle{}}
\CommentTok{\# meaning that the descriptives for accuracy will be grouped by COND}
\CommentTok{\# which is a categorical variable mat = TRUE presents the output in}
\CommentTok{\# matrix (table) form digits = 3 rounds the output to 3 decimal}
\CommentTok{\# places data = accSIM30 is a different (I think easier) way to}
\CommentTok{\# identify the object that holds the dataframe}
\NormalTok{des.mat }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{data =}\NormalTok{ accSIM30)}
\CommentTok{\# Note. Recently my students and I have been having intermittent}
\CommentTok{\# struggles with the describeBy function in the psych package. We}
\CommentTok{\# have noticed that it is problematic when using .rds files and when}
\CommentTok{\# using data directly imported from Qualtrics. If you are having}
\CommentTok{\# similar difficulties, try uploading the .csv file and making the}
\CommentTok{\# appropriate formatting changes. displays the matrix object that we}
\CommentTok{\# just created}
\NormalTok{des.mat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item  group1 vars  n  mean    sd median trimmed   mad   min   max range
X11    1 Control    1 30 1.756 0.460  1.893   1.767 0.392 0.781 2.745 1.964
X12    2     Low    1 30 1.900 0.630  2.007   1.918 0.458 0.655 3.000 2.345
X13    3    High    1 30 1.153 0.659  1.131   1.128 0.743 0.000 2.669 2.669
      skew kurtosis    se
X11 -0.275   -0.537 0.084
X12 -0.378   -0.465 0.115
X13  0.208   -0.690 0.120
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# optional to write it to a .csv file}
\FunctionTok{write.csv}\NormalTok{(des.mat, }\AttributeTok{file =} \StringTok{"Table1.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Skew and kurtosis speaks to normal distributions. The skew and kurtosis indices in the \emph{psych} package are reported as \emph{z} scores. Regarding skew, values greater than 3.0 are generally considered ``severely skewed.'' Regarding kurtosis, ``severely kurtotic'' is argued to be anywhere greater 8 to 20 \citep{kline_principles_2016}.

The \emph{Shapiro-Wilks} test evaluates the hypothesis that the distribution of the data as a whole deviates from a comparable normal distribution. If the test is non-significant (\emph{p} \textgreater.05) the distribution of the sample is not significantly different from a normal distribution. If, however, the test is significant (\emph{p} \textless{} .05), then the sample distribution is significantly different from a normal distribution. The \emph{rstatix} package has a wrapper that can conduct the Shapiro-Wilks test for us.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{shapiro }\OtherTok{\textless{}{-}}\NormalTok{ accSIM30 }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{shapiro\_test}\NormalTok{(Accurate)}
\NormalTok{shapiro}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  COND    variable statistic     p
  <fct>   <chr>        <dbl> <dbl>
1 Control Accurate     0.954 0.215
2 Low     Accurate     0.944 0.115
3 High    Accurate     0.980 0.831
\end{verbatim}

The \(p\) values for the distributions of the dependent variable (accurate) in each of the three conditions are all well above .05. This tells us that the Accurate variable does not deviate from a statistically significant distribution (Control, \emph{W} = 0.954, \emph{p} = 0.215; Low, \emph{W} = 0.944, \emph{p} = 0.115; High, \emph{W} = 0.980, \emph{p} = 0.831).

There are limitations to the Shapiro-Wilks test. As the dataset being evaluated gets larger, the Shapiro-Wilks test becomes more sensitive to small deviations; this leads to a greater probability of rejecting the null hypothesis (null hypothesis being the values come from a normal distribution). Green and Salkind \citeyearpar{green_using_2014} advised that ANOVA is relatively robust to violations of normality if there are at least 15 cases per cell and the design is reasonably balanced (i.e., equal cell sizes).

\hypertarget{are-the-variances-of-the-dependent-variable-similar-across-the-levels-of-the-grouping-factor}{%
\subsubsection{Are the variances of the dependent variable similar across the levels of the grouping factor?}\label{are-the-variances-of-the-dependent-variable-similar-across-the-levels-of-the-grouping-factor}}

The Levene's test evaluates the ANOVA assumption that variances of the dependent variable for each level of the independent variable are similarly distributed. We want this to be non-significant (\(p\) \textgreater{} .05). If violated, we need to use an ANOVA test that is ``robust to the violation of the homogeneity of variance'' (e.g., Welch's oneway).

In R, Levene's test is found in the \emph{car} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Our set up is similar: Accurate by condition, followed by the}
\CommentTok{\# object that holds the dataframe, followed by the instruction to}
\CommentTok{\# center the analysis around the mean}
\NormalTok{levene }\OtherTok{\textless{}{-}}\NormalTok{ car}\SpecialCharTok{::}\FunctionTok{leveneTest}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, accSIM30, }\AttributeTok{center =}\NormalTok{ mean)}
\NormalTok{levene}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Levene's Test for Homogeneity of Variance (center = mean)
      Df F value Pr(>F)
group  2  1.5315  0.222
      87               
\end{verbatim}

We write the result of the Levene's as \(F\)(2,87) = 1.532, \(p\) = 0.222. Because \(p\) \textgreater{} .05, we know that the result is nonsignficant -- that the variances of the three groups are not statistically significantly different than each other.

If the results had been statistically significantly different, we would have needed to use a Welch's \(F\) or robust version of ANOVA.

\hypertarget{summarizing-results-from-the-analysis-of-assumptions}{%
\subsubsection{Summarizing results from the analysis of assumptions}\label{summarizing-results-from-the-analysis-of-assumptions}}

It is common for an APA style results section to begin with a review of the evaluation of the statistical assumptions. As we have just finished these analyses, I will document what we have learned so far:

\begin{quote}
A one-way analysis of variance was conducted to evaluate the relationship between degree of racial loading of an exceptionalizing microaggression and the perceived accuracy of a research confederate's impression of the Asian or Asian American participant. The independent variable, condition, included three levels: control/none, low, and high levels of racial loading. Results of Levene's homogeneity of variance test indicated no violation of the homogeneity of variance assumption (\(F\){[}2,87{]} = 1.532, \(p\) = 0.222). Similarly, results of the Shapiro Wilk's test indicated no violation of the normality assumption in each of the cells (control, \emph{W} = 0.954, \emph{p} = 0.215; low, \emph{W} = 0.944, \emph{p} = 0.115; high, \emph{W} = 0.980, \emph{p} = 0.831).
\end{quote}

Now we can move onto computing the omnibus ANOVA. \emph{Omnibus} is the term applied to the first \emph{F} test that evaluates if all groups have the same mean \citep{chen_relationship_2018}. If this test is not significant there is no evidence in the data to reject the null; that is, there is no evidence to suggest that group means are different. If it is significant -- and there are three or more groups -- follow-up testing will be needed to determine where the differences lie.

\hypertarget{computing-the-omnibus-anova}{%
\subsection{Computing the Omnibus ANOVA}\label{computing-the-omnibus-anova}}

Having met all the assumptions, we are now ready to calculate the omnibus \(F\) test.

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw_omnibus.jpg}
\caption{An image of the workflow for one-way ANOVA, showing that we are at the stage of computing the omnibus ANOVA.}
\end{figure}

ANOVA is a special case of the general linear model (regression is a ``not so special case'' of the general linear model), therefore we use the linear model function, \emph{aov()} to run the analysis.

In the code below, we predict Accuracy from COND (3 levels: control, low, high).

By assigning the results of the \emph{aov()} function to an object (omnibus) we can then use that object (think \emph{model}) in other functions to get details about our analysis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the script looks familiar, \textquotesingle{}Accurate by Condition\textquotesingle{} DV \textasciitilde{} IV I say,}
\CommentTok{\# \textquotesingle{}DV by IV\textquotesingle{}}
\NormalTok{omnibus }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30)}
\CommentTok{\# prints the ANOVA output}
\FunctionTok{summary}\NormalTok{(omnibus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value     Pr(>F)    
COND         2  9.432   4.716   13.57 0.00000745 ***
Residuals   87 30.246   0.348                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Inserting the \emph{aov()} object (omnibus) into the summary command produces the ANOVA Source Table that we manually created above.

The values we see map onto those we calculated by hand. Our \(SS_M\) (9.432) plus \(SS_R\) (30.246) sum to equal the \(SS_T\) (39.678).

Dividing the two sums of squares by their respective degrees of freedom produces the means squared.

Then, dividing the \(MS_M\) (COND) by \(MS_R\) (4.716/0.348) provides the \emph{F} ratio. By using a table of \emph{F} critical values, we already knew that our \emph{F} value exceeded the value in the table of critical values. Here we see that \emph{p} = 0.000.

The ``\emph{F} string'' for an APA style results section should be written like this: \emph{F} (2, 87) = 13.566, \emph{p} \textless{} .001.

The object we created with the \emph{aov()} function is capable of producing much information. Applying the \emph{names()} function to the object can give us a list of values within it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(omnibus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "coefficients"  "residuals"     "effects"       "rank"         
 [5] "fitted.values" "assign"        "qr"            "df.residual"  
 [9] "contrasts"     "xlevels"       "call"          "terms"        
[13] "model"        
\end{verbatim}

One of the most commonly used functions to be applied to the \emph{aov()} objects is \emph{summary()}; but it's not the only one. Let's try some other options.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{model.tables}\NormalTok{ (omnibus, }\StringTok{"means"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tables of means
Grand mean
         
1.603042 

 COND 
COND
Control     Low    High 
 1.7562  1.9001  1.1528 
\end{verbatim}

The \emph{aov()} command has a quickplot feature.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(omnibus) }
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-226-1.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-226-2.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-226-3.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-226-4.pdf}

The first of the four plots fits the residuals. We already know from Levene's that we did not violate the homogeneity of variance test. With its straight line, this plot shows an equal spread across the three groups.

When the dots of the Q-Q plot map onto the diagonal, we have some indication of normality of the residuals (we want residuals to be normally distributed).

\hypertarget{effect-size-for-the-one-way-anova}{%
\subsubsection{Effect size for the one-way ANOVA}\label{effect-size-for-the-one-way-anova}}

\textbf{Eta squared} is one of the most commonly used measures of effect. It refers to the proportion of variability in the dependent variable/outcome that can be explained in terms of the independent variable/predictor. Traditional interpretive values are similar to the Pearson's \emph{r}:

\begin{itemize}
\tightlist
\item
  0.0 = no relationship
\item
  .02 = small
\item
  .13 = medium
\item
  .26 = large
\item
  1.0 = a perfect (one-to-one) correspondence
\end{itemize}

A useful summary of effect sizes, guide to interpreting their magnitudes, and common usage can be found \href{https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize}{here} \citep{watson_rules_2020}.

The formula for \(\eta^2\) is straightforward. If we think back to our hand-calculations of all the sums of squares, we can see that this is the proportion of variance that is accounted for by the model.

\[\eta ^{2}=\frac{SS_{M}}{SS_{T}}\]
Hand calculation, then, is straightforward.:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Calculated using the object names}
\NormalTok{SSMomnibus }\SpecialCharTok{/}\NormalTok{ (SSMomnibus }\SpecialCharTok{+}\NormalTok{ SSRomnibus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.238
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Re{-}calculated by using the numeric values}
\FloatTok{9.432}\SpecialCharTok{/}\NormalTok{(}\FloatTok{9.432} \SpecialCharTok{+} \FloatTok{30.246}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2377136
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Both options should produce the same result}
\end{Highlighting}
\end{Shaded}

The \emph{lsr} package includes an eta-squared calculator. To use it, we simply insert the model/object we created with the \emph{aov()} function to \emph{lsr}'s \emph{etaSquared()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(omnibus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        eta.sq eta.sq.part
COND 0.2377226   0.2377226
\end{verbatim}

Notice that there are two effect sizes. We described eta-squared. Partial eta-squared is the default effect size reported in SPSS. There is a long history of debate about which to use. In certain circumstances (especially in more complicated analyses), partial-eta squared can be a bit more generous (i.e., larger than \(\eta^2\)). Thus, many prefer the reporting of the more cautious \(\eta^2\).

In our case, we see no difference between the two values. Differences begin to appear in datasets that are more complicated, such as when sample sizes across the levels of a factor differ.

\hypertarget{summarizing-results-from-the-omnibus-anova}{%
\subsubsection{Summarizing results from the omnibus ANOVA}\label{summarizing-results-from-the-omnibus-anova}}

Presenting the APA style results of the omnibus test is very straightforward:

\begin{quote}
Results indicated a significant effect of COND on accuracy perception \emph{F} (2, 87) = 13.566, \emph{p} \textless{} .001, \(\eta^2\) = 0.238.
\end{quote}

\hypertarget{follow-up-to-the-omnibus-f}{%
\subsection{\texorpdfstring{Follow-up to the Omnibus \emph{F}}{Follow-up to the Omnibus F}}\label{follow-up-to-the-omnibus-f}}

The \emph{F}-test associated with the one-way ANOVA is the \emph{omnibus} -- giving the result for the overall test. Looking at the workflow for the one-way ANOVA we see that if we had had we had a non-significant \(F\), we would have stopped our analysis.

However, if the omnibus \(F\) is significant, we know that there is at least one pair of cells where there is a statistically significant difference. We have several ways (each with its own strengths/limitations) to figure out where these differences lie.

A very common option is to conduct post-hoc, pairwise comparisons.

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw_phoc.jpg}
\caption{An image of the workflow for one-way ANOVA, showing that we are at the stage of following a statistically significant omnibus F test and are now conducting posthoc comparisons.}
\end{figure}

\hypertarget{option-1-post-hoc-pairwise-comparisons}{%
\subsubsection{OPTION 1: Post-hoc, pairwise, comparisons}\label{option-1-post-hoc-pairwise-comparisons}}

Post-hoc, pairwise comparisons are:

\begin{itemize}
\tightlist
\item
  used for exploratory work when no firm hypotheses were articulated a priori,
\item
  used to compare the means of all combinations of pairs of an experimental condition,
\item
  less powerful than planned comparisons b/c strict criterion for significance must be used.
\end{itemize}

Helpful information about how to conduct post-hoc pairwise comparisons in R can be found at the \href{https://stats.idre.ucla.edu/r/faq/how-can-i-do-post-hoc-pairwise-comparisons-in-r/}{UCLA Institute for Digital Research and Education site} \citep{noauthor_how_nodate}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairwise.t.test}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{Accurate, accSIM30}\SpecialCharTok{$}\NormalTok{COND, }\AttributeTok{p.adj =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pairwise comparisons using t tests with pooled SD 

data:  accSIM30$Accurate and accSIM30$COND 

     Control Low      
Low  0.34709 -        
High 0.00015 0.0000042

P value adjustment method: none 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#can swap "bonf" or "holm" for p.adj}
\end{Highlighting}
\end{Shaded}

The output only provides the \(p\) values associated with the mean differences in each of the conditions. We see that \(p < .05\) when high is compared to control (0.00015) and high is compared to low (0.0000042). An APA style reporting results of these typically involves referencing the means (often reported in a table of means and standard deviations) or mean differences (hand calculated) with their \emph{p} values.

\textbf{Should we be concerned about Type I error?}

Recall that \emph{Type I error} is the concern about false positives -- that we would incorrectly reject a true null hypothesis (that we would say that there is a statistically significant difference when there is not one). This concern increases when we have a large number of pairwise comparisons.

Green and Salkind \citeyearpar{green_using_2014} reviewed three options for managing Type I error.

\begin{itemize}
\tightlist
\item
  Traditional Bonferroni:

  \begin{itemize}
  \tightlist
  \item
    Adjusts the \emph{p} value upward by multiplying it (the raw \emph{p} values) by the number of comparisons being completed. This holds the \emph{total} Type I error rate across these tests to \(\alpha\) (usually .05). The traditional Bonferroni is simple and therefore attractive, but when \emph{p} values hover around .05, it can be too restrictive.
  \end{itemize}
\item
  Holms Sequential Bonferroni:

  \begin{itemize}
  \tightlist
  \item
    We'll describe this in more detail later. Briefly, it allows us to rank order the comparisons by their \emph{p} value (smallest to largest). We determine the significance of each \emph{p} value \emph{sequentially} where the criteria for significance is incrementally relaxed.
  \end{itemize}
\item
  LSD method:

  \begin{itemize}
  \tightlist
  \item
    Permitted when there are only three pairwise comparisons among three groups, researchers can leave the \emph{p} values as they are. Since the Tran and Lee \citeyearpar{tran_you_2014} research vignette is one of those circumstances, I will not make adjustments for Type I error. That is, I would claim the LSD and cite Green and Salkind \citeyearpar{green_using_2014} as justification for that decision.
  \end{itemize}
\end{itemize}

There is, though, an even more powerful approach\ldots{}

\hypertarget{option-2-planned-contrasts-non-orthogonal}{%
\subsubsection{OPTION 2: Planned contrasts (non-orthogonal)}\label{option-2-planned-contrasts-non-orthogonal}}

Another option is to evaluate planned comparisons.

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw_planned.jpg}
\caption{An image of the workflow for one-way ANOVA, showing that we are at the following up to a significant omnibus F by conducting planned comparisons}
\end{figure}

Planned comparisons are

\begin{itemize}
\tightlist
\item
  theory-driven comparisons constructed prior to data collection,
\item
  based on the idea of partitioning the variance created by the overall effect of group differences into gradually smaller portions of variance, and
\item
  more powerful than post-hoc tests.
\end{itemize}

Planned contrasts involve further considerations regarding the \emph{partitioning of variance.}

\begin{itemize}
\tightlist
\item
  There will always be \emph{k}-1 contrasts.
\item
  Each contrast must involve only two chunks of variance.
\end{itemize}

\emph{Orthogonal} contrasts are even more sophisticated. Essential to conducting an orthogonal contrast is the requirement that if a group is singled out in one comparison it should be excluded from subsequent contrasts. The typical, orthogonal scenario with three ordered groups has only two contrasts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Control versus low and high (because control was excluded, it should not reappear in the next contrast)
\item
  Low versus high
\end{enumerate}

Underlying the \emph{aov()} program is the linear model (``lm''). We could have used it to calculate the omnibus ANOVA, but it has clunky output.

We use it now to retrieve some contrast information. The code below is a planned comparison that uses the coding in the database (created when we formatted COND as an ordered factor) to compare the lowest coded group (control was 1, low was 2, high was 3) to the other two groups.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary.lm}\NormalTok{(omnibus)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Accurate ~ COND, data = accSIM30)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.24533 -0.32092  0.08642  0.30101  1.51646 

Coefficients:
            Estimate Std. Error t value             Pr(>|t|)    
(Intercept)   1.7562     0.1076  16.314 < 0.0000000000000002 ***
CONDLow       0.1439     0.1522   0.945             0.347095    
CONDHigh     -0.6034     0.1522  -3.963             0.000151 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5896 on 87 degrees of freedom
Multiple R-squared:  0.2377,    Adjusted R-squared:  0.2202 
F-statistic: 13.57 on 2 and 87 DF,  p-value: 0.000007446
\end{verbatim}

From the above, regression output, we see that there was not a statistically significant difference between CONDControl and CONDLow (\emph{p} = 0.347). There were statistically significant difference when CONDControl (the intercept) was compared to CONDHigh (\emph{p} \textless{} .001).

Note that these tests are conducted as \emph{t} tests. Why? We are comparing only two groups and can use the \emph{t} distribution.

Also note that these involved the comparison of the control group to low; then the control to high. This is consistent with an orthogonal contrast in that there are \emph{k} - 1 contrasts (two contrasts with three original groups). However it is inconsistent with the requriement that once a group is singled out, it should not be used in a subsequent contrast. Therefore, it is quite possible we want something different.

\hypertarget{option-3-planned-contrasts-orthogonal}{%
\subsubsection{OPTION 3: Planned contrasts (orthogonal)}\label{option-3-planned-contrasts-orthogonal}}

\textbf{Step 1: } Specify our contrasts

\begin{itemize}
\tightlist
\item
  Specifying the contrasts means you know their order within the factor
\item
  Early in the data preparation, we created an ordered factor with Control, Low, High as the order.
\item
  We want orthogonal contrasts, this means there will be

  \begin{itemize}
  \tightlist
  \item
    \emph{k} - 1 contrasts; with three groups we will have two contrasts
  \item
    once we single out a condition for comparison, we cannot use it again.
  \end{itemize}
\end{itemize}

In \emph{contrast1} we compare the control condition to the combined low and high conditions.
In \emph{contrast2} we discard the control condition (it was already singled out) and we compare the low and high conditions.

This is sensible because we likely hypothesize that any degree of racially loaded stereotypes may have a deleterious outcome, so we first compare control to the two conditions with any degree of racial loading. Subsequently, we compare the low and high levels of the factor.

\textbf{Step 2:} Bind them together and check the output to ensure that we've mapped them correctly.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Contrast1 compares Control against the combined effects of Low and}
\CommentTok{\# High.}
\NormalTok{contrast1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Contrast2 excludes Control; compares Low to High.}
\NormalTok{contrast2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# binding the contrasts together}
\FunctionTok{contrasts}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{COND) }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(contrast1, contrast2)}
\NormalTok{accSIM30}\SpecialCharTok{$}\NormalTok{COND}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] High    High    High    High    High    High    High    High    High   
[10] High    High    High    High    High    High    High    High    High   
[19] High    High    High    High    High    High    High    High    High   
[28] High    High    High    Low     Low     Low     Low     Low     Low    
[37] Low     Low     Low     Low     Low     Low     Low     Low     Low    
[46] Low     Low     Low     Low     Low     Low     Low     Low     Low    
[55] Low     Low     Low     Low     Low     Low     Control Control Control
[64] Control Control Control Control Control Control Control Control Control
[73] Control Control Control Control Control Control Control Control Control
[82] Control Control Control Control Control Control Control Control Control
attr(,"contrasts")
        contrast1 contrast2
Control        -2         0
Low             1        -1
High            1         1
Levels: Control Low High
\end{verbatim}

Thinking back to the hand-calculations and contrast mapping, the table of weights that R just produced confirms that

\begin{itemize}
\tightlist
\item
  Contrast 1 compares the control condition against the levels with any racial loading.
\item
  Contrast 2 compares the low and high loadings.
\end{itemize}

\textbf{Step 3:} Create a new \emph{aov()} model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create a new object, the ANOVA looks the same, but it will now}
\CommentTok{\# consider the contrasts (this is where order{-}of{-}operations matters)}
\NormalTok{accPlanned }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30)}
\FunctionTok{summary.lm}\NormalTok{(accPlanned)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Accurate ~ COND, data = accSIM30)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.24533 -0.32092  0.08642  0.30101  1.51646 

Coefficients:
              Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    1.60304    0.06215  25.793 < 0.0000000000000002 ***
CONDcontrast1 -0.07658    0.04395  -1.742                0.085 .  
CONDcontrast2 -0.37365    0.07612  -4.909           0.00000425 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5896 on 87 degrees of freedom
Multiple R-squared:  0.2377,    Adjusted R-squared:  0.2202 
F-statistic: 13.57 on 2 and 87 DF,  p-value: 0.000007446
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{COND) }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These planned contrasts show that when the control condition is compared to the combined low and high racial loading conditions, there is not a statistically significant difference, \emph{t}(87) = -1.742, \emph{p} = 0.085. However, when the low and high racial loading conditions are compared, there is a statistically significant difference, \emph{t}(87) = -4.909, \emph{p} \textless{} 0.001.

\hypertarget{option-4-trend-polynomial-analysis}{%
\subsubsection{OPTION 4: Trend (polynomial) analysis}\label{option-4-trend-polynomial-analysis}}

\begin{figure}
\centering
\includegraphics{images/oneway/OnewayWrkFlw_poly.jpg}
\caption{An image of the workflow for one-way ANOVA, showing that we are at the following up to a significant omnibus F by assessing for a polynomial trend}
\end{figure}

Polynomial contrasts evaluate for the presence of a linear (or curvilinear) pattern in the data. To detect a trend, the data must be coded in an ascending order and it needs to be a sensible, theoretically defensible, comparison. Our data has a theoretically ordered effect (control, low racially loaded condition, high racially loaded condition). Recall that we created an ordered factor when we imported the data.

In a polynomial analysis, the statistical analysis looks across the ordered means to see if they fit a linear or curvilinear shape that is one less than the number of levels. Our factor has three levels, therefore the polynomial contrast can check for a linear shape (.L) or a quadratic (one change in direction) shape (.Q). If we had four levels, the contr.poly could check for cubic change (two changes in direction). Conventionally, when more than one trend is significant, we intepret the most complex one (i.e., quadratic over linear).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(accSIM30}\SpecialCharTok{$}\NormalTok{COND) }\OtherTok{\textless{}{-}} \FunctionTok{contr.poly}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{accTrend }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30)}
\FunctionTok{summary.lm}\NormalTok{(accTrend)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Accurate ~ COND, data = accSIM30)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.24533 -0.32092  0.08642  0.30101  1.51646 

Coefficients:
            Estimate Std. Error t value             Pr(>|t|)    
(Intercept)  1.60304    0.06215  25.793 < 0.0000000000000002 ***
COND.L      -0.42665    0.10765  -3.963             0.000151 ***
COND.Q      -0.36384    0.10765  -3.380             0.001087 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.5896 on 87 degrees of freedom
Multiple R-squared:  0.2377,    Adjusted R-squared:  0.2202 
F-statistic: 13.57 on 2 and 87 DF,  p-value: 0.000007446
\end{verbatim}

A quick peek back at an early plot shows illustrates the quadratic trend that was supported by the analysis.

Results of our polynomial contrast suggested statistically significant results for both linear \(t(87) = -3.963 , p < .001\) and quadratic \(t(87) = -3.380, p = .001\) trends.

\hypertarget{which-set-of-follow-up-tests-do-we-report}{%
\subsubsection{Which set of follow-up tests do we report?}\label{which-set-of-follow-up-tests-do-we-report}}

It depends! What best tells the story of your data? Here are some things to consider.

\begin{itemize}
\tightlist
\item
  If the post-hoc comparisons are robustly statistically significant (and controlling Type I error is not going to change that significance), I think this provides good information and I would lean toward reporting those.
\item
  If \emph{p} values are hovering around 0.05, an orthogonal contrast will offer more power because

  \begin{itemize}
  \tightlist
  \item
    a \emph{k} - 1 comparison will be more powerful
  \item
    a priori theory is compelling
  \end{itemize}
\item
  The polynomial can be a useful descriptive addition if there is a linear or quadratic relationship that is sensible or interesting.
\end{itemize}

Although I would report either the post-hoc or planned contrasts, I will sometimes add a polynomial if it clarifies the result (i.e., there is a meaningful linear or curvilinear pattern essential to understanding the data).

\hypertarget{what-if-we-violated-the-homogeneity-of-variance-test}{%
\subsection{What if we Violated the Homogeneity of Variance test?}\label{what-if-we-violated-the-homogeneity-of-variance-test}}

The \emph{oneway.test} produces Welch's \emph{F} -- a test more robust to violation of the homogeneity of variance assumption. The Welch's approach to attenuating error or erroneous conclusions caused by violations of the homogeneity of variance assumption is to adjust the residual degrees of freedom used to produce the Welch's \emph{F}-ratio.

Another common correction is the Brown and Forsythe \emph{F}-ratio. At this time I have not located and tried an R package that produces this.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{oneway.test}\NormalTok{(Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One-way analysis of means (not assuming equal variances)

data:  Accurate and COND
F = 11.569, num df = 2.000, denom df = 56.343, p-value = 0.00006174
\end{verbatim}

Note that the denominator \emph{df} is now 56.34 (not 87) and \emph{p} value is a little larger (it was 0.00000745). With its design intended to avoid making a Type I error, the Welch's \emph{F} is more ``conservative.'' While it doesn't matter in this case, if it were a study where the \emph{p} value was closer to .05, it could make a difference. These are some of the tradeoffs made in order to have confidence in the results.

\hypertarget{power-analysis}{%
\section{Power Analysis}\label{power-analysis}}

Power analysis allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. Utilized another way, it allows us to determine the probability of detecting an effect of a given size with a given level of confidence. If the probability is unacceptably low, we may want to revise or stop. A helpful overview of power as well as guidelines for how to use the \emph{pwr} package can be found at a \href{https://www.statmethods.net/stats/power.html}{Quick-R website} \citep{kabacoff_power_2017}.

There are four interrelating elements of power:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample size, \emph{N}
\item
  Effect size,

  \begin{itemize}
  \tightlist
  \item
    For one-way ANOVAs, Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively.
  \end{itemize}
\item
  Significance level = P(Type I error),

  \begin{itemize}
  \tightlist
  \item
    Recall that Type I error is the rejection of a true null hypothesis (a false positive).
  \item
    Stated another way, Type I error is the probability of finding an effect that is not there.
  \end{itemize}
\item
  Power = 1 - P(Type II error),

  \begin{itemize}
  \tightlist
  \item
    Recall that Type II error is the non-rejection of a false null hypothesis (a false negative).
  \item
    Power is the probability of finding an effect that is there.
  \end{itemize}
\end{enumerate}

If we have any three of these values, we can calculate the fourth.

In Champely's \emph{pwr} package, we can conduct a power analysis for a variety of designs, including the balanced one-way ANOVA (i.e., roughly equal cell sizes) design that we worked in this chapter.

The \emph{pwr.anova.test()} has five parameters:

\begin{itemize}
\tightlist
\item
  \emph{k} = \# groups
\item
  \emph{n} = sample size
\item
  \emph{f} = effect sizes, where 0.1/small, 0.25/medium, and 0.4/large

  \begin{itemize}
  \tightlist
  \item
    In the absence from an estimate from our own data, we make a guess about the expected effect size value based on our knowledge of the literature
  \end{itemize}
\item
  \emph{sig.level} = \emph{p} value that you will use
\item
  \emph{power} = .80 is the standard value
\end{itemize}

In the script below, we simply add our values. So long as we have four values, the fifth will be calculated for us.

Because this calculator requires the effect size in the metric of Cohen's \emph{f} (this is not the same as the \emph{F} ratio), we need to convert it. The \emph{effectsize} package has a series of converters. We can use the \emph{eta2\_to\_f()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effectsize}\SpecialCharTok{::}\FunctionTok{eta2\_to\_f}\NormalTok{(}\FloatTok{0.238}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Registered S3 method overwritten by 'parameters':
  method                         from      
  format.parameters_distribution datawizard
\end{verbatim}

\begin{verbatim}
[1] 0.5588703
\end{verbatim}

We simply plug this value into the ``f =''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.anova.test}\NormalTok{(}\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.5589}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced one-way analysis of variance power calculation 

              k = 3
              n = 11.3421
              f = 0.5589
      sig.level = 0.05
          power = 0.8

NOTE: n is number in each group
\end{verbatim}

This result suggested that we would need 11 people per group.

If we were unsure about what to expect in terms of our results, we could take a guess. I like to be on the safe(r) side and go with a smaller effect. What would happen if we had a Cohen's \emph{f} that represented a small effect?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr}\SpecialCharTok{::}\FunctionTok{pwr.anova.test}\NormalTok{(}\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{sig.level =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced one-way analysis of variance power calculation 

              k = 3
              n = 322.157
              f = 0.1
      sig.level = 0.05
          power = 0.8

NOTE: n is number in each group
\end{verbatim}

Yikes! We would need over 300 per group!

If effect sizes are new to you, play around with this effect size converter hosted at \href{https://www.psychometrica.de/effect_size.html}{Psychometrica.de}. For examples like this one, use the option labeled, ``Transformation of the effect sizes \emph{d}, \emph{r}, \emph{f}, \emph{Odds Ratio}, \(\eta^2\), and \emph{Common Language Effect Size (CLES}).''

\hypertarget{apa-style-results-3}{%
\section{APA Style Results}\label{apa-style-results-3}}

All that's left to do is \emph{write it up}! APA style results sections in empirical manuscripts are typically accompanied by tables and figures. APA style discourages repeated material and encourages reducing the cognitive load of the reader. For this example, I suggest two tables -- one with means and standard deviations of the groups and a second that reports the output from the one-way ANOVA. In an article with multiple statistics, the authors might wish to combine or delete these.

The package \emph{apaTables} can produce journal-ready tables by using the object produced by the \emph{aov()} function. Deciding what to report in text and table is important.

Here I create Table 1 with means and standard deviations (plus a 95\% confidence interval around each mean).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# table.number = 1 assigns a table number to the top of the table}
\CommentTok{\# filename = \textquotesingle{}Table1.doc\textquotesingle{} writes the table to Microsoft Word and puts}
\CommentTok{\# it in your project folder}
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.1way.table}\NormalTok{(}\AttributeTok{iv =}\NormalTok{ COND, }\AttributeTok{dv =}\NormalTok{ Accurate, }\AttributeTok{show.conf.interval =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ accSIM30, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename =} \StringTok{"Table1.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Descriptive statistics for Accurate as a function of COND.  

    COND    M     M_95%_CI   SD
 Control 1.76 [1.58, 1.93] 0.46
     Low 1.90 [1.66, 2.14] 0.63
    High 1.15 [0.91, 1.40] 0.66

Note. M and SD represent mean and standard deviation, respectively.
LL and UL indicate the lower and upper limits of the 95% confidence interval 
for the mean, respectively. 
The confidence interval is a plausible range of population means that could 
have caused a sample mean (Cumming, 2014). 
\end{verbatim}

I will want to give the values of mean differences (\(M_{diff}\)) in the results. I can quickly use R to calculate them here.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating mean difference between control and hig}
\FloatTok{1.76} \SpecialCharTok{{-}} \FloatTok{1.15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.61
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating mean difference between low and high}
\FloatTok{1.9} \SpecialCharTok{{-}} \FloatTok{1.15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.75
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# calculating mean difference between control and low}
\FloatTok{1.76} \SpecialCharTok{{-}} \FloatTok{1.9}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.14
\end{verbatim}

Here I create Table 2 with results of the one-way ANOVA. The result in Microsoft Word can be edited (e.g., I would replace the partial-eta squared with \(\eta^2\)) for the journal article.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.aov.table}\NormalTok{(omnibus, }\AttributeTok{table.number =} \DecValTok{2}\NormalTok{, }\AttributeTok{filename =} \StringTok{"Table2.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 2 

ANOVA results using Accurate as the dependent variable
 

   Predictor    SS df    MS      F    p partial_eta2 CI_90_partial_eta2
 (Intercept) 92.53  1 92.53 266.15 .000                                
        COND  9.43  2  4.71  13.57 .000          .24         [.11, .34]
       Error 30.25 87  0.35                                            

Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared 
\end{verbatim}

Regarding figures, there are a number of options. I find the \emph{plotmeans()} function (used earlier) to be adequate for the purpose of one-way ANOVA. As we progress through this textbook, I will demonstrate options that offer more and less complexity.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gplots}\SpecialCharTok{::}\FunctionTok{plotmeans}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ Accurate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ accSIM30, }\AttributeTok{xlab =} \StringTok{"Racial Loading Condition"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Accuracy of Confederate\textquotesingle{}s Impression"}\NormalTok{, }\AttributeTok{n.label =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-243-1.pdf}

With table and figure at hand, here is how I would write up these results:

\begin{quote}
A one-way analysis of variance was conducted to evaluate the relationship between degree of racial loading of an exceptionalizing microaggression and the perceived accuracy of a research confederate's impression of the Asian or Asian American participant. The independent variable, COND, included three levels: control/none, low, and high levels of racial loading. Results of Levene's homogeneity of variance test indicated no violation of the homogeneity of variance assumption (\(F\){[}2,87{]} = 1.532, \(p\) = 0.222). Similarly, results of the Shapiro Wilk's test indicated no violation of the normality assumption in each of the cells (Control, \emph{W} = 0.954, \emph{p} = 0.215; Low, \emph{W} = 0.944, \emph{p} = 0.115; High, \emph{W} = 0.980, \emph{p} = 0.831).
\end{quote}

\begin{quote}
Results indicated a significant effect of COND on accuracy perception \emph{F} (2, 87) = 13.566, \emph{p} \textless{} .001, \(\eta^2\) = 0.238. In a series of post-hoc comparisons, there were statistically significant differences between the control and high (\(M_{diff}\) = 0.61, \emph{p} \textless{} .001) and low and high (\(M_{diff}\) = 0.75, \emph{p} \textless{} 0.001) conditions, but not the control and low conditions (\(M_{diff}\) = -.14, \emph{p} = 0.347). A statistically significant polynomial contrast suggested a quadratic trend across the three, ordered, levels of the conditions (\(t[87] = -0.364, p = .001\)) such that perception of accuracy increased slighty from the control to low conditions, but decreased more dramatically from low to high. Consequently, it appeared that only the highest degree of racial loading (e.g., ``You speak English well for an Asian'') resulted in the decreased perceptions of accuracy of impressions from the confederate. Means and standard deviations are presented in Table 1 and complete ANOVA results are presented in Table 2. Figure 1 provides an illustration of the results.
\end{quote}

\hypertarget{a-conversation-with-dr.-tran}{%
\section{A Conversation with Dr.~Tran}\label{a-conversation-with-dr.-tran}}

Doctoral student (and student in one of my classes) Emi Ichimura and I were able to interview the first author (Alisia Tran, PhD) about the article and what it means. Here's a direct \href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=643f8a1e-bb6d-4ceb-a860-aeba01522528}{link} to that interview.

Among others, we asked:

\begin{itemize}
\tightlist
\item
  What were unexpected challenges to the research method or statistical analysis?
\item
  What were the experiences of the confederates as they offered the statements in teh racial loading conditions? And in the debriefings, did the research participants share anything more anecdotally in their experiences as research participants?
\item
  What are your current ideas about interventions or methods for mitigating the harm caused by racial microaggressions?
\item
  How do you expect the article to change science, practice, and/or advocacy?
\end{itemize}

\hypertarget{practice-problems-4}{%
\section{Practice Problems}\label{practice-problems-4}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that offer differing levels of difficulty. Whichever you choose, you will focus on these larger steps in one-way ANOVA, including:

\begin{itemize}
\tightlist
\item
  testing the statistical assumptions
\item
  conducting a one-way ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    follow-up (pairwise, planned comparisons, polynomial trends)
  \end{itemize}
\item
  writing a results section to include a figure and tables
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.}}

If one-way ANOVA is new to you, perhaps you just change the number in ``set.seed(2021)'' from 2021 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.

There are other ways to change the dataset. For example, if you are interested in power, change the sample size to something larger or smaller. If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.

\hypertarget{problem-2-conduct-a-one-way-anova-with-the-moretalk-dependent-variable.}{%
\subsection{\texorpdfstring{Problem \#2: Conduct a one-way ANOVA with the \emph{moreTalk} dependent variable.}{Problem \#2: Conduct a one-way ANOVA with the moreTalk dependent variable.}}\label{problem-2-conduct-a-one-way-anova-with-the-moretalk-dependent-variable.}}

In their study, Tran and Lee \citeyearpar{tran_you_2014} included an outcome variable where participants rated how much longer they would continue the interaction with their partner compared to their interactions in general. The scale ranged from -2 (\emph{much less than average}) through 0 (\emph{average}) to 2 (\emph{much more than average}). This variable is available in the original simulation and is an option for a slightly more challenging analysis.

\hypertarget{problem-3-try-something-entirely-new.}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a one-way ANOVA. Please have at least 3 levels for the predictor variable.

\hypertarget{grading-rubric-1}{%
\subsection{Grading Rubric}\label{grading-rubric-1}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the IV and DV & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct omnibus ANOVA (w effect size) & 5 & \_\_\_\_\_ \\
5. Conduct one set of follow-up tests; narrate your choice & 5 & \_\_\_\_\_ \\
6. Describe approach for managing Type I error & 5 & \_\_\_\_\_ \\
7. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
8 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{bonus-reel}{%
\section{Bonus Reel:}\label{bonus-reel}}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.45833in,height=2.19792in]{images/film-strip-1.jpg}
\caption{Image of a filmstrip signifying that the what follows is considered to be supplemental}\label{id}
}
\end{figure}

\hypertarget{whats-with-the-inline-code}{%
\subsection{What's with the inline code?}\label{whats-with-the-inline-code}}

If you are working in the .rmd file you may notice sections that look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this script is used for the inline coding in the lesson and,}
\CommentTok{\# although you will want to run it so you can \textquotesingle{}feed\textquotesingle{} the objects into}
\CommentTok{\# later script, there is no \textquotesingle{}lesson\textquotesingle{} relative to this lecture}
\NormalTok{df1omnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\NormalTok{, }\StringTok{"Df"}\NormalTok{], }\DecValTok{0}\NormalTok{)}
\NormalTok{df1omnibus}
\NormalTok{df2omnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{2}\NormalTok{, }\StringTok{"Df"}\NormalTok{], }\DecValTok{0}\NormalTok{)}
\NormalTok{df2omnibus}
\NormalTok{SSMomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\NormalTok{, }\StringTok{"Sum Sq"}\NormalTok{], }\DecValTok{3}\NormalTok{)}
\NormalTok{SSMomnibus}
\NormalTok{SSRomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{2}\NormalTok{, }\StringTok{"Sum Sq"}\NormalTok{], }\DecValTok{3}\NormalTok{)}
\NormalTok{SSRomnibus}
\NormalTok{SSTomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{((SSMomnibus }\SpecialCharTok{+}\NormalTok{ SSRomnibus), }\DecValTok{3}\NormalTok{)}
\NormalTok{SSTomnibus}
\NormalTok{MSMomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\NormalTok{, }\StringTok{"Mean Sq"}\NormalTok{],}
    \DecValTok{3}\NormalTok{)}
\NormalTok{MSMomnibus}
\NormalTok{MSRomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{2}\NormalTok{, }\StringTok{"Mean Sq"}\NormalTok{],}
    \DecValTok{3}\NormalTok{)}
\NormalTok{MSRomnibus}
\NormalTok{Fomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\NormalTok{, }\StringTok{"F value"}\NormalTok{], }\DecValTok{3}\NormalTok{)}
\NormalTok{Fomnibus}
\NormalTok{pomnibus }\OtherTok{\textless{}{-}}\NormalTok{ formattable}\SpecialCharTok{::}\FunctionTok{digits}\NormalTok{(}\FunctionTok{summary}\NormalTok{(omnibus)[[}\DecValTok{1}\NormalTok{]][}\DecValTok{1}\NormalTok{, }\StringTok{"Pr(\textgreater{}F)"}\NormalTok{], }\DecValTok{3}\NormalTok{)}
\NormalTok{pomnibus}
\end{Highlighting}
\end{Shaded}

I'm still learning about this and am using the lessons to practice. In these hidden (from the rendered view) boxes of script, I am capturing the output values as R objects. Later I can use inline code (the object's name, preceded with an ``r'', surrounded by tics {[}unfortunately, I cannot demo it in the .rmd file without getting an error{]}) to insert the value into the lecture notes and results. I'm working up to writing an entire journal article in R. This way, if something changes (e.g., more participants in the Qualtrics survey, a different approach to cleaning data) when I re-run the script everything auto-updates and I'm at less of a risk (or maybe a different kind of risk) for making typographical errors.

\hypertarget{between}{%
\chapter{Factorial (Between-Subjects) ANOVA}\label{between}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=3bb1bee1-c2ac-4cda-95f2-ad8b0152132c}{Screencasted Lecture Link}

In this (somewhat long and complex) lesson we conduct a 3X2 ANOVA. We will

\begin{itemize}
\tightlist
\item
  Work an actual example from the literature.

  \begin{itemize}
  \tightlist
  \item
    ``by hand'', and
  \item
    with R packages
  \end{itemize}
\item
  I will also demonstrate

  \begin{itemize}
  \tightlist
  \item
    several options for exploring interaction effects, and
  \item
    several options for exploring main effects.
  \end{itemize}
\item
  Exploring these options will allow us to:

  \begin{itemize}
  \tightlist
  \item
    Gain familiarity with the concepts central to multi-factor ANOVAs.
  \item
    Explore tools for analyzing the complexity in designs.
  \end{itemize}
\end{itemize}

The complexity is that not all of these things need to be conducted for every analysis. The Two-Way ANOVA Workflow is provided to help you map a way through your own analyses. I will periodically refer to this map so that we can more easily keep track of where we are in the process.

\hypertarget{navigating-this-lesson-6}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-6}}

There is about 1 hour and 30 minutes hours of lecture. If you work through the materials with me plan for another two hours of study.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-6}{%
\subsection{Learning Objectives}\label{learning-objectives-6}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Define, locate, and interpret all the effects associated with two-way ANOVA:

  \begin{itemize}
  \tightlist
  \item
    main
  \item
    interaction (introducing the concept, \emph{moderator})
  \item
    simple main effects
  \end{itemize}
\item
  Identify which means belong with which effects. Then compare and interpret them.

  \begin{itemize}
  \tightlist
  \item
    marginal means
  \item
    individual cell means
  \item
    comparing them
  \end{itemize}
\item
  Map a process/workflow for investigating a factorial ANOVA
\item
  Manage Type I error
\item
  Conduct a power analysis to determine sample size
\end{itemize}

\hypertarget{planning-for-practice-5}{%
\subsection{Planning for Practice}\label{planning-for-practice-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option comes from the research vignette. The Ramdhani et al. \citeyearpar{ramdhani_affective_2018} article has two dependent variables (DVs; negative and positive evaluation) which are suitable for two-way ANOVAs. I will demonstrate a simulation of one of their 3X2 ANOVAs (negative) in this lecturette. The second dependent variable (positive) is suggested for the moderate level of difficulty.

As a third option, you are welcome to use data to which you have access and is suitable for two-way ANOVA. In either case the practice options suggest that you:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a two-way ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    report main and interaction effects
  \item
    conduct follow-up testing of simple main effects
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{readings-resources-5}{%
\subsection{Readings \& Resources}\label{readings-resources-5}}

In preparing this chapter, I drew heavily from the following resource(s) that are freely available on the internet. Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Navarro, D. (2020). Chapter 16: Factorial ANOVA. In \href{https://learningstatisticswithr.com/}{Learning Statistics with R - A tutorial for Psychology Students and other Beginners}. Retrieved from \url{https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book\%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)}

  \begin{itemize}
  \tightlist
  \item
    Navarro's OER includes a good mix of conceptual information about one-way ANOVA as well as R code. My code/approach is a mix of Green and Salkind's \citeyearpar{green_using_2014}, Field's \citeyearpar{field_discovering_2012}, Navarro's \citeyearpar{navarro_chapter_2020}, and other techniques I have found on the internet and learned from my students.
  \end{itemize}
\item
  Ramdhani, N., Thontowi, H. B., \& Ancok, D. (2018). Affective Reactions Among Students Belonging to Ethnic Groups Engaged in Prior Conflict. \emph{Journal of Pacific Rim Psychology, 12}, e2. \url{https://doi.org/10.1017/prp.2017.22}

  \begin{itemize}
  \tightlist
  \item
    The source of our research vignette.
  \end{itemize}
\end{itemize}

\hypertarget{packages-4}{%
\subsection{Packages}\label{packages-4}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(knitr))\{install.packages(\textquotesingle{}knitr\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(lsr))\{install.packages(\textquotesingle{}lsr\textquotesingle{})\}}
\CommentTok{\# if(!require(car))\{install.packages(\textquotesingle{}car\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\}}
\CommentTok{\# if(!require(effectsize))\{install.packages(\textquotesingle{}effectsize\textquotesingle{})\}}
\CommentTok{\# if(!require(pwr2))\{install.packages(\textquotesingle{}pwr2\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-factorial-anova}{%
\section{Introducing Factorial ANOVA}\label{introducing-factorial-anova}}

My approach to teaching is to address the conceptual as we work problems. That said, there are some critical ideas we should address first.

\textbf{ANOVA is for experiments} (or arguably closely related designs). As we get into the assumptions you'll see that it has some rather restrictive ones (e.g., there should be an equal/equivalent number of cases per cell). To the degree that we violate these assumptions, we should locate alternative statistical approaches where these assumptions are relaxed.

\textbf{Factorial}: a term used when there are two or more independent variables (IVs; the factors). The factors could be between-groups, within-groups, repeated measures, or a combination of between and within.

\begin{itemize}
\tightlist
\item
  \textbf{Independent factorial design}: several IVs (predictors/factors) and each has been measured using different participants (between groups).
\item
  \textbf{Related factorial design}: several IVs (factors/predictors) have been measured, but the same participants have been used in all conditions (repeated measures or within-subjects).
\item
  \textbf{Mixed design}: several IVs (factors/predictors) have been measured. One or more factors uses different participants (between-subjects) and one or more factors uses the same participants (within-subjects). Thus, there is a cobination of independent (between) and related (within or repated) designs.
\item
  Factor naming follows a number/levels convention.
\item
  Today's example is a 3X2 ANOVA. We know there are two factors that have three and two levels, respectively: \emph{ethnicity} has three levels representing the two ethnic groups that were in prior conflict (Marudese, Dayaknese) and a third group who was uninvolved in the conflict (Javanese); \emph{photo stimulus} has two levels representing members of the two ethnic groups that were in prior conflict (Madurese, Dayaknese);
\end{itemize}

\textbf{Moderator} is what creates an interaction. Below are traditional representations of the \emph{statistical} and \emph{conceptual} figures of interaction effects. We will say that Factor B, \emph{moderates} the relationship between Factor A/IV and the DV.

In a later lesson we work an ANCOVA -- where we will distinguish between a \emph{moderator} and a \emph{covariate.} In regression models, you will likely be introduced to the \emph{mediator.}

\begin{figure}
\centering
\includegraphics{images/factorial/modfigs.jpg}
\caption{Graphic representations of a moderated relationship?}
\end{figure}

\hypertarget{workflow-for-two-way-anova}{%
\subsection{Workflow for Two-Way ANOVA}\label{workflow-for-two-way-anova}}

The following is a proposed workflow for conducting a two-way ANOVA.

\begin{figure}
\centering
\includegraphics{images/factorial/TwoWayWrkFlo.jpg}
\caption{An image of a workflow for the two-way ANOVA}
\end{figure}

Steps of the workflow include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Enter data

  \begin{itemize}
  \tightlist
  \item
    predictors should formatted as as factors (ordered or unordered); the dependent variable should be continuously scaled
  \item
    understanding the format of data can often provide clues as to which ANOVA/statistic to use
  \end{itemize}
\item
  Explore data

  \begin{itemize}
  \tightlist
  \item
    graphing the data
  \item
    computing descriptive statistics
  \item
    check distributional assumptions
  \item
    use Levene's test to check for homogeneity of variance
  \item
    use Shapiro Wilks to check for normality
  \end{itemize}
\item
  Construct or choose contrasts

  \begin{itemize}
  \tightlist
  \item
    select contrasts and specify for all of the IVs in the analysis
  \item
    if you want to use Type III sums of squares, contrasts must be orthogonal
  \end{itemize}
\item
  Compute the omnibus ANOVA

  \begin{itemize}
  \tightlist
  \item
    \emph{depending on what you found in the data exploration phase, you may need to run a robust version of the test}
  \end{itemize}
\item
  Follow-up testing based on significant main or interaction effects

  \begin{itemize}
  \tightlist
  \item
    significant interactions require test of simple main effects which could be further explored with contrasts, posthoc comparisons, and/or polynomials
  \item
    \emph{the exact methods you choose will depend upon the tests of assumptions during data exploration}
  \end{itemize}
\item
  Managing Type I error
\end{enumerate}

\hypertarget{research-vignette-5}{%
\section{Research Vignette}\label{research-vignette-5}}

The research vignette for this example was located in Kalimantan, Indonesia and focused on bias in young people from three ethnic groups. The Madurese and Dayaknese groups were engaged in ethnic conflict that spanned 1996 to 2001. The last incidence of mass violence was in 2001 where approximately 500 people (mostly from the Madurese ethnic group) were expelled from the province. Ramdhani et al.'s \citeyearpar{ramdhani_affective_2018} research hypotheses were based on the roles of the three ethnic groups in the study. The Madurese appear to be viewed as the transgressors when they occupied lands and took employment and business opportunities from the Dayaknese. Ramdhani et al.~also included a third group who were not involved in the conflict (Javanese). The research participants were students studying in Yogyakara who were not involved in the conflict. They included 39 Madurese, 35 Dyaknese, and 37 Javanese; 83 were male and 28 were female.

In the study \citep{ramdhani_affective_2018}, participants viewed facial pictures of three men and three women (in traditional dress) from each ethnic group (6 photos per ethnic group). Participant were asked, ``How do you feel when you see this photo? Please indicate your answers based on your actual feelings.'' Participants responded on a 7-point Likert scale ranging from 1 (\emph{strongly disagree}) to 7 (\emph{strongly agree}). Higher scores indicated ratings of higher intensity on that scale. The two scales included the following words:

\begin{itemize}
\tightlist
\item
  Positive: friendly, kind, helpful, happy
\item
  Negative: disgusting, suspicious, hateful, angry
\end{itemize}

Below is script to simulate data for the negative reactions variable from the information available from the manuscript \citep{ramdhani_affective_2018}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210731}\NormalTok{)}
\CommentTok{\# sample size, M and SD for each cell; this will put it in a long}
\CommentTok{\# file}
\NormalTok{Negative }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{17}\NormalTok{, }\AttributeTok{mean =} \FloatTok{1.91}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.73}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{, }\AttributeTok{mean =} \FloatTok{3.16}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.19}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{19}\NormalTok{, }\AttributeTok{mean =} \FloatTok{3.3}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.05}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{mean =} \DecValTok{3}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.07}\NormalTok{),}
    \FunctionTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.64}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.95}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{19}\NormalTok{, }\AttributeTok{mean =} \FloatTok{2.99}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.8}\NormalTok{)),}
    \DecValTok{3}\NormalTok{)}
\CommentTok{\# sample size, M and SD for each cell; this will put it in a long}
\CommentTok{\# file}
\NormalTok{Positive }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{17}\NormalTok{, }\AttributeTok{mean =} \FloatTok{4.99}\NormalTok{, }\AttributeTok{sd =} \FloatTok{1.38}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{, }\AttributeTok{mean =} \FloatTok{3.83}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{1.13}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{19}\NormalTok{, }\AttributeTok{mean =} \FloatTok{4.2}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.82}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{20}\NormalTok{, }\AttributeTok{mean =} \FloatTok{4.19}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.91}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{18}\NormalTok{, }\AttributeTok{mean =} \FloatTok{4.17}\NormalTok{, }\AttributeTok{sd =} \FloatTok{0.6}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{19}\NormalTok{, }\AttributeTok{mean =} \FloatTok{3.26}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{0.94}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{111}\NormalTok{))}
\NormalTok{Rater }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Dayaknese"}\NormalTok{, }\DecValTok{35}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Madurese"}\NormalTok{, }\DecValTok{39}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Javanese"}\NormalTok{, }\DecValTok{37}\NormalTok{))}
\NormalTok{Photo }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Dayaknese"}\NormalTok{, }\DecValTok{17}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Madurese"}\NormalTok{, }\DecValTok{18}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Dayaknese"}\NormalTok{,}
    \DecValTok{19}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Madurese"}\NormalTok{, }\DecValTok{20}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Dayaknese"}\NormalTok{, }\DecValTok{18}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Madurese"}\NormalTok{, }\DecValTok{19}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{Ramdhani\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, Negative, Positive, Rater, Photo)}
\end{Highlighting}
\end{Shaded}

For two-way ANOVA our variables need to be properly formatted. In our case:

\begin{itemize}
\tightlist
\item
  Negative is a continuously scaled DV and should be \emph{num}
\item
  Positive is a continuously scaled DV and should be \emph{num}
\item
  Rater should be an unordered factor
\item
  Photo should be an unordered facor
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Ramdhani\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   111 obs. of  5 variables:
 $ ID      : Factor w/ 111 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Negative: num  2.768 1.811 0.869 1.857 2.087 ...
 $ Positive: num  5.91 5.23 3.54 5.63 5.44 ...
 $ Rater   : chr  "Dayaknese" "Dayaknese" "Dayaknese" "Dayaknese" ...
 $ Photo   : chr  "Dayaknese" "Dayaknese" "Dayaknese" "Dayaknese" ...
\end{verbatim}

Our Negative variable is correctly formatted. Let's reformat Rater and Photo to be factors and ask for the structure again. R's default is to order the factors alphabetically. In this case this is fine. If we had ordered factors such as dosage (placebo, lo, hi) we would want to respecify the order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ramdhani\_df[, }\StringTok{"Rater"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Ramdhani\_df[, }\StringTok{"Rater"}\NormalTok{])}
\NormalTok{Ramdhani\_df[, }\StringTok{"Photo"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Ramdhani\_df[, }\StringTok{"Photo"}\NormalTok{])}
\FunctionTok{str}\NormalTok{(Ramdhani\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   111 obs. of  5 variables:
 $ ID      : Factor w/ 111 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Negative: num  2.768 1.811 0.869 1.857 2.087 ...
 $ Positive: num  5.91 5.23 3.54 5.63 5.44 ...
 $ Rater   : Factor w/ 3 levels "Dayaknese","Javanese",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Photo   : Factor w/ 2 levels "Dayaknese","Madurese": 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv (``Excel lite'') or .rds (R object) file. This is not a necessary step.

The code for .csv will likely lose the formatting (i.e., making the Rater and Photo variables factors), but it is easy to view in Excel.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Ramdhani\_df,}
\CommentTok{\# file=\textquotesingle{}RamdhaniCSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file Ramdhani\_df \textless{}{-}}
\CommentTok{\# read.csv (\textquotesingle{}RamdhaniCSV.csv\textquotesingle{}, header = TRUE) str(Ramdhani\_df)}
\end{Highlighting}
\end{Shaded}

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Ramdhani\_df, \textquotesingle{}Ramdhani\_RDS.rds\textquotesingle{}) bring back the}
\CommentTok{\# simulated dat from an .rds file Ramdhani\_RDS \textless{}{-}}
\CommentTok{\# readRDS(\textquotesingle{}Ramdhani\_RDS.rds\textquotesingle{}) str(Ramdhani\_RDS)}
\end{Highlighting}
\end{Shaded}

\hypertarget{preliminary-exploration-of-our-research-vignette}{%
\subsection{Preliminary exploration of our research vignette}\label{preliminary-exploration-of-our-research-vignette}}

Let's first examine the descriptive statistics (e.g., means of the variable, Negative) by group. We can use the \emph{describeBy()} function from the \emph{psych} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{negative.descripts }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{+}\NormalTok{ Photo, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{data =}\NormalTok{ Ramdhani\_df, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)  }\CommentTok{\#digits allows us to round the output}
\NormalTok{negative.descripts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1    group2 vars  n  mean    sd median trimmed   mad   min
X11    1 Dayaknese Dayaknese    1 17 1.818 0.768  1.692   1.783 0.694 0.706
X12    2  Javanese Dayaknese    1 18 2.524 0.742  2.391   2.460 0.569 1.406
X13    3  Madurese Dayaknese    1 19 3.301 1.030  3.314   3.321 1.294 1.406
X14    4 Dayaknese  Madurese    1 18 3.129 0.156  3.160   3.136 0.104 2.732
X15    5  Javanese  Madurese    1 19 3.465 0.637  3.430   3.456 0.767 2.456
X16    6  Madurese  Madurese    1 20 3.297 1.332  2.958   3.254 1.615 1.211
      max range   skew kurtosis    se
X11 3.453 2.747  0.513   -0.881 0.186
X12 4.664 3.258  1.205    1.475 0.175
X13 4.854 3.448 -0.126   -1.267 0.236
X14 3.423 0.691 -0.623    0.481 0.037
X15 4.631 2.175 -0.010   -1.307 0.146
X16 5.641 4.430  0.215   -1.238 0.298
\end{verbatim}

The \emph{write.table()} function can be a helpful way to export output to .csv files so that you can manipulate it into tables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.table}\NormalTok{(negative.descripts, }\AttributeTok{file =} \StringTok{"NegativeDescripts.csv"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{,}
    \AttributeTok{col.names =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At this stage, it would be useful to plot our data. Figures can assist in the conceptualization of the analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Photo"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-255-1.pdf}
Narrating results is sometimes made easier if variables are switched. There is usually not a right or wrong answer. Here is another view, switching the Rater and Photo predictors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Photo"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Rater"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Photo Stimulus"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-256-1.pdf}
Yet another option plots the raw data as bubbles, the means as lines, and denotes differences in the moderator with color.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggline}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Photo"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{, }\AttributeTok{add =} \FunctionTok{c}\NormalTok{(}\StringTok{"mean\_se"}\NormalTok{,}
        \StringTok{"dotplot"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Bin width defaults to 1/30 of the range of the data. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
Warning: Computation failed in `stat_summary()`:
object 'mean_se_' of mode 'function' was not found
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-257-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# add this for a different color palette: palette = c(\textquotesingle{}\#00AFBB\textquotesingle{},}
\CommentTok{\# \textquotesingle{}\#E7B800\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

We can reverse this to see if it assists with our conceptualization.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggline}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Photo"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Rater"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Photo Stimulus"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{, }\AttributeTok{add =} \FunctionTok{c}\NormalTok{(}\StringTok{"mean\_se"}\NormalTok{,}
        \StringTok{"dotplot"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Bin width defaults to 1/30 of the range of the data. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
Warning: Computation failed in `stat_summary()`:
object 'mean_se_' of mode 'function' was not found
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-258-1.pdf}

\hypertarget{working-the-factorial-anova-by-hand}{%
\section{Working the Factorial ANOVA (by hand)}\label{working-the-factorial-anova-by-hand}}

Before we work an ANOVA let's take a moment to consider what we are doing and how it informs our decision-making. This figure (which already contains ``the answers'') may help conceptualize how variance becomes partitioned.

\begin{figure}
\centering
\includegraphics{images/factorial/partition.png}
\caption{Image of a flowchart that partitions variance from sums of squares totals to its component pieces}
\end{figure}

As in one-way ANOVA, we partition variance into \textbf{total}, \textbf{model}, and \textbf{residual}. However, we now further divide the \(SS_M\) into its respective factors A(column), B(row,) and their a x b product.

In this, we begin to talk about main effects and interactions.

\hypertarget{sums-of-squares-total-1}{%
\subsection{Sums of Squares Total}\label{sums-of-squares-total-1}}

Our formula is the same as it was for one-way ANOVA:

\[SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}\]
Let's calculate it for the Ramdhani et al. \citeyearpar{ramdhani_affective_2018} data.
Our grand (i.e., overall) mean is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Negative)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.947369
\end{verbatim}

Subtracting the grand mean from each Negative rating yields a mean difference.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{Ramdhani\_df }\OtherTok{\textless{}{-}}\NormalTok{ Ramdhani\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_dev =}\NormalTok{ Negative }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Negative))}
\end{Highlighting}
\end{Shaded}

Pop quiz: What's the sum of our new \emph{m\_dev} variable?

Let's find out!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{m\_dev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.000000000000007549517
\end{verbatim}

Of course! The sum of squared deviations around the mean is zero. Next we square those mean deviations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Ramdhani\_df }\OtherTok{\textless{}{-}}\NormalTok{ Ramdhani\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_devSQ =}\NormalTok{ m\_dev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we sum the squared mean deviations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{m\_devSQ)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 114.7746
\end{verbatim}

This value, 114.775, the sum of squared deviations around the grand mean, is our \(SS_T\); the associated \emph{degrees of freedom} is \(N\) - 1.

In factorial ANOVA, we divide \(SS_T\) into \textbf{model/between} sums of squares and \textbf{residual/within} sums of squares.

\hypertarget{sums-of-squares-for-the-model}{%
\subsection{Sums of Squares for the Model}\label{sums-of-squares-for-the-model}}

\[SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}\]

The \emph{model} generally represents the notion that the means are different than each other. We want the variation between our means to be greater than the variation within each of the groups from which our means are calculated.

In factorial, we need to obtain means for each of the combinations of the factors. We have a 3 x 2:

\begin{itemize}
\tightlist
\item
  Rater with three levels: Dayaknese, Madurese, Javanese
\item
  Photo with two levels: Dayaknese, Madurese
\end{itemize}

Let's repeat some code we used before to obtain the cell-level means and cell sizes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{+}\NormalTok{ Photo, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df,}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1    group2 vars  n  mean    sd median trimmed   mad   min
X11    1 Dayaknese Dayaknese    1 17 1.818 0.768  1.692   1.783 0.694 0.706
X12    2  Javanese Dayaknese    1 18 2.524 0.742  2.391   2.460 0.569 1.406
X13    3  Madurese Dayaknese    1 19 3.301 1.030  3.314   3.321 1.294 1.406
X14    4 Dayaknese  Madurese    1 18 3.129 0.156  3.160   3.136 0.104 2.732
X15    5  Javanese  Madurese    1 19 3.465 0.637  3.430   3.456 0.767 2.456
X16    6  Madurese  Madurese    1 20 3.297 1.332  2.958   3.254 1.615 1.211
      max range   skew kurtosis    se
X11 3.453 2.747  0.513   -0.881 0.186
X12 4.664 3.258  1.205    1.475 0.175
X13 4.854 3.448 -0.126   -1.267 0.236
X14 3.423 0.691 -0.623    0.481 0.037
X15 4.631 2.175 -0.010   -1.307 0.146
X16 5.641 4.430  0.215   -1.238 0.298
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note. Recently my students and I have been having intermittent}
\CommentTok{\# struggles with the describeBy function in the psych package. We}
\CommentTok{\# have noticed that it is problematic when using .rds files and when}
\CommentTok{\# using data directly imported from Qualtrics. If you are having}
\CommentTok{\# similar difficulties, try uploading the .csv file and making the}
\CommentTok{\# appropriate formatting changes.}
\end{Highlighting}
\end{Shaded}

We also need the grand mean (i.e., the mean that disregards {[}or ``collapses across''{]} the factors).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Negative)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.947369
\end{verbatim}

This formula occurs in six chunks, representing the six cells of our designed. In each of the chunks we have the \(n\), group mean, and grand mean.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{17} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{1.818} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{18} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{2.524} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{19} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.301} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}
    \DecValTok{18} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.129} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{19} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.465} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{20} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.297} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 35.41501
\end{verbatim}

This value, 35.415, \(SS_M\) is the value accounted for by the model. That is, the amount of variance accounted for by the grouping variable/factors, Rater and Photo.

\hypertarget{sums-of-squares-residual-or-within-1}{%
\subsection{Sums of Squares Residual (or within)}\label{sums-of-squares-residual-or-within-1}}

\(SS_R\) is error associated with within group variability. If people are randomly assigned to treatment group there should be no other covariate (confounding variable) so that all \(SS_R\) variability is \emph{uninteresting} for the research and treated as noise.

\[SS_{R}= \sum(x_{ik}-\bar{x}_{k})^{^{2}}\]
Here's another configuration of the same:

\[SS_{R}= s_{group1}^{2}(n-1) + s_{group2}^{2}(n-1) + s_{group3}^{2}(n-1) + s_{group4}^{2}(n-1) + s_{group5}^{2}(n-1) + s_{group6}^{2}(n-1))\]

Again, the formula is in six chunks -- but this time the calculations are \emph{within-group}. We need the variance (the standard deviation squared) for the calculation. Let's take another look at our descriptives.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{+}\NormalTok{ Photo, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df,}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1    group2 vars  n  mean    sd median trimmed   mad   min
X11    1 Dayaknese Dayaknese    1 17 1.818 0.768  1.692   1.783 0.694 0.706
X12    2  Javanese Dayaknese    1 18 2.524 0.742  2.391   2.460 0.569 1.406
X13    3  Madurese Dayaknese    1 19 3.301 1.030  3.314   3.321 1.294 1.406
X14    4 Dayaknese  Madurese    1 18 3.129 0.156  3.160   3.136 0.104 2.732
X15    5  Javanese  Madurese    1 19 3.465 0.637  3.430   3.456 0.767 2.456
X16    6  Madurese  Madurese    1 20 3.297 1.332  2.958   3.254 1.615 1.211
      max range   skew kurtosis    se
X11 3.453 2.747  0.513   -0.881 0.186
X12 4.664 3.258  1.205    1.475 0.175
X13 4.854 3.448 -0.126   -1.267 0.236
X14 3.423 0.691 -0.623    0.481 0.037
X15 4.631 2.175 -0.010   -1.307 0.146
X16 5.641 4.430  0.215   -1.238 0.298
\end{verbatim}

Calculating \(SS_R\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{((}\FloatTok{0.768}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{17} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ ((}\FloatTok{0.742}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{18} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ ((}\FloatTok{1.03}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{19} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
\NormalTok{    ((}\FloatTok{0.156}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{18} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ ((}\FloatTok{0.637}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{19} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ ((}\FloatTok{1.332}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{20} \SpecialCharTok{{-}}
    \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 79.32078
\end{verbatim}

The value for our \(SS_R\) is 79.321. Its degrees of freedom is \(N - k\). That is, the total \(N\) minus the number of groups:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{111} \SpecialCharTok{{-}} \DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 105
\end{verbatim}

\hypertarget{a-recap-on-the-relationship-between-ss_t-ss_m-and-ss_r}{%
\subsection{\texorpdfstring{A Recap on the Relationship between \(SS_T\), \(SS_M\), and \(SS_R\)}{A Recap on the Relationship between SS\_T, SS\_M, and SS\_R}}\label{a-recap-on-the-relationship-between-ss_t-ss_m-and-ss_r}}

\(SS_T = SS_M + SS_R\)
In our case:

\begin{itemize}
\tightlist
\item
  \(SS_T\) was 114.775
\item
  \(SS_M\) was 35.415
\item
  \(SS_R\) was 79.321
\end{itemize}

Considering rounding error, we were successful!

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{35.415} \SpecialCharTok{+} \FloatTok{79.321}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 114.736
\end{verbatim}

\hypertarget{calculating-ss-for-each-factor-and-their-products}{%
\subsection{Calculating SS for Each Factor and Their Products}\label{calculating-ss-for-each-factor-and-their-products}}

\hypertarget{rater-main-effect}{%
\subsubsection{Rater Main Effect}\label{rater-main-effect}}

\(SS_a:Rater\) is calculated the same way as \(SS_M\) for one-way ANOVA. Simply collapse across Photo and calculate the \emph{marginal means} for Negative as a function of the Rater's ethnicity.

Reminder of the formula: \(SS_{a:Rater}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}\)

There are three cells involved in the calculation of \(SS_a:Rater\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1 vars  n  mean    sd median trimmed   mad   min   max range
X11    1 Dayaknese    1 35 2.492 0.856  2.900   2.561 0.480 0.706 3.453 2.747
X12    2  Javanese    1 37 3.007 0.831  2.913   2.986 0.984 1.406 4.664 3.258
X13    3  Madurese    1 39 3.299 1.179  3.116   3.288 1.588 1.211 5.641 4.430
      skew kurtosis    se
X11 -0.682   -1.132 0.145
X12  0.239   -0.923 0.137
X13  0.117   -1.036 0.189
\end{verbatim}

Again, we need the grand mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Negative)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.947369
\end{verbatim}

Now to calculate the Rater main effect.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{35} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{2.491} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{37} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.007} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{39} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.299} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 12.24322
\end{verbatim}

\hypertarget{photo-main-effect}{%
\subsubsection{Photo Main Effect}\label{photo-main-effect}}

\(SS_b:Photo\) is calculated the same way as \(SS_M\) for one-way ANOVA. Simply collapse across Rater and calculate the \emph{marginal means} for Negative as a function of the ethnicity reflected in the Photo stimulus:

Reminder of the formula: \(SS_{a:Photo}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}\).

With Photo, we have only two cells.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Photo, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1 vars  n  mean    sd median trimmed   mad   min   max range
X11    1 Dayaknese    1 54 2.575 1.043  2.449   2.516 0.921 0.706 4.854 4.148
X12    2  Madurese    1 57 3.300 0.871  3.166   3.280 0.667 1.211 5.641 4.430
    skew kurtosis    se
X11 0.47   -0.555 0.142
X12 0.35    0.581 0.115
\end{verbatim}

Again, we need the grand mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Negative)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.947369
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{54} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{2.575} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+} \DecValTok{57} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{3.3} \SpecialCharTok{{-}} \FloatTok{2.947}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 14.57545
\end{verbatim}

\hypertarget{interaction-effect}{%
\subsubsection{Interaction effect}\label{interaction-effect}}

The interaction term is simply the \(SS_M\) remaining after subtracting the SS from the main effects.

\(SS_{axb} = SS_M - (SS_a + SS_b)\)

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{35.415} \SpecialCharTok{{-}}\NormalTok{ (}\FloatTok{12.243} \SpecialCharTok{+} \FloatTok{14.575}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 8.597
\end{verbatim}

Let's revisit the figure I showed at the beginning of this section to see, again, how variance is partitioned.

\begin{figure}
\centering
\includegraphics{images/factorial/partition.png}
\caption{Image of a flowchart that partitions variance from sums of squares totals to its component pieces}
\end{figure}

\hypertarget{source-table-games-1}{%
\subsection{Source Table Games!}\label{source-table-games-1}}

As in the lesson for one-way ANOVA, we can use the hints in this source table to determine if we have statistically significance in the model. The formulas in the table provide some hints.

\begin{longtable}[]{@{}l@{}}
\toprule()
Summary ANOVA for Negative Reaction \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1961}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(MS = \frac{SS}{df}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F = \frac{MS_{source}}{MS_{resid}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F_{CV}\)
\end{minipage} \\
\midrule()
\endhead
Model & & \(k-1\) & & & \\
a & & \(k_{a}-1\) & & & \\
b & & \(k_{b}-1\) & & & \\
aXb & & \((df_{a})(df_{b})\) & & & \\
Residual & & \(n-k\) & & & \\
Total & & & & & \\
\bottomrule()
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hand{-}calculating the MS values}
\FloatTok{35.415}\SpecialCharTok{/}\DecValTok{5}  \CommentTok{\#Model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.083
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{12.243}\SpecialCharTok{/}\DecValTok{2}  \CommentTok{\#a: Rater}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.1215
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{14.575}\SpecialCharTok{/}\DecValTok{1}  \CommentTok{\#b:  Photo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 14.575
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{8.597}\SpecialCharTok{/}\DecValTok{2}  \CommentTok{\#axb interaction term}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.2985
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{79.321}\SpecialCharTok{/}\DecValTok{105}  \CommentTok{\#residual}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7554381
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hand{-}calculating the F values}
\FloatTok{7.083}\SpecialCharTok{/}\FloatTok{0.755}  \CommentTok{\#Model}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9.381457
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{6.122}\SpecialCharTok{/}\FloatTok{0.755}  \CommentTok{\#a: Rater}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 8.108609
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{14.575}\SpecialCharTok{/}\FloatTok{0.755}  \CommentTok{\#b:  Photo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 19.30464
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{4.299}\SpecialCharTok{/}\FloatTok{0.755}  \CommentTok{\#axb interaction term}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.69404
\end{verbatim}

To find the \(F_{CV}\) we can use an \href{https://www.statology.org/f-distribution-table/}{F distribution table}.

Or use a look-up function, which follows this general form: qf(p, df1, df2. lower.tail=FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# looking up the F critical values}
\FunctionTok{qf}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{105}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)  }\CommentTok{\#Model F critical value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.300888
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qf}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{105}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)  }\CommentTok{\#a and axb F critical value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.082852
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qf}\NormalTok{(}\FloatTok{0.05}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{105}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)  }\CommentTok{\#b F critical value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.931556
\end{verbatim}

When the \(F\) value exceeds the \(F_{CV}\), the effect is statistically significant.

\begin{longtable}[]{@{}l@{}}
\toprule()
Summary ANOVA for Negative Reaction \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1961}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2157}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1373}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
df
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(MS = \frac{SS}{df}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F = \frac{MS_{source}}{MS_{resid}}\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(F_{CV}\)
\end{minipage} \\
\midrule()
\endhead
Model & 35.415 & 5 & 7.083 & 9.381 & 2.301 \\
a & 12.243 & 2 & 6.122 & 8.109 & 3.083 \\
b & 14.575 & 1 & 14.575 & 19.305 & 3.932 \\
aXb & 8.597 & 2 & 4.299 & 5.694 & 3.083 \\
Residual & 79.321 & 105 & 0.755 & & \\
Total & 114.775 & & & & \\
\bottomrule()
\end{longtable}

\hypertarget{interpreting-the-results}{%
\subsection{Interpreting the results}\label{interpreting-the-results}}

What have we learned?

\begin{itemize}
\tightlist
\item
  there is a main effect for Rater
\item
  there is a main effect for Photo
\item
  there is a significant interaction effect
\end{itemize}

In the face of this significant interaction effect, we would follow-up by investigating the interaction effect. Why? The significant interaction effect means that findings (e.g., the story of the results) are more complex than group identity or photo stimulus, alone, can explain.

\hypertarget{working-the-factorial-anova-with-r-packages}{%
\section{Working the Factorial ANOVA with R packages}\label{working-the-factorial-anova-with-r-packages}}

\hypertarget{evaluating-the-statistical-assumptions-1}{%
\subsection{Evaluating the statistical assumptions}\label{evaluating-the-statistical-assumptions-1}}

All statistical tests have some assumptions about the data. This particular ANOVA has four:

Assumptions

\begin{itemize}
\tightlist
\item
  Cases represent random samples from the populations

  \begin{itemize}
  \tightlist
  \item
    This is an issue of research design
  \item
    Although we see ANOVA used (often incorrectly) in other settings, ANOVA was really designed for the random clinical trial (RCT).
  \end{itemize}
\item
  Scores on the DV are independent of each other.

  \begin{itemize}
  \tightlist
  \item
    With correlated observations, there is a dramatic increase of Type I error
  \item
    There are options designed for analyzing data that has dependencies (e.g., repeated measures ANOVA, dyadic data analysis, multilevel modeling)
  \end{itemize}
\item
  The DV is normally distributed for each of the populations

  \begin{itemize}
  \tightlist
  \item
    that is, data for each cell (representing the combinations of each factor) is normally distributed
  \end{itemize}
\item
  Population variances of the DV are the same for all cells

  \begin{itemize}
  \tightlist
  \item
    When cell sizes are not equal, ANOVA not robust to this violation and cannot trust F ratio
  \end{itemize}
\end{itemize}

Even though we position the evaluation of assumptions first -- some of the best tests of the assumptions use the resulting ANOVA model. Because of this, I will quickly run the model now. I will not explain the results until after we evaluate the assumptions.

I have marked our Two-Way ANOVA Workflow with a yellow box outlined in red to let us know that we are just beginning the process of analyzing our data.

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlw_Assumptions.jpg}
\caption{Image of a flowchart showing that we are on the ``Evaluating assumptions'' portion of the workflow}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TwoWay\_neg }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{*}\NormalTok{ Photo, Ramdhani\_df)}
\FunctionTok{summary}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Df Sum Sq Mean Sq F value    Pr(>F)    
Rater         2  12.21   6.103   8.077  0.000546 ***
Photo         1  14.62  14.619  19.346 0.0000262 ***
Rater:Photo   2   8.61   4.304   5.696  0.004480 ** 
Residuals   105  79.34   0.756                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{model.tables}\NormalTok{(TwoWay\_neg, }\StringTok{"means"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tables of means
Grand mean
         
2.947369 

 Rater 
    Dayaknese Javanese Madurese
        2.492    3.007    3.299
rep    35.000   37.000   39.000

 Photo 
    Dayaknese Madurese
        2.575    3.301
rep    54.000   57.000

 Rater:Photo 
           Photo
Rater       Dayaknese Madurese
  Dayaknese  1.818     3.129  
  rep       17.000    18.000  
  Javanese   2.524     3.465  
  rep       18.000    19.000  
  Madurese   3.301     3.298  
  rep       19.000    20.000  
\end{verbatim}

\hypertarget{dv-is-normally-distributed}{%
\subsubsection{DV is normally distributed}\label{dv-is-normally-distributed}}

Let's start by analyzing \textbf{skew} and \textbf{kurtosis}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{+}\NormalTok{ Photo, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df,}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item    group1    group2 vars  n  mean    sd median trimmed   mad   min
X11    1 Dayaknese Dayaknese    1 17 1.818 0.768  1.692   1.783 0.694 0.706
X12    2  Javanese Dayaknese    1 18 2.524 0.742  2.391   2.460 0.569 1.406
X13    3  Madurese Dayaknese    1 19 3.301 1.030  3.314   3.321 1.294 1.406
X14    4 Dayaknese  Madurese    1 18 3.129 0.156  3.160   3.136 0.104 2.732
X15    5  Javanese  Madurese    1 19 3.465 0.637  3.430   3.456 0.767 2.456
X16    6  Madurese  Madurese    1 20 3.297 1.332  2.958   3.254 1.615 1.211
      max range   skew kurtosis    se
X11 3.453 2.747  0.513   -0.881 0.186
X12 4.664 3.258  1.205    1.475 0.175
X13 4.854 3.448 -0.126   -1.267 0.236
X14 3.423 0.691 -0.623    0.481 0.037
X15 4.631 2.175 -0.010   -1.307 0.146
X16 5.641 4.430  0.215   -1.238 0.298
\end{verbatim}

Using guidelines from Kline \citeyearpar{kline_principles_2016} our values for skewness should fall below 3.0 (they do) and all values for kurtosis should fall below 8 to 20 (ours do).

In a factorial design, the Shapiro-Wilk test is applied to residuals from the model itself. Examination of those residuals can give us a good indication of normality.

First, we extract the residuals (i.e., that which is left-over/unexplained) from the model. We can examine their distribution with a plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# creates object of residuals}
\NormalTok{resid\_neg }\OtherTok{\textless{}{-}} \FunctionTok{residuals}\NormalTok{(TwoWay\_neg)}
\FunctionTok{hist}\NormalTok{(resid\_neg)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-282-1.pdf}
So far so good -- these look normal. Let's examine a QQ plot. We want the dots (the
Our distribution of \emph{residuals} (i.e., what is leftover after the model is applied) resembles a normal distribution.

The Q-Q plot provides another view. If the residuals are normally distributed, they will line up on the diagonal line (within reason).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(resid\_neg)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-283-1.pdf}
We can formally test the distribution of the residuals with a Shapiro test. We want the

We can formally test the distribution of the residuals with a Shapiro test. We want the associated \emph{p} value to be greather than 0.05.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(resid\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  resid_neg
W = 0.98464, p-value = 0.2344
\end{verbatim}

Whooo hoo! \(p > 0.05\)

Here's how I would summarize our data in terms of normality:

Factorial ANOVA assumes that the dependent variable is normally is distributed for all cells in the design. Our analysis suggested skew and kurtosis were within the bounds considered to be normally distributed. Further, the Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) suggested that the plotting of the residuals did not differ significantly from a normal distribution (\(W\) = 0.9846, \(p\) = 0.234).

\hypertarget{homogeneity-of-variance}{%
\subsubsection{Homogeneity of variance}\label{homogeneity-of-variance}}

We can evaluate the homogeneity of variance test with the Levene's test for the equality of error variances. Levene's requires a \emph{fully saturated model.} This means that the prediction model requires an interaction effect (not just two, non-interacting predictors). We can use the \emph{leveneTest()} function from the \emph{car} package. Within the function we specify the model we will be testing in the factorial ANOVA. That is, predicting Negative from the Rater and Photo factors. The asterisk indicates that they will also be added as an interaction term.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{leveneTest}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{*}\NormalTok{ Photo, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Levene's Test for Homogeneity of Variance (center = median)
       Df F value       Pr(>F)    
group   5  8.6342 0.0000007002 ***
      105                         
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Levene's test has indicated a violation of the homogeneity of variance assumption (\(F\){[}5, 105{]} = 8.634, \(p\) \textless{} .001). This is not surprising. The boxplots shows some widely varying variances.

\hypertarget{evaluating-the-omnibus-anova}{%
\subsection{Evaluating the Omnibus ANOVA}\label{evaluating-the-omnibus-anova}}

The \emph{F}-tests associated with the two-way ANOVA are the \emph{omnibus} -- providing the result for the main and interaction effects.

Here's where we are in the workflow.

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlw_Omnibus.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

When we run the two-way ANOVA we will be looking for several effects:

\begin{itemize}
\tightlist
\item
  main effects for each predictor, and
\item
  the interaction effect.
\end{itemize}

It is possible that all effects will be significant, none will be significant, or some will be significant. The interaction effect always takes precedence over the main effect because it let's us know there is a more nuanced/complex story.

In specifying the ANOVA, order of entry matters. If you have a distinction between IV and moderator, put the IV first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{TwoWay\_neg }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater }\SpecialCharTok{*}\NormalTok{ Photo, Ramdhani\_df)}
\FunctionTok{summary}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Df Sum Sq Mean Sq F value    Pr(>F)    
Rater         2  12.21   6.103   8.077  0.000546 ***
Photo         1  14.62  14.619  19.346 0.0000262 ***
Rater:Photo   2   8.61   4.304   5.696  0.004480 ** 
Residuals   105  79.34   0.756                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{model.tables}\NormalTok{(TwoWay\_neg, }\StringTok{"means"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tables of means
Grand mean
         
2.947369 

 Rater 
    Dayaknese Javanese Madurese
        2.492    3.007    3.299
rep    35.000   37.000   39.000

 Photo 
    Dayaknese Madurese
        2.575    3.301
rep    54.000   57.000

 Rater:Photo 
           Photo
Rater       Dayaknese Madurese
  Dayaknese  1.818     3.129  
  rep       17.000    18.000  
  Javanese   2.524     3.465  
  rep       18.000    19.000  
  Madurese   3.301     3.298  
  rep       19.000    20.000  
\end{verbatim}

Let's write the \emph{F strings}''* from the above table.

\begin{itemize}
\tightlist
\item
  Rater main effect: (\emph{F}{[}2, 105{]} = 8.077, \emph{p} \textless{} .001).
\item
  Photo stimulus main effect: (\emph{F}{[}1, 105{]} = 19.346, \emph{p} \textless{} .001).
\item
  Interaction effect: (\emph{F}{[}2, 105{]} = 5.696, \emph{p} = .004).
\end{itemize}

The \emph{plot()} function provides some quick plots from the object created from the ANOVA.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-287-1.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-287-2.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-287-3.pdf} \includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-287-4.pdf}

\hypertarget{effect-sizes}{%
\subsubsection{Effect sizes}\label{effect-sizes}}

\textbf{Eta squared} is one of the most commonly used measures of effect. It refers to the proportion of variability in the DV/outcome variable that can be explained in terms of the IVs/predictors. Traditional interpretive values are similar to the Pearson's \emph{r}:

\begin{itemize}
\tightlist
\item
  0 = no relationship
\item
  .02 = small
\item
  .13 = medium
\item
  .26 = large
\item
  1.0 = a perfect (one-to-one) correspondence
\end{itemize}

The formula for \(\eta ^{2}\) is straightforward:

\[\eta ^{2}=\frac{SS_{M}}{SS_{T}}\]
We can apply the \emph{etaSquared()} function from the \emph{lsr} package to our ANOVA object to retrieve \$\eta \^{}\{2\}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                eta.sq eta.sq.part
Rater       0.10662441  0.13363091
Photo       0.12736755  0.15558329
Rater:Photo 0.07500609  0.09788289
\end{verbatim}

We can update our \emph{F} strings to include the effect size:

\begin{itemize}
\tightlist
\item
  Rater main effect: (\emph{F}{[}2, 105{]} = 8.077, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.107).
\item
  Photo stimulus main effect: (\emph{F}{[}1, 105{]} = 19.346, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.127).
\item
  Interaction effect: (\emph{F}{[}2, 105{]} = 5.696, \emph{p} = .004, \(\eta ^{2}\) = 0.075).
\end{itemize}

Before moving to follow-up, let's capture an APA style write-up so far.

\hypertarget{apa-write-up-of-the-omnibus-results}{%
\subsubsection{APA Write-up of the omnibus results}\label{apa-write-up-of-the-omnibus-results}}

\begin{quote}
A 3 X 2 ANOVA was conducted to evaluate the effects of rater ethnicity (3 levels, Dayaknese, Madurese, Javanese) and photo stimulus (2 levels, Dayaknese on Madurese,) on negative reactions to the photo stimuli. Results of Levene's Test for Equality of Error Variances indicated violation of the homogeneity of variance assumption, (\(F\){[}5, 105{]} = 8.834, \(p\) \textless{} .001). Our analysis of the individual cell means (see Table 1 for means and standard deviations) suggested skew and kurtosis were within the bounds considered to be normally distributed \citep{kline_principles_2016}. A non-significant Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) provided further evidence that the assumption of normality was not violated (\(W\) = 0.9846, \(p\) = 0.234).
\end{quote}

\begin{quote}
Computing sums of squares with a Type II approach, the results for the ANOVA indicated a significant main effect for ethnicity of the rater (\emph{F}{[}2, 105{]} = 8.077, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.107), a significant main effect for photo stimulus, (\emph{F}{[}1, 105{]} = 19.346, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.127), and a significant interaction effect (\emph{F}{[}2, 105{]} = 5.696, \emph{p} = .004, \(\eta ^{2}\) = 0.075).
\end{quote}

Note. The next paragraph will have one of the follow-up options. We will add it later in the lesson

\hypertarget{follow-up-a-significant-interaction-effect}{%
\subsection{Follow-up a significant interaction effect}\label{follow-up-a-significant-interaction-effect}}

In factorial ANOVA we are interested in main effects and interaction effects. When the result is explained by a main effect, then there is a consistent trend as a function of a factor (e.g., Madurese raters had consistently higher Negative evaluations, irrespective of stimulus). In an interaction effect, the results are more complex (e.g., the ratings across the stimulus differed for the three groups of raters).

There are a variety of strategies to follow-up a significant interaction effect. I will demonstrate the four I believe to be the most useful in the context of psychologists operating within the scientist-practitioner-advocacy context.

When an interaction effect is significant (irrespective of the significance of one or more main effects), examination of \textbf{simple main effects} is a common statistical/explanatory approached that is used. The Two-Way ANOVA Workflow shows where we are in this process.

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlo_IntSmp.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

\hypertarget{option-1-the-simple-main-effect-of-photo-stimulus-within-ethnicity-of-the-rater}{%
\subsubsection{Option \#1 the simple main effect of photo stimulus within ethnicity of the rater}\label{option-1-the-simple-main-effect-of-photo-stimulus-within-ethnicity-of-the-rater}}

Here we subset each of the three ethnic groups and then compare their ratings of the two photos. Essentially, we are conducting a one-way ANOVA for the Dyaknese ratings of the Dayaknese and Madurese photos.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset data}
\NormalTok{Dayaknese }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Ramdhani\_df, Rater }\SpecialCharTok{==} \StringTok{"Dayaknese"}\NormalTok{)}
\CommentTok{\# change df to subset, new model name}
\NormalTok{Dayaknese\_simple }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Photo, }\AttributeTok{data =}\NormalTok{ Dayaknese)}
\CommentTok{\# output for simple main effect}
\FunctionTok{summary}\NormalTok{(Dayaknese\_simple)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value       Pr(>F)    
Photo        1 15.040  15.040    50.4 0.0000000395 ***
Residuals   33  9.847   0.298                         
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size for simple main effect can add \textquotesingle{}type = 1,2,3,4\textquotesingle{} to}
\CommentTok{\# correspond with the ANOVA that was run}
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(Dayaknese\_simple, }\AttributeTok{anova =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         eta.sq eta.sq.part
Photo 0.6043362   0.6043362
\end{verbatim}

Within the Dayaknese ethnic group, there is a statistically significant difference in negative reactions to Dayaknese and Madurese photos: \emph{F} (1, 33) = 50.4, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.60.

Next we evaluate photo rating within the Madurese ethnic group.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset data}
\NormalTok{Madurese }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Ramdhani\_df, Rater }\SpecialCharTok{==} \StringTok{"Madurese"}\NormalTok{)}
\CommentTok{\# change df to subset, new model name}
\NormalTok{Madurese\_simple }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Photo, }\AttributeTok{data =}\NormalTok{ Madurese)}
\CommentTok{\# output for simple main effect}
\FunctionTok{summary}\NormalTok{(Madurese\_simple)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value Pr(>F)
Photo        1   0.00  0.0001       0  0.993
Residuals   37  52.82  1.4275               
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size for simple main effect can add \textquotesingle{}type = 1,2,3,4\textquotesingle{} to}
\CommentTok{\# correspond with the ANOVA that was run}
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(Madurese\_simple, }\AttributeTok{anova =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              eta.sq    eta.sq.part
Photo 0.000002060568 0.000002060568
\end{verbatim}

Within the Madurese ethnic group, there was a nonsignificant difference in negative reactions to Dayaknese and Madurese photos: \emph{F} (1, 37) = 0.00, \emph{p} = .993, \(\eta ^{2}\) \textless{} .001.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset data}
\NormalTok{Javanese }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Ramdhani\_df, Rater }\SpecialCharTok{==} \StringTok{"Javanese"}\NormalTok{)}
\CommentTok{\# change df to subset, new model name}
\NormalTok{Javanese\_simple }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Photo, }\AttributeTok{data =}\NormalTok{ Javanese)}
\CommentTok{\# output for simple main effect}
\FunctionTok{summary}\NormalTok{(Javanese\_simple)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value   Pr(>F)    
Photo        1  8.188   8.188   17.18 0.000205 ***
Residuals   35 16.678   0.477                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size for simple main effect can add \textquotesingle{}type = 1,2,3,4\textquotesingle{} to}
\CommentTok{\# correspond with the ANOVA that was run}
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(Javanese\_simple, }\AttributeTok{anova =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         eta.sq eta.sq.part
Photo 0.3292776   0.3292776
\end{verbatim}

Within the Javanese ethnic group, there was a significant difference in negative reactions to Dayaknese and Madurese photos: \emph{F} (1, 35) = 17.18, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.33.

If I were using this approach in a 3 X 2 ANOVA, I would probably not control for Type I error. Why? I only conducted follow-up comparisons to evaluate the simple main effect of photo stimulus within rater ethnicity; that is, I would hold it at alpha = 0.05.

\begin{itemize}
\tightlist
\item
  Photo stimulus (Dayaknese or Madurese) within the Dayaknese ethnic group.
\item
  Photo stimulus (Dayaknese or Madurese) within the Madurese ethnic group.
\item
  Photo stimulus (Dayaknese or Madurese) within the Javanese ethnic group.
\end{itemize}

However, because it is good for instruction, it would be equally fine to use a traditional Bonferroni, dividing .05/3 = 0.017 and testing each at 0.017. I will use this approach in the write-up.

FAQ: Could we do the reverse simple effect, ethnicity of rater within the photo stimulus? Absolutely! The choice is yours (and sometimes the results will differ). I usually run both and then report ONE -- the one that conveys the story the data has to tell. You \emph{could} report both sets, but then you would really want to control Type I error and your repetitive contrasts are far from independent/orthogonal.

\textbf{APA Style Results for Option \#1 follow-up.} This would be added to the results of the omnibus two-way ANOVA.

\begin{quote}
\emph{Option \#1}: To explore the interaction effect, we followed with a test of the simple main effect of photo stimulus within the ethnicity of the rater. That is, we looked at the effect of the photo stimulus within the Dayaknese, Madurese, and Javanese groups, separately. To control for Type I error across the three simple main effects, we set alpha at .017 (.05/3). Results indicated significant differences for Dayaknese (\emph{F} {[}1, 33{]} = 50.4, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.60.) and Javanese ethnic groups (\emph{F} {[}1, 35{]}= 17.18, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.33), but not for the Madurese ethnic group (\emph{F} {[}1, 37{]} = 0.000, \emph{p} = .993, \(\eta ^{2}\) \textless{} .001). As illustrated in Figure 1, the Dayaknese and Javanese rathers both reported stronger negative reactions to the Madurese. The differences in ratings for the Madurese were not statistically significantly different. In this way, the rater's ethnic group moderated the relationship between the photo stimulus and negative reactions.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Photo"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-292-1.pdf}

\hypertarget{option-2-the-simple-main-effect-of-ethnicity-of-rater-within-photo-stimulus.}{%
\subsubsection{Option \#2 the simple main effect of ethnicity of rater within photo stimulus.}\label{option-2-the-simple-main-effect-of-ethnicity-of-rater-within-photo-stimulus.}}

In this simple main effect of ethnicity of rater (3 levels) within photo stimulus (2 levels), we will conduct two one-way ANOVAs for the Dayaknese and Madurese photos, separately. However, we will want to do orthogonal contrast-coding for rater ethnicity for the follow-up (to the follow-up).

It helps to know what the default contrast codes are; we can get that information with the \emph{contrasts()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          Javanese Madurese
Dayaknese        0        0
Javanese         1        0
Madurese         0        1
\end{verbatim}

Let's create custom contrasts. Recall that an orthogonal contrast requires that there be one less contrast than the number of groups and that once a group is singled out, it cannot be compared again.

Thus, I want to compare the

\begin{itemize}
\tightlist
\item
  Javanese to the Dayaknese and Madurese combined, then
\item
  Dayaknese to Madurese
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# tell R which groups to compare}
\NormalTok{c1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{c2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{mat }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(c1, c2)  }\CommentTok{\#combine the above bits}
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}}\NormalTok{ mat  }\CommentTok{\# attach the contrasts to the variable}
\end{Highlighting}
\end{Shaded}

This allows us to recheck the contrasts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          c1 c2
Dayaknese  1 -1
Javanese  -2  0
Madurese   1  1
\end{verbatim}

Yes, in contrast 1 we are comparing the Javanese to the combined Dayaknese and Madurese. In contrast 2 we are comparing the Dayaknese to the Madureses.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset data}
\NormalTok{Dayaknese\_Ph }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Ramdhani\_df, Photo }\SpecialCharTok{==} \StringTok{"Dayaknese"}\NormalTok{)}
\CommentTok{\# change df to subset, new model name}
\NormalTok{Dykn\_simple }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Dayaknese\_Ph)}
\CommentTok{\# output for simple main effect}
\FunctionTok{summary}\NormalTok{(Dykn\_simple)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value    Pr(>F)    
Rater        2  19.81   9.903   13.32 0.0000221 ***
Residuals   51  37.90   0.743                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size for simple main effect can add \textquotesingle{}type = 1,2,3,4\textquotesingle{} to}
\CommentTok{\# correspond with the ANOVA that was run}
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(Dykn\_simple, }\AttributeTok{anova =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         eta.sq eta.sq.part
Rater 0.3432006   0.3432006
\end{verbatim}

We can capture the \emph{F} string from this output: \emph{F} {[}2, 51{]}) = 13.32, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.343.

This code produces the contrasts we specified. Note that in our code we can improve the interpretability of the output by adding labels. We know the specific contrasts from our prior work.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary.aov}\NormalTok{(Dykn\_simple, }\AttributeTok{split =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Rater =} \FunctionTok{list}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Javanese v Dayaknese and Madurese}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{1}\NormalTok{,}
    \StringTok{\textasciigrave{}}\AttributeTok{Dayaknese Madurese}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                           Df Sum Sq Mean Sq F value     Pr(>F)
Rater                                       2  19.81   9.903  13.325 0.00002211
  Rater: Javanese v Dayaknese and Madurese  1   0.07   0.071   0.095      0.759
  Rater: Dayaknese Madurese                 1  19.73  19.735  26.554 0.00000419
Residuals                                  51  37.90   0.743                   
                                              
Rater                                      ***
  Rater: Javanese v Dayaknese and Madurese    
  Rater: Dayaknese Madurese                ***
Residuals                                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The simple main effect of ethnicity of the rater within the reaction to the photos of members of the Dayaknese ethnic group was statistically significant: \emph{F} {[}2, 51{]} = 13.32, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.343. Follow-up testing indicated non-significant differences when the ratings from members of the Javanese ethnic group were compared to the Dayaknese and Madurese, combined (\emph{F} {[}1, 51{]} = 0.095, \emph{p} = .759). There was a statistically significant difference when Dayaknese and Madurese raters were compared (\emph{F} {[}1, 51{]} =26.554, \emph{p} \textless{} .001)

We repeat the simple main effect process when the Madurese photos were the stimulus.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subset data}
\NormalTok{Madurese\_Ph }\OtherTok{\textless{}{-}} \FunctionTok{subset}\NormalTok{(Ramdhani\_df, Photo }\SpecialCharTok{==} \StringTok{"Madurese"}\NormalTok{)}
\CommentTok{\# change df to subset, new model name}
\NormalTok{Mdrs\_simple }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Madurese\_Ph)}
\CommentTok{\# output for simple main effect}
\FunctionTok{summary}\NormalTok{(Mdrs\_simple)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df Sum Sq Mean Sq F value Pr(>F)
Rater        2   1.04  0.5207   0.679  0.512
Residuals   54  41.44  0.7674               
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# effect size for simple main effect can add \textquotesingle{}type = 1,2,3,4\textquotesingle{} to}
\CommentTok{\# correspond with the ANOVA that was run}
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(Mdrs\_simple, }\AttributeTok{anova =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          eta.sq eta.sq.part
Rater 0.02451385  0.02451385
\end{verbatim}

Let's capture the \emph{F} string for ratings of the Madurese photos: \emph{F} {[}2, 54{]} = 0.679, \emph{p} = .512, \(\eta ^{2}\) = 0.024.

We can use the procedure described above to obtain our orthogonal contrasts.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary.aov}\NormalTok{(Mdrs\_simple, }\AttributeTok{split =} \FunctionTok{list}\NormalTok{(}\AttributeTok{Rater =} \FunctionTok{list}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Javanese v Dayaknese and Madurese}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{1}\NormalTok{,}
    \StringTok{\textasciigrave{}}\AttributeTok{Dayaknese Madurese}\StringTok{\textasciigrave{}} \OtherTok{=} \DecValTok{2}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                           Df Sum Sq Mean Sq F value Pr(>F)
Rater                                       2   1.04  0.5207   0.679  0.512
  Rater: Javanese v Dayaknese and Madurese  1   0.77  0.7734   1.008  0.320
  Rater: Dayaknese Madurese                 1   0.27  0.2679   0.349  0.557
Residuals                                  54  41.44  0.7674               
\end{verbatim}

Here's a write-up of this portion of the result.

\begin{quote}
The simple main effect of ethnicity of the rater within rating the photos of Madurese people was not statistically significant: (\emph{F} {[}2, 54{]} = 0.679, \emph{p} = .512, \(\eta ^{2}\) = 0.024). Correspondingly, follow-up testing indicated non-significant differences when the ratings of the Javanese were compared to Dayaknese and Madurese, combined (\emph{F} {[}1, 54{]} = 1.008, \emph{p} = .320) and when the ratings of the Dayaknese and Madurese were compared (\emph{F} {[}1, 54{]} = 0.349, \emph{p} = .557)
\end{quote}

To control for Type I error, we have 4 follow-up contrasts (2 for Dayaknese, 2 for Madurese). We'll control Type I error with .05/4 = .0125

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{4}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0125
\end{verbatim}

\textbf{APA Write-up of the simple main effect of photo stimulus within rater ethnicity.}

This would be added to the write-up of the omnibus two-way ANOVA test.

\begin{quote}
\emph{Option \#2}: To explore the interaction effect, we followed with tests of simple effect of rater ethnicity within the photo stimulus. That is, we looked at the effect of each each rater's ethnicity within the Madurese and Dayaknese photo stimulus, separately. Our first analysis evaluated the effect of the rater's ethnicity when evaluating the Dayaknese photo; our second analysis evaluated effect of the rater's ethnicity when evaluating the Madurese photo. To control for Type I error across the two simple main effects, we set alpha at .0125 (.05/4). The simple main effect of ethnicity of the rater within the reaction to the photos of members of the Dayaknese ethnic group was statistically significant: \emph{F} {[}2, 51{]} = 13.32, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.343. Follow-up testing indicated non-significant differences when the ratings from members of the Javanese ethnic group were compared to the Dayaknese and Madurese, combined (\emph{F} {[}1, 51{]} = 0.095, \emph{p} = .759). There was a statistically significant difference when Dayaknese and Madurese raters were compared (\emph{F} {[}1, 51{]} =26.554, \emph{p} \textless{} .001). The simple main effect of ethnicity of the rater within when rating the photos of Madurese people was not statistically significant: (\emph{F} {[}2, 54{]} = 0.679, \emph{p} = .512, \(\eta ^{2}\) = 0.024). Correspondingly, follow-up testing indicated non-significant differences when the ratings of the Javanese were compared to Dayaknese and Madurese, combined (\emph{F} {[}1, 54{]} = 1.008, \emph{p} = .320) and when the ratings of the Dayaknese and Madurese were compared (\emph{F} {[}1, 54{]} = 0.349, \emph{p} = .557). This moderating effect of ethnicity of the rater on the negative reaction to the photo stimulus is illustrated in Figure 1.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Photo"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Rater"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Photo Stimulus"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-301-1.pdf}

\hypertarget{option-3-post-hoc-comparisons}{%
\subsubsection{Option \#3 post hoc comparisons}\label{option-3-post-hoc-comparisons}}

Another option is compare all possible cells. These are termed \emph{post hoc comparisons.} They are an alternative to simple main effects; you would not report both. The figure shows our place on the Two-Way ANOVA Workflow.

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlw_IntPH.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

As the numbers of levels increase, post hoc comparisons become somewhat unwieldly. Even though this procedure produces them all, you can select which sensible number you want to compare and control for Type I error according to the number in that set.

With rater ethnicity (3 levels) and photo stimulus (2 levels), we have 6 groupings. When \emph{k} is the number of groups, the total number of paired comparisons is: k(k-1)*2

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{6} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{6} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15
\end{verbatim}

Obtain the 15 post-hoc paired comparisons with the \emph{TukeyHSD()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{posthocs }\OtherTok{\textless{}{-}} \FunctionTok{TukeyHSD}\NormalTok{(TwoWay\_neg, }\AttributeTok{ordered =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{posthocs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level
    factor levels have been ordered

Fit: aov(formula = Negative ~ Rater * Photo, data = Ramdhani_df)

$Rater
                        diff         lwr       upr     p adj
Javanese-Dayaknese 0.5147954  0.02750358 1.0020872 0.0358235
Madurese-Dayaknese 0.8068425  0.32566283 1.2880222 0.0003629
Madurese-Javanese  0.2920471 -0.18222911 0.7663234 0.3124227

$Photo
                       diff       lwr      upr     p adj
Madurese-Dayaknese 0.726071 0.3987575 1.053385 0.0000262

$`Rater:Photo`
                                              diff         lwr       upr
Javanese:Dayaknese-Dayaknese:Dayaknese 0.706013072 -0.14743916 1.5594653
Dayaknese:Madurese-Dayaknese:Dayaknese 1.311568627  0.45811640 2.1650209
Madurese:Madurese-Dayaknese:Dayaknese  1.479735294  0.64726775 2.3122028
Madurese:Dayaknese-Dayaknese:Dayaknese 1.483077399  0.64060458 2.3255502
Javanese:Madurese-Dayaknese:Dayaknese  1.647182663  0.80470985 2.4896555
Dayaknese:Madurese-Javanese:Dayaknese  0.605555556 -0.23561614 1.4467273
Madurese:Madurese-Javanese:Dayaknese   0.773722222 -0.04615053 1.5935950
Madurese:Dayaknese-Javanese:Dayaknese  0.777064327 -0.05296553 1.6070942
Javanese:Madurese-Javanese:Dayaknese   0.941169591  0.11113973 1.7711995
Madurese:Madurese-Dayaknese:Madurese   0.168166667 -0.65170609 0.9880394
Madurese:Dayaknese-Dayaknese:Madurese  0.171508772 -0.65852109 1.0015386
Javanese:Madurese-Dayaknese:Madurese   0.335614035 -0.49441582 1.1656439
Madurese:Dayaknese-Madurese:Madurese   0.003342105 -0.80509532 0.8117795
Javanese:Madurese-Madurese:Madurese    0.167447368 -0.64099006 0.9758848
Javanese:Madurese-Madurese:Dayaknese   0.164105263 -0.65463115 0.9828417
                                           p adj
Javanese:Dayaknese-Dayaknese:Dayaknese 0.1652148
Dayaknese:Madurese-Dayaknese:Dayaknese 0.0002907
Madurese:Madurese-Dayaknese:Dayaknese  0.0000171
Madurese:Dayaknese-Dayaknese:Dayaknese 0.0000211
Javanese:Madurese-Dayaknese:Dayaknese  0.0000018
Dayaknese:Madurese-Javanese:Dayaknese  0.3005963
Madurese:Madurese-Javanese:Dayaknese   0.0760131
Madurese:Dayaknese-Javanese:Dayaknese  0.0802217
Javanese:Madurese-Javanese:Dayaknese   0.0166363
Madurese:Madurese-Dayaknese:Madurese   0.9911395
Madurese:Dayaknese-Dayaknese:Madurese  0.9908344
Javanese:Madurese-Dayaknese:Madurese   0.8482970
Madurese:Dayaknese-Madurese:Madurese   1.0000000
Javanese:Madurese-Madurese:Madurese    0.9907331
Javanese:Madurese-Madurese:Dayaknese   0.9920328
\end{verbatim}

If we want to consider all 15 pairwise comparisons and also control for Type I error, a Holm's sequential Bonerroni \citep{green_using_2014} will help us take a middle-of-the-road approach (not as strict as .05/15 with the traditional Bonferroni; not as lenient as ``none'') to managing Type I error.

With the Holms, we rank order the \emph{p} values associated with the 15 comparisons in order from lowest (e.g., .0000018) to highest (e.g., 1.000). The first \emph{p} value is evaluated with the most strict criterion (.05/15; the traditional Bonferonni approach). Then, each successive comparison calculates the \emph{p} value by using the number of \emph{remaining} comparisons as the denominator (e.g., .05/14, .05/13, .05/12). As the \emph{p} values rise and the alpha levels relax, there will be a cut-point where remaining comparisons are not statistically significant.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{15}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003333333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{14}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.003571429
\end{verbatim}

To facilitate this contrast, let's extract the 15 TukeyHSD tests and work with them in Excel.

First, obtain the structure of the \emph{posthoc} object

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(posthocs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
List of 3
 $ Rater      : num [1:3, 1:4] 0.5148 0.8068 0.292 0.0275 0.3257 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:3] "Javanese-Dayaknese" "Madurese-Dayaknese" "Madurese-Javanese"
  .. ..$ : chr [1:4] "diff" "lwr" "upr" "p adj"
 $ Photo      : num [1, 1:4] 0.726071 0.3987575 1.0533845 0.0000262
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr "Madurese-Dayaknese"
  .. ..$ : chr [1:4] "diff" "lwr" "upr" "p adj"
 $ Rater:Photo: num [1:15, 1:4] 0.706 1.312 1.48 1.483 1.647 ...
  ..- attr(*, "dimnames")=List of 2
  .. ..$ : chr [1:15] "Javanese:Dayaknese-Dayaknese:Dayaknese" "Dayaknese:Madurese-Dayaknese:Dayaknese" "Madurese:Madurese-Dayaknese:Dayaknese" "Madurese:Dayaknese-Dayaknese:Dayaknese" ...
  .. ..$ : chr [1:4] "diff" "lwr" "upr" "p adj"
 - attr(*, "class")= chr [1:2] "TukeyHSD" "multicomp"
 - attr(*, "orig.call")= language aov(formula = Negative ~ Rater * Photo, data = Ramdhani_df)
 - attr(*, "conf.level")= num 0.95
 - attr(*, "ordered")= logi TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{write.csv}\NormalTok{(posthocs}\SpecialCharTok{$}\StringTok{"Rater:Photo"}\NormalTok{, }\StringTok{"posthocsOUT.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In Excel, I would sort my results by their \emph{p} values (low to high) and consider my threshold (\emph{p} \textless{} .0033) to determine which effects were statistically significant. Using the strictest criteria of \emph{p} \textless{} .0033, we would have four statistically significant values.

\begin{figure}
\centering
\includegraphics{images/factorial/Holmsequential.jpg}
\caption{Image of the results of the Holms sequential Bonferroni.}
\end{figure}

I would ask, ``Is this what we want?'' Similar to the simple main effects we just tested, I am interested in two sets of comparisons:

First, how are the two sets of photos (Madurese and Dayaknese) rated within each set of raters.

\begin{itemize}
\tightlist
\item
  Javanese:Madurese - Javanese:Dayaknese
\item
  Dayaknese:Madurese - Dayaknese:Dayaknese
\item
  Madurese:Madurese - Madurese:Dayaknese
\end{itemize}

Second, focused on each photo, what are the relative ratings.

\begin{itemize}
\tightlist
\item
  Javanese:Madurese - Dayaknese:Madurese
\item
  Madurese: Madurese - Dayaknese:Madurese
\item
  Javanese:Dayaknese - Dayaknese:Dayaknese
\item
  Madurese: Dayaknese - Dayaknese:Dayaknese
\end{itemize}

This is only seven sets of comparisons and would considerably reduce the alpha:

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{0.05}\SpecialCharTok{/}\DecValTok{7}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007142857
\end{verbatim}

Below I have greyed-out the comparisons that are less interesting to me and left the seven that are my focal interest. I have highlighted in green the two comparisons that are statistically significant based on the Holms' sequential criteria. In this case, it does not make any difference in our interpretation of these focal predictors.

\begin{figure}
\centering
\includegraphics{images/factorial/HolmsSelect.jpg}
\caption{Image of the results of the Holms sequential Bonferroni.}
\end{figure}

\hypertarget{option-4-polynomial-trends}{%
\subsubsection{Option \#4 polynomial trends}\label{option-4-polynomial-trends}}

In the context of the significant interaction effect, we might also be interested in polynomial trends for any simple main effects where 3 or more cells are compared.

Why? If there are only two cells being compared, then the significance of that has already been tested and if significant, it is also a significant linear effect (because the shape between any two points is a line). Below is a figure of where the polynomial test of an interaction effect may fall in the process.

At the outset, let me acknowledge that this is not the best example to demonstrate a polynomial trend. Why? We do not necessarily have an ordered prediction across categories for this vignette. Other research scenarios (e.g., when dosage is none, low, high) are more readily suited for this approach.

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlw_Poly.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

In our example, Rater has three groups. Thus, we could evaluate a polynomial for the simple main effect of ethnicity of the rater within photo stimulus (separately for the photos of the Dayaknese and Madurese). We conduct these separately for Dayaknese, Madurese, and Javanese groups.

In the event that more than one polynomial trend select the higher one. For example, if both linear and quadratic are selected, interpret the quadratic trend

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Dayaknese\_Ph}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}} \FunctionTok{contr.poly}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{poly\_Dy }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Dayaknese\_Ph)}
\FunctionTok{summary.lm}\NormalTok{(poly\_Dy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Negative ~ Rater, data = Dayaknese_Ph)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8948 -0.5463 -0.1098  0.5155  2.1402 

Coefficients:
            Estimate Std. Error t value             Pr(>|t|)    
(Intercept)  2.54746    0.11744  21.693 < 0.0000000000000002 ***
Rater.L      1.04869    0.20351   5.153           0.00000419 ***
Rater.Q      0.02901    0.20330   0.143                0.887    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.8621 on 51 degrees of freedom
Multiple R-squared:  0.3432,    Adjusted R-squared:  0.3174 
F-statistic: 13.32 on 2 and 51 DF,  p-value: 0.00002211
\end{verbatim}

Results of a polynomial trend analysis indicated a statistically significant linear trend for evaluation of the Dayaknese photos across the three raters \emph{t} (51) = 5.153, \emph{p} \textless{} .001.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Madurese\_Ph}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}} \FunctionTok{contr.poly}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{poly\_Md }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Madurese\_Ph)}
\FunctionTok{summary.lm}\NormalTok{(poly\_Md)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Negative ~ Rater, data = Madurese_Ph)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.08650 -0.54395  0.01367  0.35905  2.34350 

Coefficients:
            Estimate Std. Error t value            Pr(>|t|)    
(Intercept)   3.2973     0.1161  28.391 <0.0000000000000002 ***
Rater.L       0.1189     0.2012   0.591               0.557    
Rater.Q      -0.2054     0.2011  -1.021               0.312    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.876 on 54 degrees of freedom
Multiple R-squared:  0.02451,   Adjusted R-squared:  -0.01162 
F-statistic: 0.6785 on 2 and 54 DF,  p-value: 0.5116
\end{verbatim}

Results of a polynomial trend analysis were non-significant when ethnicity of the rater was evaluated when rating Madurese photos.

\hypertarget{investigating-main-effects}{%
\section{Investigating Main Effects}\label{investigating-main-effects}}

We now focus on the possibility that there might be significant main effects, but a non-significant interaction effect. We only interpret main effects when there is a non-significant interaction effect. Why? Because in the presence of a significant interaction effect, the main effect will not tell a complete story. \emph{(And, if we didn't specify a correct model, we still might have an incomplete story. But that's another issue.)} Here's where we are on the workflow.

\begin{figure}
\centering
\includegraphics{images/factorial/WayWrkFlw_Main.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

Recall that main effects are the \emph{marginal means} -- that is the effects of Factor A \emph{collapsed across} all the levels of Factor B.

If the main effect has only two levels (e.g., the ratings of the Dayaknese and Madurese photos):

\begin{itemize}
\tightlist
\item
  the comparison was already ignoring/including all levels of the rater ethnicity factor (Dayaknese, Madurese, Javanese),
\item
  it was only a comparison of two cells (Dayaknese rater, Madurese rater), therefore
\item
  there is no need for further follow-up.
\end{itemize}

If the main effect has three or more levels (e.g,. ethnicity of rater with Dayaknese, Madurese, Javanese levels), then you would follow-up with one or more of the myriad of options. In this class we have focused on three:

\begin{itemize}
\tightlist
\item
  planned contrasts
\item
  posthoc comparisons (all possible cells)
\item
  polynomial
\end{itemize}

I will demonstrate how to do each as follow-up to a \emph{pretend} scenario where a main effect (but not an interaction) had been significant. I will write up the portion that would be inserted in an APA style results section.

Essentially, we treat these main effect analyses as the follow-up to a significant one-way ANOVA evaluating, in our case, the ethnicity of the Rater.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RaterMain }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df)  }\CommentTok{\#DV \textasciitilde{} IV I say, \textquotesingle{}DV by IV\textquotesingle{}}
\FunctionTok{model.tables}\NormalTok{(RaterMain)  }\CommentTok{\#ANOVA output}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Tables of effects

 Rater 
    Dayaknese Javanese Madurese
      -0.4551  0.05971   0.3518
rep   35.0000 37.00000  39.0000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(RaterMain)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             Df Sum Sq Mean Sq F value  Pr(>F)   
Rater         2  12.21   6.103   6.426 0.00231 **
Residuals   108 102.57   0.950                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(RaterMain)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         eta.sq eta.sq.part
Rater 0.1063485   0.1063485
\end{verbatim}

A boxplot representing this main effect may help convey how the main effect of Rater (collapsed across Photo) is different than an interaction effect.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-311-1.pdf}

\hypertarget{follow-up-with-all-post-hocs}{%
\subsection{Follow-up with all Post-Hocs}\label{follow-up-with-all-post-hocs}}

An easy possibility is to follow-up with all possible post-hocs. In the main effect case, these are far simpler than where we conducted all possile posthocs for the interaction effect (remember the Holms sequential Bonferroni?).

Here is a reminder of our location on the workflow.

\begin{figure}
\centering
\includegraphics{images/factorial/wfMain_PH.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

The \emph{TukeyHSD()} function produces posthoc comparisons by providing the mean difference, a 95\% confidence interval of those differences, and the associated \emph{p} value.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{TukeyHSD}\NormalTok{(RaterMain, }\AttributeTok{ordered =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Tukey multiple comparisons of means
    95% family-wise confidence level
    factor levels have been ordered

Fit: aov(formula = Negative ~ Rater, data = Ramdhani_df)

$Rater
                        diff         lwr       upr     p adj
Javanese-Dayaknese 0.5147954 -0.03128503 1.0608758 0.0690316
Madurese-Dayaknese 0.8068425  0.26761161 1.3460734 0.0016135
Madurese-Javanese  0.2920471 -0.23944747 0.8235417 0.3950430
\end{verbatim}

Results suggest there were statistically significant differences (\emph{p} \textless{} .05) between the Madurese and Dayaknese. These differences, though, would have been when rating \emph{all} photos. This analysis disregards the ethnicity portrayed in the photo.

\hypertarget{follow-up-with-planned-contrasts}{%
\subsection{Follow-up with planned contrasts}\label{follow-up-with-planned-contrasts}}

We generally try for \emph{orthogonal} contrasts so that the partitioning of variance is independent (clean, not overlapping). Planned contrasts are a great way to do this. Here's where we are in the workflow.

\begin{figure}
\centering
\includegraphics{images/factorial/wfMain_Plnd.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

If you aren't extremely careful about your order-of-operations in R, it can confuse objects, so I have named these contrasts \emph{main\_c1} and \emph{main\_c2} to remind myself that they refer to the main effect of ethnicity of the rater.

In this hypothetical scenario (remember we are pretending we are in the circumstance of a non-significant interaction effect but a significant main effect), I am:

\begin{itemize}
\tightlist
\item
  Constrast \#1: comparing the DV for the Javanese rater to the combined Dayaknese and Madurese raters.
\item
  Contrast \#2: comparing the DV for the Dayaknese and Madurese raters.
\end{itemize}

These are orthogonal because:

\begin{itemize}
\tightlist
\item
  there are \emph{k} - 1 comparisons, and
\item
  once a contrast is isolated (i.e., the Javanese rater in contrast \#1) it cannot be used again

  \begin{itemize}
  \tightlist
  \item
    Recall the piece of cake analogy: once you take out a piece of the cake, you really can't put it back in
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Contrast1 compares Control against the combined effects of Low and}
\CommentTok{\# High.}
\NormalTok{main\_c1 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\CommentTok{\# Contrast2 excludes Control; compares Low to High.}
\NormalTok{main\_c2 }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(main\_c1, main\_c2)}
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          main_c1 main_c2
Dayaknese       1      -1
Javanese       -2       0
Madurese        1       1
\end{verbatim}

Then we run the contrast

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mainPlanned }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df)}
\FunctionTok{summary.lm}\NormalTok{(mainPlanned)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Negative ~ Rater, data = Ramdhani_df)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.08813 -0.74921  0.05792  0.71482  2.34187 

Coefficients:
             Estimate Std. Error t value             Pr(>|t|)    
(Intercept)   2.93283    0.09259  31.676 < 0.0000000000000002 ***
Ratermain_c1 -0.03712    0.06544  -0.567             0.571670    
Ratermain_c2  0.40342    0.11345   3.556             0.000561 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9745 on 108 degrees of freedom
Multiple R-squared:  0.1063,    Adjusted R-squared:  0.0898 
F-statistic: 6.426 on 2 and 108 DF,  p-value: 0.002307
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

These planned contrasts show that when the Javanese raters are compared to the combined Dayaknese and Madurese raters, there was a non significant difference, \emph{t}(108) = -0.567, \emph{p} = .572. However, there were significant differences between Dayaknese and Javanese raters, \emph{t}(108) = 3.556, \emph{p} \textless{} .001.

\hypertarget{polynomial-trends}{%
\subsection{Polynomial Trends}\label{polynomial-trends}}

Polynomial contrasts let us see if there is a linear (or curvilinear) pattern to the data. To detect a trend, the data must be coded in an ascending order\ldots and it needs to be a sensible comparison. Here's where this would fall in our workflow.

\includegraphics{images/factorial/wfMain_Poly.jpg}
Because these three ethnic groups are not \emph{ordered} in the same way as would an experiment involving dosage (e.g,. placebo, lo dose, hi dose), evaluation of the polynomial trend is not really justified (even though it is statistically possible). None-the-less, I will demonstrate how it is conducted.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{contrasts}\NormalTok{(Ramdhani\_df}\SpecialCharTok{$}\NormalTok{Rater) }\OtherTok{\textless{}{-}} \FunctionTok{contr.poly}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{mainTrend }\OtherTok{\textless{}{-}} \FunctionTok{aov}\NormalTok{(Negative }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Rater, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df)}
\FunctionTok{summary.lm}\NormalTok{(mainTrend)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
aov(formula = Negative ~ Rater, data = Ramdhani_df)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.08813 -0.74921  0.05792  0.71482  2.34187 

Coefficients:
            Estimate Std. Error t value             Pr(>|t|)    
(Intercept)  2.93283    0.09259  31.676 < 0.0000000000000002 ***
Rater.L      0.57052    0.16045   3.556             0.000561 ***
Rater.Q     -0.09094    0.16029  -0.567             0.571670    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9745 on 108 degrees of freedom
Multiple R-squared:  0.1063,    Adjusted R-squared:  0.0898 
F-statistic: 6.426 on 2 and 108 DF,  p-value: 0.002307
\end{verbatim}

\textbf{Rater.L} tests the data to see if there is a significant linear trend. There is: \emph{t} = 3.556, \emph{p} \textless{} .001.

\textbf{Rater.Q} tests to see if there is a significant quadratic (curvilinear, one hump) trend. There is not: \emph{t} = -0.567, \emph{p} = .572.

Results supported a significant linear trend (\emph{t} = 3.556, \emph{p} \textless{} .001) such that negative reactions increased in a linear reaction across the three rating groups.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{,}
    \AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-316-1.pdf}

\hypertarget{my-apa-style-results-section}{%
\section{My APA Style Results Section}\label{my-apa-style-results-section}}

First, I am reluctant to term anything ``final.'' It seems like there is always the possibility or revision. Given that I demonstrated a number of options, let me first show the workflow with the particular path I took:

\begin{figure}
\centering
\includegraphics{images/factorial/WrkFlw_mypath.jpg}
\caption{Image our place in the Two-Way ANOVA Workflow.}
\end{figure}

In light of that, here's the final write-up:

\textbf{Results}

\begin{quote}
A 3 X 2 ANOVA was conducted to evaluate the effects of rater ethnicity (3 levels, Dayaknese, Madurese, Javanese) and photo stimulus (2 levels, Dayaknese on Madurese,) on negative reactions to the photo stimuli. Results of Levene's test for equality of error variances indicated violation of the assumption, (\(F\){[}5, 105{]} = 8.834, \(p\) \textless{} .001). Our analysis of the individual cell means (see Table 1 for means and standard deviations) suggested skew and kurtosis were within the bounds considered to be normally distributed \citep{kline_principles_2016}. A non-significant Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) provided further evidence that the assumption of normality was not violated (\(W\) = 0.9846, \(p\) = 0.234).
\end{quote}

\begin{quote}
Computing sums of squares with a Type II approach, the results for the ANOVA indicated a significant main effect for ethnicity of the rater (\emph{F}{[}2, 105{]} = 8.077, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.107), a significant main effect for photo stimulus, (\emph{F}{[}1, 105{]} = 19.346, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.127), and a significant interaction effect (\emph{F}{[}2, 105{]} = 5.696, \emph{p} = .004, \(\eta ^{2}\) = 0.075).
\end{quote}

\begin{quote}
To explore the interaction effect, we followed with a test of the simple main effect of photo stimulus within the ethnicity of the rater. That is, we looked at the effect of the photo stimulus within the Dayaknese, Madurese, and Javanese groups, separately. To control for Type I error across the three simple main effects, we set alpha at .017 (.05/3). Results indicated significant differences for Dayaknese (\emph{F} {[}1, 33{]} = 50.4, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.60.) and Javanese ethnic groups (\emph{F} {[}1, 35{]}= 17.18, \emph{p} \textless{} .001, \(\eta ^{2}\) = 0.33), but not for the Madurese ethnic group (\emph{F} {[}1, 37{]} = 0.000, \emph{p} = .993, \(\eta ^{2}\) \textless{} .001). As illustrated in Figure 1, the Dayaknese and Javanese rathers both reported stronger negative reactions to the Madurese. The differences in ratings for the Madurese were not statistically significantly different. In this way, the rater's ethnic group moderated the relationship between the photo stimulus and negative reactions.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Ramdhani\_df, }\AttributeTok{x =} \StringTok{"Rater"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Negative"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Photo"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Ethnicity of Rater"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Negative Reaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-317-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.2way.table}\NormalTok{(}\AttributeTok{iv1 =}\NormalTok{ Rater, }\AttributeTok{iv2 =}\NormalTok{ Photo, }\AttributeTok{dv =}\NormalTok{ Negative, }\AttributeTok{data =}\NormalTok{ Ramdhani\_df,}
    \AttributeTok{landscape =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{, }\AttributeTok{filename =} \StringTok{"Table\_1\_MeansSDs.doc"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means and standard deviations for Negative as a function of a 3(Rater) X 2(Photo) design 

               Photo                   
           Dayaknese      Madurese     
     Rater         M   SD        M   SD
 Dayaknese      1.82 0.77     3.13 0.16
  Javanese      2.52 0.74     3.46 0.64
  Madurese      3.30 1.03     3.30 1.33

Note. M and SD represent mean and standard deviation, respectively. 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.aov.table}\NormalTok{(TwoWay\_neg, }\AttributeTok{filename =} \StringTok{"Table\_2\_effects.doc"}\NormalTok{,}
    \AttributeTok{table.number =} \DecValTok{2}\NormalTok{, }\AttributeTok{type =} \StringTok{"II"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 2 

ANOVA results using Negative as the dependent variable
 

     Predictor    SS  df    MS     F    p partial_eta2 CI_90_partial_eta2
         Rater 12.24   2  6.12  8.10 .001          .13                   
         Photo 14.62   1 14.62 19.35 .000          .16         [.06, .26]
 Rater x Photo  8.61   2  4.30  5.70 .004          .10         [.02, .18]
         Error 79.34 105  0.76                                           

Note: Values in square brackets indicate the bounds of the 90% confidence interval for partial eta-squared 
\end{verbatim}

\hypertarget{comparing-our-results-to-rhamdani-et-al.--ramdhani_affective_2018}{%
\subsection{\texorpdfstring{Comparing Our Results to Rhamdani et al. \citeyearpar{ramdhani_affective_2018}}{Comparing Our Results to Rhamdani et al. {[}-@ramdhani\_affective\_2018{]}}}\label{comparing-our-results-to-rhamdani-et-al.--ramdhani_affective_2018}}

As is common in simulations, our results approximate the findings reported in the manuscript, but does not replicate them exactly. Our main and interaction effects map on very closely. However, in the follow-up tests, while our findings that Dayaknese rated the Madurese photos more negatively, the findings related to the Javanese' and Madurese' ratings wiggled around some. A close look at the figures can explain that with varying variability and means with similar values, this is probable. I find it to be a useful lesson in ``what it takes'' to get stable, meaningful results.

\hypertarget{options-for-assumption-violations}{%
\section{Options for Assumption Violations}\label{options-for-assumption-violations}}

In one-way ANOVA we could simply apply the Welch's alternative. It's not so easy in factorial ANOVA. One alternative, though, is to change the sums of squares type used in the ANOVA calculations.

In ANOVA models sums of squares can be calculated four different ways: Type I, II, III, and IV. This matters.

SS Type II is the \emph{aov()} default. It may be a best practice to go ahead and specify the SS Type in both the \emph{aov()}, eta-squared, and apaTables script so that they are consistent.

\textbf{Type I} sums of squares is similar to hierarchical linear regression in that the first predictor in the model claims as much variance as it can and the leftovers are claimed by the variable entered next -- each claiming as much as possible leaving the leftovers for what follows. Unless the variables are completely independent of each other (unlikely), Type I sums of squares cannot evaluate the true main effect of each variable. Type I should not be used to evaluate main effects and interactions because the order of predictors will affect the results.

\textbf{Type II} (the R default) is appropriate if you are interested main effects because it ignores the effect of any interactions involving the main effect. Thus, variance from a main effect is not ``lost'' to any interaction terms containing that effect. Type II is appropriate for main effects analyses only, but should not be used when evaluating interaction effects. Type II sums of squares is not affected by the type of contrast coding used to specify the predictor variables.

\textbf{Type III} is the default in many stats packages -- but not R. In Type III all effects (main effects and interactions) are evaluated (simultaneously) taking into consideration all other effects in the model (not just the ones entered before). Type III is more robust to unequal samples sizes (e.g., unbalanced designs). Type III is best when predictors are encoded with orthogonal contrasts.

*\textbf{Type IV} is identical to Type III except it requires no missing cells.

Field \citeyearpar{field_discovering_2012} suggested that it is safest to stick with Type III sums of squares. We apply the type to the model we create in the initial run. For more information, check out this explanation on \href{https://www.r-bloggers.com/2011/03/anova-\%E2\%80\%93-type-iiiiii-ss-explained/}{r-bloggers}.

Many researchers automatically use Type III as the SS type. Today I went with the R default because

\begin{itemize}
\tightlist
\item
  Type II sums of squares was used in hand-calculations,
\item
  Our example was reasonably balanced (equal cell sizes), and
\item
  We had only violated the homogeneity of variance assumption.
\end{itemize}

For demonstration purposes, let's run the Type III alternative to see the differences:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# this is what we did}
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{Anova}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: Negative
            Sum Sq  Df F value     Pr(>F)    
Rater       12.238   2  8.0977  0.0005363 ***
Photo       14.619   1 19.3462 0.00002623 ***
Rater:Photo  8.609   2  5.6964  0.0044803 ** 
Residuals   79.341 105                       
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We change the SS type by applying it to our model.}
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{Anova}\NormalTok{(TwoWay\_neg, }\AttributeTok{type =} \StringTok{"III"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type III tests)

Response: Negative
            Sum Sq  Df F value              Pr(>F)    
(Intercept) 56.173   1 74.3388 0.00000000000007422 ***
Rater       19.805   2 13.1051 0.00000830116650983 ***
Photo       15.040   1 19.9034 0.00002051829053731 ***
Rater:Photo  8.609   2  5.6964             0.00448 ** 
Residuals   79.341 105                                
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Note that the sums of squares are somewhat different between models -- and that the Type III tests includes an intercept. In today's example, the statistical significance remains the same across the models.

Now let's compare the effect sizes across models.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(TwoWay\_neg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                eta.sq eta.sq.part
Rater       0.10662441  0.13363091
Photo       0.12736755  0.15558329
Rater:Photo 0.07500609  0.09788289
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(TwoWay\_neg, }\AttributeTok{type =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                eta.sq eta.sq.part
Rater       0.17255745  0.19975734
Photo       0.13103575  0.15935009
Rater:Photo 0.07500609  0.09788289
\end{verbatim}

The Type III effect size for Rater is higher; the others are quite similar.

\hypertarget{power}{%
\section{Power}\label{power}}

Asking about \emph{power} can be a euphemistic way of asking, ``How large should my sample size be?''

Power is defined as the ability of the test to detect statistical significance when there is such. It's represented formulaically as (1 - \emph{P})(Type II error). Power is traditionally set at 80\% (or .8)

We will do both -- evaluate the power of our current example and then work backwards to estimate the sample size needed (which is our usual question for MRPs and dissertations).

We'll use the \emph{pwr.2way()} function from the \emph{pwr2} package.
Helpful resources are found here:

\begin{itemize}
\tightlist
\item
  \url{https://cran.r-project.org/web/packages/pwr2/pwr2.pdf}
\item
  \url{https://rdrr.io/cran/pwr2/man/ss.2way.html}
\end{itemize}

The \emph{pwr.2way()} and \emph{ss.2way()} functions require the following:

\begin{itemize}
\tightlist
\item
  \textbf{a} number of groups in Factor A
\item
  \textbf{b} number of groups in Factor B
\item
  \textbf{alpha} significant level (Type I error probaility)
\item
  \textbf{beta} Type II error probability (Power = 1 - beta; traditionally set at .1 or .2)
\item
  \textbf{f.A} the \emph{f} effect size of Factor A
\item
  \textbf{f.B} the \emph{f} effect size of Factor B
\item
  \textbf{B} Iteration times, default is 100
\end{itemize}

Hints for calculating the \emph{f.A} and \emph{f.B} values:

\begin{itemize}
\tightlist
\item
  In this case, we will rerun the statistic, grab both effect sizes, and convert them to the \emph{f} (not the \(f^2\))

  \begin{itemize}
  \tightlist
  \item
    calculation can be straightforward, either use an online calculator, a hand-calculated formula, or the \emph{eta2\_to\_f} function from the \emph{effectsize}
  \end{itemize}
\item
  When an effect size is unknown, you can substitute what you expect using Cohen's guidelines of .10, .25, and .40 as small, medium, and large (for the \emph{f}, not \(F^2\))
\end{itemize}

Let's quickly rerun our model to get both the df and calculate the \emph{f} effect value

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lsr}\SpecialCharTok{::}\FunctionTok{etaSquared}\NormalTok{(TwoWay\_neg, }\AttributeTok{anova =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                eta.sq eta.sq.part        SS  df         MS         F
Rater       0.10662441  0.13363091 12.237770   2  6.1188848  8.097730
Photo       0.12736755  0.15558329 14.618555   1 14.6185546 19.346190
Rater:Photo 0.07500609  0.09788289  8.608791   2  4.3043955  5.696435
Residuals   0.69127788          NA 79.341113 105  0.7556296        NA
                        p
Rater       0.00053629628
Photo       0.00002622821
Rater:Photo 0.00448026007
Residuals              NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# get the partial eta{-}square (second number) and dfs}
\end{Highlighting}
\end{Shaded}

If we want to understand power in our analysis, we need to convert our effect size for the \emph{interaction} to \(f\) effect size (this is not the same as the \emph{F} test). The \emph{effectsize} package has a series of converters. We can use the \emph{eta2\_to\_f()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effectsize}\SpecialCharTok{::}\FunctionTok{eta2\_to\_f}\NormalTok{(}\FloatTok{0.1066}\NormalTok{)  }\CommentTok{\#FactorA {-}{-} Rater}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3454265
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effectsize}\SpecialCharTok{::}\FunctionTok{eta2\_to\_f}\NormalTok{(}\FloatTok{0.1274}\NormalTok{)  }\CommentTok{\#Factor B {-}{-} Photo}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3821001
\end{verbatim}

\hypertarget{post-hoc-power-analysis}{%
\subsection{Post Hoc Power Analysis}\label{post-hoc-power-analysis}}

Now we calculate power for our existing model. We'll use the package \emph{pwr2} and the function \emph{pwr.2way()}. To specify this we identify:

\begin{itemize}
\tightlist
\item
  a: number of groups for Factor A (Rater)
\item
  b: number of groups for Factor B (Photo)
\item
  size.A: sample size per group in Factor A (because ours differ slightly, I divided the N by the number of groups)
\item
  size.B: sample size per group in Factor B (because ours differ slightly, I divided the N by the number of groups)
\item
  f.A: Effect size of Factor A
\item
  f.A.: Effect size of Factor B
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr2}\SpecialCharTok{::}\FunctionTok{pwr.2way}\NormalTok{(}\AttributeTok{a =} \DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{size.A =} \DecValTok{37}\NormalTok{, }\AttributeTok{size.B =} \DecValTok{55}\NormalTok{, }\AttributeTok{f.A =} \FloatTok{0.345}\NormalTok{,}
    \AttributeTok{f.B =} \FloatTok{0.382}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced two-way analysis of variance power calculation 

              a = 3
              b = 2
            n.A = 37
            n.B = 55
      sig.level = 0.05
        power.A = 0.9974259
        power.B = 0.9999996
          power = 0.9974259

NOTE: power is the minimum power among two factors
\end{verbatim}

At 0.997 (Rater), 0.999 (Photo), and 0.997 (interaction), our power to detect a significant effect for Factor A/Rater and Factor B/Photo was huge!

\hypertarget{estimating-sample-size-requirements}{%
\subsection{Estimating Sample Size Requirements}\label{estimating-sample-size-requirements}}

If we want to replicate this study we could use its results to estimate what would be needed for the replication.

In this specification:

\begin{itemize}
\tightlist
\item
  a: number of groups for Factor A (Rater)
\item
  b: number of groups for Factor B (Photo)
\item
  alpha: significance level (Type I error probability); usually .05
\item
  beta: Type II error probability (Power = 1-beta); usually .80
\item
  f.A: Effect size (\emph{f}) of Factor A (this time we know; other times we can guess from previously published literature)
\item
  f.A.: Effect size (\emph{f}) of Factor B
\item
  B: iteration times, default number is 100
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr2}\SpecialCharTok{::}\FunctionTok{ss.2way}\NormalTok{(}\AttributeTok{a =} \DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{beta =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{f.A =} \FloatTok{0.345}\NormalTok{, }\AttributeTok{f.B =} \FloatTok{0.382}\NormalTok{,}
    \AttributeTok{B =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced two-way analysis of variance sample size adjustment 

              a = 3
              b = 2
      sig.level = 0.05
          power = 0.2
              n = 3

NOTE: n is number in each group, total sample = 18
\end{verbatim}

Curiously, that's just about the number that was in each of the six cells!

Often times researchers will play around with the \emph{f} values. Remember Cohen's indication of small (.10), medium (.25), and large (.40). Let's see what happens when we enter different values. Specifically, what if we only had a medium effect?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr2}\SpecialCharTok{::}\FunctionTok{ss.2way}\NormalTok{(}\AttributeTok{a =} \DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{beta =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{f.A =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{f.B =} \FloatTok{0.25}\NormalTok{,}
    \AttributeTok{B =} \DecValTok{100}\NormalTok{)  }\CommentTok{\#if we expected a medium effect}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced two-way analysis of variance sample size adjustment 

              a = 3
              b = 2
      sig.level = 0.05
          power = 0.2
              n = 6

NOTE: n is number in each group, total sample = 36
\end{verbatim}

And what would happen if we only had a small effect?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwr2}\SpecialCharTok{::}\FunctionTok{ss.2way}\NormalTok{(}\AttributeTok{a =} \DecValTok{3}\NormalTok{, }\AttributeTok{b =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{beta =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{f.A =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{f.B =} \FloatTok{0.1}\NormalTok{,}
    \AttributeTok{B =} \DecValTok{100}\NormalTok{)  }\CommentTok{\#if we expected a small effect}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Balanced two-way analysis of variance sample size adjustment 

              a = 3
              b = 2
      sig.level = 0.05
          power = 0.2
              n = 30

NOTE: n is number in each group, total sample = 180
\end{verbatim}

\hypertarget{practice-problems-5}{%
\section{Practice Problems}\label{practice-problems-5}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that offer differing levels of difficulty. Whichever you choose, you will focus on these larger steps in factorial-way ANOVA, including:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a two-way ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    report main and interaction effects
  \item
    conduct follow-up testing of simple main effects
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-1}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-1}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

\begin{itemize}
\tightlist
\item
  If two-way ANOVA is new to you, perhaps you just change the number in ``set.seed(210731)'' from 210731 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.
\item
  If you are interested in power, change the sample size to something larger or smaller.
\item
  If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.
\end{itemize}

\hypertarget{problem-2-conduct-a-factorial-anova-with-the-positive-evaluation-dependent-variable.}{%
\subsection{\texorpdfstring{Problem \#2: Conduct a factorial ANOVA with the \emph{positive evaluation} dependent variable.}{Problem \#2: Conduct a factorial ANOVA with the positive evaluation dependent variable.}}\label{problem-2-conduct-a-factorial-anova-with-the-positive-evaluation-dependent-variable.}}

The Ramdhani et al. \citeyearpar{ramdhani_affective_2018} article has two dependent variables (negative and positive evaluation). Each is suitable for two-way ANOVA. I used \emph{negative evaluation} as the dependent variable; you are welcome to conduct the analysis with \emph{positive evaluation} as the dependent variable.

\hypertarget{problem-3-try-something-entirely-new.-1}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-1}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a two-way, factorial ANOVA. Please have at least 3 levels for one predictor and at least 2 levels for the second predictor.

\hypertarget{grading-rubric-2}{%
\subsection{Grading Rubric}\label{grading-rubric-2}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the IV and DV & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct omnibus ANOVA (w effect size) & 5 & \_\_\_\_\_ \\
5. Conduct one set of follow-up tests; narrate your choice & 5 & \_\_\_\_\_ \\
6. Describe approach for managing Type I error & 5 & \_\_\_\_\_ \\
7. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
8 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{Repeated}{%
\chapter{One-Way Repeated Measures ANOVA}\label{Repeated}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=c8f5737f-d00d-4fa4-ba3c-ad8b01762258}{Screencasted Lecture Link}

In the prior lessons, a critical assumption is that the observations must be ``independent.'' That is, related people (partners, parent/child, manager/employee) cannot comprise the data and there cannot be multiple waves of data for the same person. Repeated measures ANOVA is created specifically for this \emph{dependent} purpose. This lessons focuses on the one-way repeated measures ANOVA, where we measure changes across time.

\hypertarget{navigating-this-lesson-7}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-7}}

There is just over one hour of lecture. If you work through the materials with me plan for an additional two hours

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-7}{%
\subsection{Learning Objectives}\label{learning-objectives-7}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Evaluate the suitability of a research design/question and dataset for conducting a one-way repeated measures ANOVA; identify alternatives if the data is not suitable.
\item
  Hand-calculate a one-way repeated measures ANOVAs

  \begin{itemize}
  \tightlist
  \item
    describing the partitioning of variance as it relates to model/residual; within/between.
  \end{itemize}
\item
  Test the assumptions for one-way repeated measures ANOVA.
\item
  Conduct a one-way repeated measures ANOVA (omnibus and follow-up) in R.
\item
  Interpret output from the one-way repeated measures ANOVA (and follow-up).
\item
  Prepare an APA style results section of the one-way repeated measures ANOVA output.
\item
  Demonstrate how an increased sample size increases the power of a statistical test.
\end{itemize}

\hypertarget{planning-for-practice-6}{%
\subsection{Planning for Practice}\label{planning-for-practice-6}}

The suggestions for homework are graded in complexity with more complete descriptions at the end of the chapter follow these suggestions.

\begin{itemize}
\tightlist
\item
  Rework the problem in the chapter by changing the random seed in the code that simulates the data. This should provide minor changes to the data, but the results will likely be very similar.
\item
  There were no additional variables in this example. However, you'll see we do have an issue with statistical power. Perhaps change the sample size to see if it changes (maybe stabilizes?) the results.
\item
  Conduct a one-way repeated measures ANOVA with data to which you have access. This could include data you simulate on your own or from a published article.
\end{itemize}

\hypertarget{readings-resources-6}{%
\subsection{Readings \& Resources}\label{readings-resources-6}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  \emph{Repeated Measures ANOVA in R: The Ultimate Guide}. (n.d.). Datanovia. Retrieved October 19, 2020, from \url{https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r}

  \begin{itemize}
  \tightlist
  \item
    This website is an excellent guide for both one-way repeated measures and mixed design ANOVA. A great resource for both the conceptual and procedural. This is the guide I have used for the basis of the lecture. Working through their example would be great additional practice.
  \end{itemize}
\item
  Green, S. B., \& Salkind, N. J. (2014). One-Way Repeated Measures Analysis of Variance (Lesson 29). In \emph{Using SPSS for Windows and Macintosh: Analyzing and understanding data} (Seventh edition., pp.~209--217). Pearson.

  \begin{itemize}
  \tightlist
  \item
    For years I taught from the Green and Salkind text. Even though it was written for SPSS, the authors do a terrific job of walking the reader through the one-way repeated measures logic and process.
  \end{itemize}
\item
  Amodeo, A. L., Picariello, S., Valerio, P., \& Scandurra, C. (2018). Empowering transgender youths: Promoting resilience through a group training program. \emph{Journal of Gay \& Lesbian Mental Health, 22}(1), 3--19.

  \begin{itemize}
  \tightlist
  \item
    This mixed methods (qualitative and quantitative) includes a one-way repeated measures example.
  \end{itemize}
\end{itemize}

\hypertarget{packages-5}{%
\subsection{Packages}\label{packages-5}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(knitr))\{install.packages(\textquotesingle{}knitr\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\} \#manipulate}
\CommentTok{\# data if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\} \#easy plots}
\CommentTok{\# if(!require(rstatix))\{install.packages(\textquotesingle{}rstatix\textquotesingle{})\} \#pipe{-}friendly R}
\CommentTok{\# functions if(!require(data.table))\{install.packages(\textquotesingle{}data.table\textquotesingle{})\}}
\CommentTok{\# \#pipe{-}friendly R functions}
\CommentTok{\# if(!require(reshape2))\{install.packages(\textquotesingle{}reshape2\textquotesingle{})\} \#pipe{-}friendly}
\CommentTok{\# R functions}
\CommentTok{\# if(!require(effectsize))\{install.packages(\textquotesingle{}effectsize\textquotesingle{})\} \#converts}
\CommentTok{\# effect sizes for use in power analysis}
\CommentTok{\# if(!require(WebPower))\{install.packages(\textquotesingle{}WebPower\textquotesingle{})\} \#power}
\CommentTok{\# analysis tools for repeated measures}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\} \#power analysis tools}
\CommentTok{\# for repeated measures}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-one-way-repeated-measures-anova}{%
\section{Introducing One-way Repeated Measures ANOVA}\label{introducing-one-way-repeated-measures-anova}}

There are a couple of typical use cases for one-way repeated measures ANOVA. In the first, the research participant is assessed in multiple conditions -- with no interested in change-over-time.

An example of a research design using this approach occurred in the Green and Salkind \citeyearpar{green_using_2014} statistics text, the one-way repeated measures vignette compared teachers' perception of stress when responding to parents, teachers, and school administrators.

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/repeated_conditions.jpg}
\caption{Illustration of a research design appropriate for one-way repeated measures ANOVA}
\end{figure}

Another common use case is about time. The classic design is a pre-test, an intervention, a post-test, and a follow up. In designs like these researchers often hope that there is a positive change from pre-to-post and that that change either stays constant (from post-to-follow-up) or, perhaps, increases even further. The research vignette for this lesson is interested in change-over-time.

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/repeated_design.jpg}
\caption{Illustration of a research design appropriate for one-way repeated measures ANOVA}
\end{figure}

\hypertarget{workflow-for-oneway-repeated-measures-anova}{%
\subsection{Workflow for Oneway Repeated Measures ANOVA}\label{workflow-for-oneway-repeated-measures-anova}}

The following is a proposed workflow for conducting a one-way repeated measures ANOVA.

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/wf_repeated.jpg}
\caption{Image of a workflow for the one-way repeated measures ANOVA}
\end{figure}

Steps involved in analyzing the data include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Preparing and importing the data.
\item
  Exploring the data

  \begin{itemize}
  \tightlist
  \item
    graphs
  \item
    descriptive statistics
  \end{itemize}
\item
  Checking distributional assumptions

  \begin{itemize}
  \tightlist
  \item
    assessing normality via skew, kurtosis, Shapiro Wilks
  \item
    checking or violation of the \emph{sphericity} assumption with Mauchly's test; if violated interpret the corrected output or use a multivariate approach for the analysis
  \end{itemize}
\item
  Computing the omnibus ANOVA
\item
  Computing post-hoc comparisons, planned contrasts, or polynomial trends
\item
  Managing Type I error
\item
  Sample size/power analysis (which you should think about first -- but in the context of teaching ANOVA, it's more pedagogically sensible, here)
\end{enumerate}

\hypertarget{research-vignette-6}{%
\section{Research Vignette}\label{research-vignette-6}}

Amodeo \citep{amodeo_empowering_2018} and colleagues conducted a mixed methods study (qualitative and quantitative) to evaluate the effectiveness of an empowerment, peer-group-based, intervention with participants (\emph{N} = 8) who experienced transphobic episodes. Focus groups used qualitative methods to summarize emergent themes from the program (identity affirmation, self-acceptance, group as support) and a one-way, repeated measures ANOVA provided evidence of increased resilience from pre to three-month followup.

Eight participants (seven transgender women and one genderqueer person) participated in the intervention. The mean age was 28.5 (\emph{SD} = 5.85). All participants were located in Italy.

The within-subjects condition was wave, represented by T1, T2, and T3:

\begin{itemize}
\tightlist
\item
  T1, beginning of training
\item
  Training, three 8-hour days,

  \begin{itemize}
  \tightlist
  \item
    content included identity and heterosexism, sociopolitical issues and minority stress, resilience and empowerment
  \end{itemize}
\item
  T2, at the conclusion of the 3-day training
\item
  Follow-up session 3 months later
\item
  T3, at the conclusion of the +3 month follow-up session
\end{itemize}

The dependent variable (assessed at each wave) was a 14-item resilience scale \citep{wagnild_development_1993}. Items were assessed on a 7-point scale ranging from \emph{strongly disagree} to \emph{strongly agree} with higher scores indicating higher levels of resilience. An example items was, ``I usually manage one way or another.''

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=10.41667in,height=1.04167in]{images/oneway_repeated/Amodio_design.jpg}
\caption{Diagram of the research design for the Amodeo et al study}\label{id}
}
\end{figure}

\hypertarget{code-for-simulating-the-data-used-today.}{%
\subsection{Code for simulating the data used today.}\label{code-for-simulating-the-data-used-today.}}

Below is the code I used to simulate data. The following code assumes 8 participants who each participated in 3 waves (pre, post, followup).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2022}\NormalTok{)}
\CommentTok{\# gives me 8 numbers, assigning each number 3 consecutive spots, in}
\CommentTok{\# sequence}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{), }\AttributeTok{each =} \DecValTok{3}\NormalTok{)))}
\CommentTok{\# gives me a column of 24 numbers with the specified Ms and SD}
\NormalTok{Resilience }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{24}\NormalTok{, }\AttributeTok{mean =} \FunctionTok{c}\NormalTok{(}\FloatTok{5.7}\NormalTok{, }\FloatTok{6.21}\NormalTok{, }\FloatTok{6.26}\NormalTok{), }\AttributeTok{sd =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.88}\NormalTok{, }\FloatTok{0.79}\NormalTok{, }\FloatTok{0.37}\NormalTok{))}
\CommentTok{\# repeats pre, post, follow{-}up once each, 8 times}
\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Pre"}\NormalTok{, }\StringTok{"Post"}\NormalTok{, }\StringTok{"FollowUp"}\NormalTok{), }\AttributeTok{each =} \DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{)}
\NormalTok{Amodeo\_long }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, Wave, Resilience)}
\end{Highlighting}
\end{Shaded}

Let's take a look at the structure of our variables. We want ID to be a factor, Resilience to be numeric, and Wave to be an ordered factor (Pre, Post, FollowUp).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   24 obs. of  3 variables:
 $ ID        : Factor w/ 8 levels "1","2","3","4",..: 1 1 1 2 2 2 3 3 3 4 ...
 $ Wave      : chr  "Pre" "Post" "FollowUp" "Pre" ...
 $ Resilience: num  6.49 5.28 5.93 4.43 5.95 ...
\end{verbatim}

We just need to change Wave to be an ordered factor. Because R's default is to order factors alphabetically, we can use the levels command and identify our preferred order.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Amodeo\_long}\SpecialCharTok{$}\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{Wave, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Pre"}\NormalTok{, }\StringTok{"Post"}\NormalTok{,}
    \StringTok{"FollowUp"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We check the structure again.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   24 obs. of  3 variables:
 $ ID        : Factor w/ 8 levels "1","2","3","4",..: 1 1 1 2 2 2 3 3 3 4 ...
 $ Wave      : Factor w/ 3 levels "Pre","Post","FollowUp": 1 2 3 1 2 3 1 2 3 1 ...
 $ Resilience: num  6.49 5.28 5.93 4.43 5.95 ...
\end{verbatim}

\textbf{Shape Shifters}

The form of our data matters. The simulation created a \emph{long} form (formally called the \emph{person-period} form) of data. That is, each observation for each person is listed in its own row. In this dataset where we have 8 people with 3 observation (pre, post, follow-up) each, we have 24 rows. This is convenient, because this is the form we need for repeated measures ANOVA.

However, for some of the calculations (particularly those we will do by hand), we need the data to be in its more familiar wide form (formally called the \emph{person level} form). We can do this with the \emph{data.table} and \emph{reshape2}()* packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new df (Amodeo\_wide) Identify the original df We are}
\CommentTok{\# telling it to connect the values of the Resilience variable its}
\CommentTok{\# respective Wave designation}
\NormalTok{Amodeo\_wide }\OtherTok{\textless{}{-}}\NormalTok{ reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Amodeo\_long, }\AttributeTok{formula =}\NormalTok{ ID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave,}
    \AttributeTok{value.var =} \StringTok{"Resilience"}\NormalTok{)}
\CommentTok{\# doublecheck to see if they did what you think}
\FunctionTok{str}\NormalTok{(Amodeo\_wide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   8 obs. of  4 variables:
 $ ID      : Factor w/ 8 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8
 $ Pre     : num  6.49 4.43 4.77 5.91 4.84 ...
 $ Post    : num  5.28 5.95 6.43 7 6.28 ...
 $ FollowUp: num  5.93 5.19 6.54 6.19 6.24 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Amodeo\_wide}\SpecialCharTok{$}\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Amodeo\_wide}\SpecialCharTok{$}\NormalTok{ID)}
\end{Highlighting}
\end{Shaded}

In this reshape script, I asked for a quick structure check. The format of the variables looks correct.

If you want to export these data as files to your computer, remove the hashtags to save (and re-import) them as .rds (R object) or .csv (``Excel lite'') files. This is not a necessary step to continue working the problem in this lesson.

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R. I would choose this option.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Amodeo\_long, \textquotesingle{}Amodeo\_longRDS.rds\textquotesingle{})}
\CommentTok{\# saveRDS(Amodeo\_wide, \textquotesingle{}Amodeo\_wideRDS.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file Amodeo\_long \textless{}{-} readRDS(\textquotesingle{}Amodeo\_longRDS.rds\textquotesingle{})}
\CommentTok{\# Amodeo\_wide \textless{}{-} readRDS(\textquotesingle{}Amodeo\_wideRDS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

Another option is to write them as .csv files. The code for .csv will likely lose any variable formatting, but the .csv file is easy to view and manipulate in Excel. If you choose this option, you will probably need to re-run the prior code to reformat Wave as an ordered factor

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Amodeo\_long,}
\CommentTok{\# file=\textquotesingle{}Amodeo\_longCSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE,}
\CommentTok{\# row.names=FALSE) write.table(Amodeo\_wide,}
\CommentTok{\# file=\textquotesingle{}Amodeo\_wideCSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE,}
\CommentTok{\# row.names=FALSE) bring back the simulated dat from a .csv file}
\CommentTok{\# Amodeo\_long \textless{}{-} read.csv (\textquotesingle{}Amodeo\_longCSV.csv\textquotesingle{}, header = TRUE)}
\CommentTok{\# Amodeo\_wide \textless{}{-} read.csv (\textquotesingle{}Amodeo\_wideCSV.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{quick-peek-at-the-data}{%
\subsection{Quick peek at the data}\label{quick-peek-at-the-data}}

As we work the problem we will switch between long and wide formats.Before we get into the statistic let's inspect our data. We can start with the long form.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   24 obs. of  3 variables:
 $ ID        : Factor w/ 8 levels "1","2","3","4",..: 1 1 1 2 2 2 3 3 3 4 ...
 $ Wave      : Factor w/ 3 levels "Pre","Post","FollowUp": 1 2 3 1 2 3 1 2 3 1 ...
 $ Resilience: num  6.49 5.28 5.93 4.43 5.95 ...
\end{verbatim}

In the following output, note the order of presentation of the grouping variable (i.e., FollowUp, Post, Pre). Even though we have ordered our factor so that ``Pre'' is first, the \emph{describeBy()} function seems to be ordering them alphabetically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{Resilience, Wave, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Amodeo\_long,}
    \AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item   group1 vars n  mean    sd median trimmed   mad   min   max range
X11    1 FollowUp    1 8 6.137 0.473  6.216   6.137 0.442 5.187 6.708 1.521
X12    2     Post    1 8 6.328 0.655  6.356   6.328 0.875 5.283 7.090 1.807
X13    3      Pre    1 8 5.588 0.822  5.771   5.588 1.147 4.429 6.597 2.168
      skew kurtosis    se
X11 -0.720   -0.610 0.167
X12 -0.231   -1.629 0.232
X13 -0.144   -1.812 0.291
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note. Recently my students and I have been having intermittent}
\CommentTok{\# struggles with the describeBy function in the psych package. We}
\CommentTok{\# have noticed that it is problematic when using .rds files and when}
\CommentTok{\# using data directly imported from Qualtrics. If you are having}
\CommentTok{\# similar difficulties, try uploading the .csv file and making the}
\CommentTok{\# appropriate formatting changes.}
\end{Highlighting}
\end{Shaded}

Another view (if we use the wide file).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Amodeo\_wide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars n mean   sd median trimmed  mad  min  max range  skew kurtosis
ID*         1 8 4.50 2.45   4.50    4.50 2.97 1.00 8.00  7.00  0.00    -1.65
Pre         2 8 5.59 0.82   5.77    5.59 1.15 4.43 6.60  2.17 -0.14    -1.81
Post        3 8 6.33 0.66   6.36    6.33 0.88 5.28 7.09  1.81 -0.23    -1.63
FollowUp    4 8 6.14 0.47   6.22    6.14 0.44 5.19 6.71  1.52 -0.72    -0.61
           se
ID*      0.87
Pre      0.29
Post     0.23
FollowUp 0.17
\end{verbatim}

Our means suggest that resilience increases from pre to post, then declines a bit. We use one-way repeated measures ANOVA to learn if there are statistically significant differences between the pairs of means and over time.

Let's also take a quick look at a boxplot of our data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{boxplot}\NormalTok{(Resilience }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{data =}\NormalTok{ Amodeo\_long, }\AttributeTok{xlab =} \StringTok{"Wave"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Resilience"}\NormalTok{,}
    \AttributeTok{n.label =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-343-1.pdf}

\hypertarget{working-the-one-way-repeated-measures-anova-by-hand}{%
\section{Working the One-Way Repeated Measures ANOVA (by hand)}\label{working-the-one-way-repeated-measures-anova-by-hand}}

Before working our problem in R, let's gain a conceptual understanding by partitioning the variance by hand.

In repeated measures ANOVA: \(SS_T = SS_B + SS_W\), where

\begin{itemize}
\tightlist
\item
  B = between-subjects variance
\item
  W = within-subjects variance

  \begin{itemize}
  \tightlist
  \item
    \(SS_W = SS_M + SS_R\)
  \end{itemize}
\end{itemize}

What differs is that \(SS_M\) and \(SS_R\) (model and residual) are located in \(SS_W\)

\begin{itemize}
\tightlist
\item
  \(SS_T = SS_B + (SS_M + SS_R)\)
\end{itemize}

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/SourceTable.jpg}
\caption{Demonstration of partitioning variance}
\end{figure}

\hypertarget{sums-of-squares-total-2}{%
\subsection{Sums of Squares Total}\label{sums-of-squares-total-2}}

Our formulas for \(SS_{T}\) are the same as they were for one-way and factorial ANOVA:

\[SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}\]
\[SS_{T}= s_{grand}^{2}(n-1)\]
Degrees of freedom for \(SS_T\) is \emph{N} - 1, where \emph{N} is the total number of cells.

Let's take a moment to \emph{hand-calculate} \(SS_{T}\) (but using R).

Our grand (i.e., overall) mean is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{Resilience)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.017408
\end{verbatim}

Subtracting the grand mean from each resilience score yields a mean difference.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{Amodeo\_long }\OtherTok{\textless{}{-}}\NormalTok{ Amodeo\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_dev =}\NormalTok{ Resilience}\SpecialCharTok{{-}}\FunctionTok{mean}\NormalTok{(Resilience))}

\FunctionTok{head}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID     Wave Resilience       m_dev
1  1      Pre   6.492125  0.47471697
2  1     Post   5.283057 -0.73435114
3  1 FollowUp   5.927930 -0.08947756
4  2      Pre   4.428839 -1.58856921
5  2     Post   5.948499 -0.06890871
6  2 FollowUp   5.186767 -0.83064071
\end{verbatim}

\textbf{Pop quiz}: What's the sum of our new \emph{m\_dev} variable?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{m\_dev)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.000000000000007993606
\end{verbatim}

If we square those mean deviations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Amodeo\_long }\OtherTok{\textless{}{-}}\NormalTok{ Amodeo\_long }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{m\_devSQ =}\NormalTok{ m\_dev}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\FunctionTok{head}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  ID     Wave Resilience       m_dev     m_devSQ
1  1      Pre   6.492125  0.47471697 0.225356199
2  1     Post   5.283057 -0.73435114 0.539271599
3  1 FollowUp   5.927930 -0.08947756 0.008006235
4  2      Pre   4.428839 -1.58856921 2.523552145
5  2     Post   5.948499 -0.06890871 0.004748410
6  2 FollowUp   5.186767 -0.83064071 0.689963983
\end{verbatim}

If we sum the squared mean deviations:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{m\_devSQ)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 11.65769
\end{verbatim}

This value, the sum of squared deviations around the grand mean, is our \(SS_T\); the associated \emph{degrees of freedom} is 23 (24 - 1; \emph{N} - 1).

\hypertarget{sums-of-squares-within-for-repated-measures-anova}{%
\subsection{Sums of Squares Within for Repated Measures ANOVA}\label{sums-of-squares-within-for-repated-measures-anova}}

The format of the formula is parallel to the formulae for \(SS\) we have seen before. In this case each person serves as its own grouping factor.

\[SS_W = s_{person1}^{2}(n_{1}-1)+s_{person2}^{2}(n_{2}-1)+s_{person3}^{2}(n_{3}-1)+...+s_{personk}^{2}(n_{k}-1)\]
The degrees of freedom (df) within is \emph{N - k}; or the summation of the df for each of the persons.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Resilience }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ID, }\AttributeTok{data =}\NormalTok{ Amodeo\_long, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item group1 vars n  mean    sd median trimmed   mad   min   max range
X11    1      1    1 3 5.901 0.605  5.928   5.901 0.836 5.283 6.492 1.209
X12    2      2    1 3 5.188 0.760  5.187   5.188 1.124 4.429 5.948 1.520
X13    3      3    1 3 5.912 0.992  6.430   5.912 0.160 4.768 6.537 1.769
X14    4      4    1 3 6.370 0.568  6.191   6.370 0.414 5.913 7.005 1.092
X15    5      5    1 3 5.787 0.824  6.240   5.787 0.064 4.836 6.283 1.447
X16    6      6    1 3 5.744 0.146  5.693   5.744 0.095 5.629 5.908 0.279
X17    7      7    1 3 6.627 0.248  6.597   6.627 0.300 6.395 6.889 0.494
X18    8      8    1 3 6.612 0.533  6.708   6.612 0.565 6.038 7.090 1.052
      skew kurtosis    se
X11 -0.044   -2.333 0.349
X12  0.002   -2.333 0.439
X13 -0.380   -2.333 0.573
X14  0.283   -2.333 0.328
X15 -0.384   -2.333 0.475
X16  0.304   -2.333 0.084
X17  0.118   -2.333 0.143
X18 -0.175   -2.333 0.307
\end{verbatim}

With 8 people, there will be 8 chunks of the analysis, in each:

\begin{itemize}
\tightlist
\item
  SD squared (to get the variance)
\item
  multiplied by the number of observations less 1
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\FloatTok{0.605}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.76}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.992}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.568}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}
\NormalTok{    (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.824}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.146}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.248}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}}
    \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}\NormalTok{ (}\FloatTok{0.553}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3} \SpecialCharTok{{-}} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.635836
\end{verbatim}

\hypertarget{sums-of-squares-model-effect-of-time}{%
\subsection{Sums of Squares Model -- Effect of Time}\label{sums-of-squares-model-effect-of-time}}

The \(SS_{M}\) conceptualizes the within-persons (or repeated measures) element as the grouping factor. In our case these are the pre, post, and follow-up clusters.

\[SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}\]
The degrees of freedom will be \emph{k} - 1 (number of levels, minus one).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Amodeo\_wide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars n mean   sd median trimmed  mad  min  max range  skew kurtosis
ID*         1 8 4.50 2.45   4.50    4.50 2.97 1.00 8.00  7.00  0.00    -1.65
Pre         2 8 5.59 0.82   5.77    5.59 1.15 4.43 6.60  2.17 -0.14    -1.81
Post        3 8 6.33 0.66   6.36    6.33 0.88 5.28 7.09  1.81 -0.23    -1.63
FollowUp    4 8 6.14 0.47   6.22    6.14 0.44 5.19 6.71  1.52 -0.72    -0.61
           se
ID*      0.87
Pre      0.29
Post     0.23
FollowUp 0.17
\end{verbatim}

In this case, we are interested in change in resilience over time. Hence, \emph{time} is our factor. In our equation, we have three chunks representing the pre, post, and follow-up \emph{conditions} (waves). From each, we subtract the grand mean, square it, and multiply by the \emph{n} observed in each wave.

The degrees of freedom (df) for \(SS_M\) is \emph{k} - 1

Let's calculate grand mean; that is the resilience score for all participants across all waves.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Amodeo\_long}\SpecialCharTok{$}\NormalTok{Resilience)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6.017408
\end{verbatim}

Now we can calculate the \(SS_M\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{8} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{6.14} \SpecialCharTok{{-}} \FloatTok{6.017}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{8} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{6.33} \SpecialCharTok{{-}} \FloatTok{6.017}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{8} \SpecialCharTok{*}\NormalTok{ (}\FloatTok{5.59} \SpecialCharTok{{-}} \FloatTok{6.017}\NormalTok{)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.363416
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# df is 3{-}1 = 2}
\end{Highlighting}
\end{Shaded}

\hypertarget{sums-of-squares-residual}{%
\subsection{Sums of Squares Residual}\label{sums-of-squares-residual}}

Because \(SS_W = SS_M + SS_R\) we can caluclate \(SS_R\) with simple subtraction:

\begin{itemize}
\tightlist
\item
  \(SS_w\) = 6.636
\item
  \(SS_M\) = 2.363
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{6.636} \SpecialCharTok{{-}} \FloatTok{2.363}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.273
\end{verbatim}

Correspondingly, the degrees of freedom (also taking the easy way out) is calculated by subtracting (the associated degrees of freedom) \(SS_M\) from \(SS_W\).

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{16{-}2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 14
\end{verbatim}

\hypertarget{sums-of-squares-between}{%
\subsection{Sums of Squares Between}\label{sums-of-squares-between}}

The \(SS_B\) is not used in our calculations today, but also calculated easily. Given that \(SS_T\) = \(SS_W\) + \(SS_B\):

\begin{itemize}
\tightlist
\item
  \(SS_T\) = 11.66; \emph{df} = 23
\item
  \(SS_W\) = 6.64; \emph{df} = 16
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{11.66} \SpecialCharTok{{-}} \FloatTok{6.64}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.02
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{23{-}16}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7
\end{verbatim}

\(SS_B\) = 5.02, \emph{df} = 7

\includegraphics{images/oneway_repeated/SourceTable.jpg}
Looking again at our sourcetable, we can move through the steps to calculate our \emph{F} statistic.

\hypertarget{mean-squares-model-residual-1}{%
\subsection{Mean Squares Model \& Residual}\label{mean-squares-model-residual-1}}

Now that we have the Sums of Squares, we can calculate the mean squares (we need these for our \(F\) statistic). Here is the formula for the mean square model.

\[MS_M = \frac{SS_{M}}{df^{_{M}}}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#mean squares for the model}
\FloatTok{2.36}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.18
\end{verbatim}

HEre is the formula for mean square residual.

And \(MS_R=\)
\[MS_R = \frac{SS_{R}}{df^{_{R}}}\]
Recall, degrees of freedom for the residual is \(N - k\). In our case that is 90 - 3.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#mean squares for the residual}
\FloatTok{4.27} \SpecialCharTok{/} \DecValTok{14}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.305
\end{verbatim}

\hypertarget{f-ratio}{%
\subsection{\texorpdfstring{\emph{F} ratio}{F ratio}}\label{f-ratio}}

The \emph{F} ratio is calculated with \(MS_M\) and \(MS_R=\).

\[F = \frac{MS_{M}}{MS_{R}}\]

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{1.18} \SpecialCharTok{/}\NormalTok{ .}\DecValTok{305}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.868852
\end{verbatim}

To find the \(F_{CV}\) we can use an \href{https://www.statology.org/f-distribution-table/}{F distribution table}. Or use a look-up function in R, which follows this general form: qf(p, df1, df2. lower.tail=FALSE)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qf}\NormalTok{(.}\DecValTok{05}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{14}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.738892
\end{verbatim}

Our example has 2 (numerator) and 14 (denominator) degrees of freedom. If we use a table we find the corresponding degrees of freedom combinations for the column where \(\alpha = .05\). We observe that any \(F\) value \textgreater{} 3.73 will be statistically significant. Our \(F\) = 3.87, so we have (just barely) exceeded the threshold. This is our \emph{omnibus F}. We know there is at least 1 statistically significant difference between our pre, post, and follow-up conditions.

\hypertarget{working-the-one-way-anova-with-r-packages}{%
\section{Working the One-Way ANOVA with R packages}\label{working-the-one-way-anova-with-r-packages}}

\hypertarget{testing-the-assumptions}{%
\subsection{Testing the assumptions}\label{testing-the-assumptions}}

We will start by testing the assumptions. Highlighting in the figure notes our place in the one-way ANOVA workflow:

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/wf_rptd_assumptions.jpg}
\caption{Image of our position in the workflow for the one-way repeated measures ANOVA}
\end{figure}

There are several different ways to conduct a repeated measures ANOVA. Each has different assumptions/requirements. These include:

\begin{itemize}
\tightlist
\item
  univariate statistics

  \begin{itemize}
  \tightlist
  \item
    This is what we will use today.
  \end{itemize}
\item
  multivariate statistics

  \begin{itemize}
  \tightlist
  \item
    Functionally similar to univariate, except the underlying algorithm does not require the sphericity assumption.
  \end{itemize}
\item
  multi-level modeling/hierarchical linear modeling

  \begin{itemize}
  \tightlist
  \item
    This a different statistic altogether and is addressed in the \href{https://lhbikos.github.io/MultilevelModeling}{multilevel modeling OER}.
  \end{itemize}
\end{itemize}

\hypertarget{univariate-assumptions-for-repeated-measures-anova}{%
\subsubsection{Univariate assumptions for repeated measures ANOVA}\label{univariate-assumptions-for-repeated-measures-anova}}

\begin{itemize}
\tightlist
\item
  The cases represent a random sample from the population.
\item
  There is no dependency in the scores \emph{between} participants.

  \begin{itemize}
  \tightlist
  \item
    Of course there is intentional dependency in the repeated measures (or within-subjects) factor.
  \end{itemize}
\item
  There are no significant outliers in any cell of the design

  \begin{itemize}
  \tightlist
  \item
    Check by visualizing the data using box plots. The \emph{identify\_outliers()} function in the \emph{rstatix} package identifies extreme outliers.
  \end{itemize}
\item
  The dependent variable is normally distributed in the population for each level of the within-subjects factor.

  \begin{itemize}
  \tightlist
  \item
    Conduct a Shapiro-Wilk test of normality for each of the levels of the DV.
  \item
    Visually examine Q-Q plots.
  \end{itemize}
\item
  The population variance of difference scores computed between any two levels of a within-subjects factor is the same value regardless of which two levels are chosen; termed the \textbf{sphericity assumption}. This assumption is

  \begin{itemize}
  \tightlist
  \item
    akin to compound symmetry (both variances across conditions are equal).
  \item
    akin to the homogeneity of variance assumption in between-group designs.
  \item
    sometimes called the homogeneity-of-variance-of-differences assumption.
  \item
    statistically evaluated with \emph{Mauchly's test.} If Mauchly's \emph{p} \textless{} .05, there are statistically significant differences. The \emph{anova\_test()} function in the \emph{rstatix} package reports Mauchly's and two alternatives to the traditional \emph{F} that correct the values by the degree to which the sphericity assumption is violated.
  \end{itemize}
\end{itemize}

\hypertarget{demonstrating-sphericity}{%
\subsubsection{Demonstrating sphericity}\label{demonstrating-sphericity}}

Using the data from our motivating example, I calculated differences for each of the time variables. These are the three columns (in green shading) on the right. The variance for each is reported at the bottom of the column.

When we get into the analysis, we will use \emph{Mauchly's test} in hopes that there are non-significant differences in variances between all three of the comparisons.

We are only concerned with the sphericity assumption if there are three or more groups.

\[variance_{A-B} \approx variance_{A-C}\approx variance_{B-C}\]

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=5.20833in,height=2.60417in]{images/oneway_repeated/mauchly.jpg}
\caption{Demonstration of unequal variances}\label{id}
}
\end{figure}

\hypertarget{any-outliers}{%
\subsubsection{Any outliers?}\label{any-outliers}}

The boxplot is one common way for identifying outliers. The boxplot uses the median and the lower (25th percentile) and upper (75th percentile) quartiles. The difference bewteen Q3 and Q1 is the \emph{interquartile range} (IQR). Outliers are generally identified when values fall outside these lower and upper boundaries:

\begin{itemize}
\tightlist
\item
  Q1 - 1.5xIQR
\item
  Q3 + 1.5xIQR
\end{itemize}

Extreme values occur when values fall outside these boundaries:

\begin{itemize}
\tightlist
\item
  Q1 - 3xIQR
\item
  Q3 + 3xIQR
\end{itemize}

Let's take a look at a boxplot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note that we are creating an object (bxp) from our work.  This}
\CommentTok{\# script creates the basic boxplot, we will add to it (by using the}
\CommentTok{\# object) later.}
\NormalTok{bxp }\OtherTok{\textless{}{-}}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Amodeo\_long, }\AttributeTok{x =} \StringTok{"Wave"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Resilience"}\NormalTok{, }\AttributeTok{add =} \StringTok{"point"}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Assessment Wave"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Self{-}Perception of Resilience"}\NormalTok{)}
\NormalTok{bxp}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-361-1.pdf}
The package \emph{rstatix} has features that allow us to identify outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Amodeo\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Wave)}\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  rstatix}\SpecialCharTok{::}\FunctionTok{identify\_outliers}\NormalTok{(Resilience)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] Wave       ID         Resilience m_dev      m_devSQ    is.outlier is.extreme
<0 rows> (or 0-length row.names)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#?identify\_outliers}
\end{Highlighting}
\end{Shaded}

The output, ``0 rows'' indicates there are no outliers.

This is consistent with the visual inspection of boxplots (above), where all observed scores fell within the 1.5x the interquartile range.

\hypertarget{assessing-normality}{%
\subsubsection{Assessing normality}\label{assessing-normality}}

We can obtain skew and kurtosis values for each cell in our model with the \emph{psych::describeBy()} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Resilience }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{data =}\NormalTok{ Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item   group1 vars n     mean        sd   median  trimmed       mad
X11    1      Pre    1 8 5.587693 0.8217561 5.770952 5.587693 1.1471137
X12    2     Post    1 8 6.327615 0.6550520 6.356491 6.327615 0.8751431
X13    3 FollowUp    1 8 6.136916 0.4729432 6.215983 6.136916 0.4416578
         min      max    range       skew   kurtosis        se
X11 4.428839 6.597214 2.168376 -0.1437061 -1.8118551 0.2905347
X12 5.283057 7.089591 1.806534 -0.2307393 -1.6287654 0.2315959
X13 5.186767 6.708259 1.521491 -0.7204842 -0.6102953 0.1672107
\end{verbatim}

Our skew and kurtosis values fall below the thresholds of concern \citep{kline_principles_2016}:

\begin{itemize}
\tightlist
\item
  \textless{} 3 for skew
\item
  8 - 20 indicates extreme skew for kurtosis
\end{itemize}

We can use the Shapiro-Wilk test for a formal detection of normality. When \emph{p} \textless{} .05, it indicates that the distribution is statistically significantly different than a normal one. Therefore, \emph{p} \textgreater{} .05 indicates we did not violate the normal distribution assumption. The code below groups the DV by wave so that we can test normality for each cell in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Amodeo\_long }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Wave) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  rstatix}\SpecialCharTok{::}\FunctionTok{shapiro\_test}\NormalTok{(Resilience)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 4
  Wave     variable   statistic     p
  <fct>    <chr>          <dbl> <dbl>
1 Pre      Resilience     0.919 0.419
2 Post     Resilience     0.941 0.617
3 FollowUp Resilience     0.926 0.480
\end{verbatim}

The \emph{p} value is \textgreater{} .05 for each of the cells. This provides some assurance that we have not violated the assumption of normality at any level of the design.

The Shapiro-Wilk test is sensitive to sample size \citep{datanovia_repeated_nodate}. Samples \textgreater{} 50 may lead to \emph{p} values that are \textless{} .05, even when non-normality is not problematic. Therefore a second check with a Q-Q plot can be helpful. In a normal distribution the residuals will align along the line for each of the cells in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggqqplot}\NormalTok{(Amodeo\_long, }\StringTok{"Resilience"}\NormalTok{, }\AttributeTok{facet.by =} \StringTok{"Wave"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-365-1.pdf}

\textbf{APA Assumption Write-up So Far}

\begin{quote}
Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the \emph{identify\_outliers()} function in the \emph{rstatix} package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (at each wave of assessment) fell within cautionary ranges for skew and kurtosis \citep{kline_principles_2016}. Additionally, the Shapiro-Wilk tests applied at each wave of the design were non-significant.
\end{quote}

\hypertarget{assumption-of-sphericity}{%
\subsubsection{Assumption of Sphericity}\label{assumption-of-sphericity}}

The sphericity assumption is automatically checked with Mauchley's test during the computation of the ANOVA when the \emph{rstatix::anova\_test()} function is used. When the \emph{rstatix::get\_anova\_table()} function is used, the Greenhouse-Geisser sphericity correction is automatically applied to factors violating the sphericity assumption.

The effect size, \(\eta^2\) is reported in the column labeled ``ges.''

Earlier in the lesson I noted that the evaluation of the sphericity assumption occurs at the same time that we evaluate the omnibus ANOVA. We are at that point in the analyses. The workflow points to our stage in the process.

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/wf_rptd_omnibus.jpg}
\caption{Image of our position in the workflow for the one-way repeated measures ANOVA}
\end{figure}

\hypertarget{omnibus-repeated-measures-anova}{%
\subsection{Omnibus Repeated Measures ANOVA}\label{omnibus-repeated-measures-anova}}

As we prepare to run the omnibus ANOVA it may be helpful to think again about our variables. Our DV, Resilience, should be a continuous variable. In R, its structure should be ``num.'' Our predictor, Wave, should be categorical. In R case, Wave should be an ordered factor that is consistent with its timing: pre, post, follow-up.

The repeated measures ANOVA must be run with a long form of the data. This means that there needs to be a within-subjects identifier. In our case, it is the ``ID'' variable which is also formatted as a factor.

We can verify the format of our variables by examining the structure of our dataframe. Recall that we created the ``m\_dev'' and ``m\_devSQ'' variables earlier in the demonstration. We will not use them in this analysis; it does not harm anything for them to ``ride along'' in the dataframe.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Amodeo\_long)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   24 obs. of  5 variables:
 $ ID        : Factor w/ 8 levels "1","2","3","4",..: 1 1 1 2 2 2 3 3 3 4 ...
 $ Wave      : Factor w/ 3 levels "Pre","Post","FollowUp": 1 2 3 1 2 3 1 2 3 1 ...
 $ Resilience: num  6.49 5.28 5.93 4.43 5.95 ...
 $ m_dev     : num  0.4747 -0.7344 -0.0895 -1.5886 -0.0689 ...
 $ m_devSQ   : num  0.22536 0.53927 0.00801 2.52355 0.00475 ...
\end{verbatim}

We can use the \emph{rstatix::anova\_test()} function to calculate the omnibus ANOVA. Notice where our variables are included in the script:

\begin{itemize}
\tightlist
\item
  Resilience is the dv
\item
  ID is the wid
\item
  Wave is assigned to within
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{RM\_AOV }\OtherTok{\textless{}{-}}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Amodeo\_long, }\AttributeTok{dv =}\NormalTok{ Resilience, }\AttributeTok{wid =}\NormalTok{ ID,}
    \AttributeTok{within =}\NormalTok{ Wave)}
\NormalTok{RM\_AOV}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA Table (type III tests)

$ANOVA
  Effect DFn DFd    F     p p<.05   ges
1   Wave   2  14 3.91 0.045     * 0.203

$`Mauchly's Test for Sphericity`
  Effect     W     p p<.05
1   Wave 0.566 0.182      

$`Sphericity Corrections`
  Effect   GGe    DF[GG] p[GG] p[GG]<.05   HFe      DF[HF] p[HF] p[HF]<.05
1   Wave 0.698 1.4, 9.77 0.068           0.817 1.63, 11.44 0.057          
\end{verbatim}

We can assemble our \emph{F} string from the ANOVA object: \(F(2,14) = 3.91, p = 0.045, \eta^2 = 0.203\)

From the Mauchly's Test for Sphericity object we learn that we did not violate the sphericity assumption: \(W = 0.566, p = .182\)

From the Sphericity Corrections object are output for two alternative corrections to the \emph{F} statistic, the Greenhouse-Geiser epsilon (GGe), and Huynh-Feldt epsilon (HFe). Because we did not violate the sphericity assumption we do not need to use them. Notice that these two tests adjust our df (both numerator and denominator) to have a more conservative p value.

If we needed to write an \emph{F} string that is corrected for violation of the sphericity assumption, it might look like this:

\begin{quote}
The Greenhouse Geiser estimate was 0.698 the correct omnibus was \emph{F}(1.4, 9.77) = 3.91, \emph{p} = .068.
The Huyhn Feldt estimate was 0.817 and the corrected omnibus was \emph{F} (1.63, 11.44) = 3.91 \emph{p} = .057.
\end{quote}

You might be surprised that we are at follow-up already.

\begin{figure}
\centering
\includegraphics{images/oneway_repeated/wf_rptd_pairwise.jpg}
\caption{Image of our position in the workflow for the one-way repeated measures ANOVA}
\end{figure}

\hypertarget{follow-up}{%
\subsection{Follow-up}\label{follow-up}}

Given the simplicity of our design, it makes sense to me to follow-up with post hoc, pairwise, comparisons. Note that when I am calculating these pairwise \emph{t} tests, I am creating an object (named ``pwc''). The object will be a helpful tool in creating a Figure and an APA Style table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc }\OtherTok{\textless{}{-}}\NormalTok{ Amodeo\_long }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{pairwise\_t\_test}\NormalTok{(Resilience }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{p.adjust.method =} \StringTok{"bonferroni"}\NormalTok{)}
\NormalTok{pwc}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  .y.        group1 group2     n1    n2 statistic    df     p p.adj p.adj.signif
* <chr>      <chr>  <chr>   <int> <int>     <dbl> <dbl> <dbl> <dbl> <chr>       
1 Resilience Pre    Post        8     8     -2.15     7 0.069 0.206 ns          
2 Resilience Pre    Follow~     8     8     -2.00     7 0.086 0.257 ns          
3 Resilience Post   Follow~     8     8      1.06     7 0.325 0.975 ns          
\end{verbatim}

Although we had a statistically significant omnibus test, we did not obtain statistically significant results in an of the posthoc pairwise comparisons. Why not?

\begin{itemize}
\tightlist
\item
  Our omnibus \emph{F} was right at the margins

  \begin{itemize}
  \tightlist
  \item
    a larger sample size (assuming that the effects would hold) would have been more powerful.
  \item
    there could be significance if we compared pre to the combined effects of post and follow-up.
  \end{itemize}
\end{itemize}

How would we manage Type I error? With only three possible post-omnibus comparisons, I would cite the Tukey LSD approach and not adjust the alpha to a more conservative level \citep{green_using_2014}.

We can combine information from the object we created (``bxp'') from an earlier boxplot with the object we saved from the posthoc pairwise comparisons (``pwc) to enhance our boxplot with the \emph{F} string and indications of pairwise significant (or, in our case, non-significance).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc }\OtherTok{\textless{}{-}}\NormalTok{ pwc }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{add\_xy\_position}\NormalTok{(}\AttributeTok{x =} \StringTok{"Wave"}\NormalTok{)}
\NormalTok{bxp }\SpecialCharTok{+}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_pvalue\_manual}\NormalTok{(pwc) }\SpecialCharTok{+} \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_test\_label}\NormalTok{(RM\_AOV,}
    \AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{), }\AttributeTok{caption =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_pwc\_label}\NormalTok{(pwc))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-369-1.pdf}
Unfortunately, the \emph{apaTables} package does not work with the \emph{rstatix} package, so a table would need to be crafted by hand.

\hypertarget{results-section}{%
\subsection{Results Section}\label{results-section}}

\begin{quote}
Repeated measures ANOVA has several assumptions regarding outliers, normality, and sphericity. Visual inspection of boxplots for each wave of the design, assisted by the \emph{rstatix::identify\_outliers()} function (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated no outliers. Regarding normality, no values of skew and kurtosis (at each wave of assessment) fell within cautionary ranges for skew and kurtosis \citep{kline_principles_2016}. Additionally, the Shapiro-Wilk tests applied at each wave of the design were non-significant. A non-significant Mauchley's test (\(W = 0.566, p = .182\)) indicated that the sphericity assumption was not violated.
\end{quote}

\begin{quote}
The omnibus ANOVA was significant: \(F(2,14) = 3.91, p = 0.045, \eta^2 = 0.203\). We followed up with all pairwise comparisons. Curiously, and in spite of a significant omnibus test, there were not statistically significant differences between any of the pairs. Regarding pre versus post, \emph{t} = -2.15, \emph{p}= .069. Regarding pre versus follow-up, \emph{t} = -2.00, \emph{p} = .068. Regarding post versus follow-up, \emph{t} = 1.059, \emph{p} = .325. Because there were only three pairwise comparisons subsequent to the omnibus test, alpha was retained at .05 \citep{green_using_2014}. While the trajectories from pre-to-post and pre-to-follow-up were in the expected direction, the small sample size likely contributed to a Type II error. Descriptive statistics are reported in Table 1 and the differences are illustrated in Figure 1.
\end{quote}

\hypertarget{creating-an-apa-style-table}{%
\subsubsection{Creating an APA Style Table**}\label{creating-an-apa-style-table}}

While I have not located a package that will take \emph{rstatix} output to make an APA style table, we can use the \emph{MASS} package to write the pwc object to a .csv file, then manually make our own table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(pwc, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"PWC.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparison-with-amodeo-et-al.-amodeo_empowering_2018}{%
\subsubsection{\texorpdfstring{Comparison with Amodeo et al.\citeyearpar{amodeo_empowering_2018}}{Comparison with Amodeo et al.{[}-@amodeo\_empowering\_2018{]}}}\label{comparison-with-amodeo-et-al.-amodeo_empowering_2018}}

How do our findings and our write-up from the simulated data compare with the original article?

The \emph{F} string is presented in the Table 1 note (\emph{F}{[}1.612, 11.283{]}) = 6.390, \emph{p} = 0.18, \(\eta _{p}^{2}\)). We can tell from the fractional degrees of freedom that the \emph{p} value has been had a correction for violation of the sphericity assumption.

Table 1 also reports all of the post-hoc, pairwise comparisons. There is no mention of control for Type I error. Had they used a traditional Bonferroni, they would have needed to reduce the alpha to (k*(k-1)/2) and then divide .05 by that number.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\DecValTok{3} \SpecialCharTok{*}\NormalTok{ (}\DecValTok{3{-}1}\NormalTok{))}\SpecialCharTok{/}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{05}\SpecialCharTok{/}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01666667
\end{verbatim}

Although they report 6 comparisons; 3 are repeated because they are merely in reverse. Yet, the revised alpha would be .016 and the one, lone, comparison would not have been statistically significant. That said, we can use the Tukey LSD because there are only 3 comparisons and holding alpha at .05 can be defended \citep{green_using_2014}.

\begin{itemize}
\tightlist
\item
  Regarding the presentation of the results

  \begin{itemize}
  \tightlist
  \item
    there is no figure
  \item
    there is no data presented in the text; all data is presented in Table 1
  \end{itemize}
\item
  Regarding the research design and its limitations

  \begin{itemize}
  \tightlist
  \item
    the authors note that a control condition would have better supported the conclusions
  \item
    the authors note the limited sample size and argue that this is a difficult group to recruit for intervention and evaluation
  \item
    the article is centered around the qualitative aspect of the design; the quantitative portion is, appropriately, secondary
  \end{itemize}
\end{itemize}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=10.41667in,height=1.04167in]{images/oneway_repeated/Amodio_design.jpg}
\caption{Another peek at the research design for the Amodeo et al study}\label{id}
}
\end{figure}

\hypertarget{power-in-repeated-measures-anova}{%
\section{Power in Repeated Measures ANOVA}\label{power-in-repeated-measures-anova}}

The package \emph{wp.rmanova} was designed for power analysis in repeated measures ANOVA.

Power analysis allows us to determine the probability of detecting an effect of a given size with a given level of confidence. Especially when we don't achieve significance, we may want to stop.

In the \emph{WebPower} package, we specify 6 of 7 interrelated elements; the package computes the missing one.

\begin{itemize}
\tightlist
\item
  \emph{n} = sample size (number of individuals in the whole study)
\item
  \emph{ng} = number of groups
\item
  \emph{nm} = number of measurements/conditions/waves
\item
  \emph{f} = Cohen's \emph{f} (an effect size; we can use a conversion calculator)
\item
  \emph{nscor} = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package's default; \textless{} 1 means some correction was applied.
\item
  \emph{alpha} = is the probability of Type I error; we traditionally set this at .05
\item
  \emph{power} = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
\item
  \emph{type} = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect.
\end{itemize}

I used \emph{effectsize} packages converter to transform our \(\eta^2\) to Cohen's \emph{f}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{effectsize}\SpecialCharTok{::}\FunctionTok{eta2\_to\_f}\NormalTok{(.}\DecValTok{203}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5046832
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WebPower}\SpecialCharTok{::}\FunctionTok{wp.rmanova}\NormalTok{(}\AttributeTok{n =} \DecValTok{8}\NormalTok{, }\AttributeTok{ng =} \DecValTok{1}\NormalTok{, }\AttributeTok{nm =} \DecValTok{3}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.5047}\NormalTok{, }\AttributeTok{nscor =} \FloatTok{0.689}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{type =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Repeated-measures ANOVA analysis

    n      f ng nm nscor alpha     power
    8 0.5047  1  3 0.689  0.05 0.1619613

NOTE: Power analysis for within-effect test
URL: http://psychstat.org/rmanova
\end{verbatim}

Here we learned that we were only powered at .16. That is, we had a 16\% chance of finding a statistically significant effect if, in fact, it existed. This is low!

In reverse, setting \emph{power} at .80 (the traditional value) and changing \emph{n} to \emph{NULL} yields a recommended sample size.\\
In many cases we won't know some of the values in advance. We can make best guesses based on our review of the literature. In the script below:

\begin{itemize}
\tightlist
\item
  \emph{nscor} is the degree of violation of the sphericity assumption. If we think we won't violate it, we can enter 1.0 or leave it out (the wp.rmanova default is 1.0)
\item
  \emph{f} is the effect size estimate; Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WebPower}\SpecialCharTok{::}\FunctionTok{wp.rmanova}\NormalTok{(}\AttributeTok{n =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{ng =} \DecValTok{1}\NormalTok{, }\AttributeTok{nm =} \DecValTok{3}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.5047}\NormalTok{, }\AttributeTok{nscor =} \FloatTok{0.689}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{type =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Repeated-measures ANOVA analysis

           n      f ng nm nscor alpha power
    50.87736 0.5047  1  3 0.689  0.05   0.8

NOTE: Power analysis for within-effect test
URL: http://psychstat.org/rmanova
\end{verbatim}

With these new values, we learn that we would need 50 individuals in order to feel confident in our ability to get a statistically significant result if, in fact, it existed.

\hypertarget{practice-problems-6}{%
\section{Practice Problems}\label{practice-problems-6}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that offer differing levels of difficulty. Whichever you choose, you will focus on these larger steps in one-way repeated measures/within-subjects ANOVA, including:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a one-way, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    conduct follow-up testing
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{problem-1-change-the-random-seed-1}{%
\subsection{Problem \#1: Change the Random Seed}\label{problem-1-change-the-random-seed-1}}

If repeated measures ANOVA is new to you, perhaps change the random seed and follow-along with the lesson.

\hypertarget{problem-2-increase-n}{%
\subsection{\texorpdfstring{Problem \#2: Increase \emph{N}}{Problem \#2: Increase N}}\label{problem-2-increase-n}}

Our analysis of the Amodeo et al. \citep{amodeo_empowering_2018} data failed to find significant increases in resilience from pre-to-post through follow-up. Our power analysis suggested that a sample size of 50 would be sufficient to garner statistical significance. The script below resimulates the data by increasing the sample size to 50 (from 8). All else remains the same.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2022}\NormalTok{)}
\NormalTok{ID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{50}\NormalTok{), }\AttributeTok{each =} \DecValTok{3}\NormalTok{)))  }\CommentTok{\#gives me 8 numbers, assigning each number 3 consecutive spots, in sequence}
\NormalTok{Resilience }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{150}\NormalTok{, }\AttributeTok{mean =} \FunctionTok{c}\NormalTok{(}\FloatTok{5.7}\NormalTok{, }\FloatTok{6.21}\NormalTok{, }\FloatTok{6.26}\NormalTok{), }\AttributeTok{sd =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.88}\NormalTok{, }\FloatTok{0.79}\NormalTok{,}
    \FloatTok{0.37}\NormalTok{))  }\CommentTok{\#gives me a column of 24 numbers with the specified Ms and SD}
\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Pre"}\NormalTok{, }\StringTok{"Post"}\NormalTok{, }\StringTok{"FollowUp"}\NormalTok{), }\AttributeTok{each =} \DecValTok{1}\NormalTok{, }\DecValTok{50}\NormalTok{)  }\CommentTok{\#repeats pre, post, follow{-}up once each, 8 times}
\NormalTok{Amodeo50\_long }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ID, Wave, Resilience)}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-3-try-something-entirely-new}{%
\subsection{Problem \#3: Try Something Entirely New}\label{problem-3-try-something-entirely-new}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a one-way repeated measures ANOVA. Please have at least 3 levels for the predictor variable.

\hypertarget{grading-rubric-3}{%
\subsection{Grading Rubric}\label{grading-rubric-3}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Check and, if needed, format data & 5 & \_\_\_\_\_ \\
2. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
3. Conduct omnibus ANOVA (w effect size) & 5 & \_\_\_\_\_ \\
4. Conduct all possible pairwise comparisons (like in the lecture) & 5 & \_\_\_\_\_ \\
5. Describe approach for managing Type I error & 5 & \_\_\_\_\_ \\
6. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
7. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 35 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{bonus-reel-1}{%
\section{Bonus Reel:}\label{bonus-reel-1}}

\begin{figure}
\hypertarget{id}{%
\centering
\includegraphics[width=6.45833in,height=2.19792in]{images/film-strip-1.jpg}
\caption{Image of a filmstrip}\label{id}
}
\end{figure}

Without the \emph{rstatix} helper package, here is how the analysis would be run in the package, \emph{car.} Note that this package results in the multivariate output. The \emph{p} value of the omnibus \emph{F} was non-significant from the start (\emph{p} = .213).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}

\NormalTok{waveLevels }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\NormalTok{waveFactor }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(waveLevels)}
\NormalTok{waveFrame }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(waveFactor)}
\NormalTok{waveBind }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(Amodeo\_wide}\SpecialCharTok{$}\NormalTok{Pre, Amodeo\_wide}\SpecialCharTok{$}\NormalTok{Post, Amodeo\_wide}\SpecialCharTok{$}\NormalTok{FollowUp)}
\NormalTok{waveModel }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(waveBind }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{)}
\NormalTok{analysis }\OtherTok{\textless{}{-}} \FunctionTok{Anova}\NormalTok{(waveModel, }\AttributeTok{idata =}\NormalTok{ waveFrame, }\AttributeTok{idesign =} \SpecialCharTok{\textasciitilde{}}\NormalTok{waveFactor)}
\FunctionTok{summary}\NormalTok{(analysis)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Type III Repeated Measures MANOVA Tests:

------------------------------------------
 
Term: (Intercept) 

 Response transformation matrix:
     (Intercept)
[1,]           1
[2,]           1
[3,]           1

Sum of squares and products for the hypothesis:
            (Intercept)
(Intercept)    2607.062

Multivariate Tests: (Intercept)
                 Df test stat approx F num Df den Df          Pr(>F)    
Pillai            1    0.9942 1200.028      1      7 0.0000000043326 ***
Wilks             1    0.0058 1200.028      1      7 0.0000000043326 ***
Hotelling-Lawley  1  171.4325 1200.028      1      7 0.0000000043326 ***
Roy               1  171.4325 1200.028      1      7 0.0000000043326 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

------------------------------------------
 
Term: waveFactor 

 Response transformation matrix:
     waveFactor1 waveFactor2
[1,]           1           0
[2,]           0           1
[3,]          -1          -1

Sum of squares and products for the hypothesis:
            waveFactor1 waveFactor2
waveFactor1   2.4131705  -0.8378898
waveFactor2  -0.8378898   0.2909282

Multivariate Tests: waveFactor
                 Df test stat approx F num Df den Df  Pr(>F)
Pillai            1 0.4026101 2.021846      2      6 0.21319
Wilks             1 0.5973899 2.021846      2      6 0.21319
Hotelling-Lawley  1 0.6739486 2.021846      2      6 0.21319
Roy               1 0.6739486 2.021846      2      6 0.21319

Univariate Type III Repeated-Measures ANOVA Assuming Sphericity

            Sum Sq num Df Error SS den Df   F value         Pr(>F)    
(Intercept) 869.02      1   5.0692      7 1200.0279 0.000000004333 ***
waveFactor    2.36      2   4.2272     14    3.9102        0.04476 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1


Mauchly Tests for Sphericity

           Test statistic p-value
waveFactor        0.56648 0.18179


Greenhouse-Geisser and Huynh-Feldt Corrections
 for Departure from Sphericity

            GG eps Pr(>F[GG])  
waveFactor 0.69759    0.06754 .
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

              HF eps Pr(>F[HF])
waveFactor 0.8172743 0.05734876
\end{verbatim}

\hypertarget{Mixed}{%
\chapter{Mixed Design ANOVA}\label{Mixed}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=b08debbb-948e-4f25-923a-ad8c01037e05}{Screencasted Lecture Link}

The focus of this lecture is mixed design ANOVA. That is, we are conducting a two-way ANOVA where one of the factors is repeated measures and one of the factors is between groups. The mixed design ANOVA is often associated with the random clinical trial (RCT) where the researcher hopes for a significant interaction effect. Specifically, the researcher hopes that the individuals who were randomly assigned to the treatment condition improve from pre-test to post-test and maintain (or continue to improve) after post-test, while the people assigned to the no-treatment control are not statistically significantly different from treatment group at pre-test, and do not improve over time.

\hypertarget{navigating-this-lesson-8}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-8}}

There is just over one hour of lecture. If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-8}{%
\subsection{Learning Objectives}\label{learning-objectives-8}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Evaluate the suitability of a research design/question and dataset for conducting a mixed design ANOVA; identify alternatives if the data is not suitable.
\item
  Test the assumptions for mixed design ANOVA.
\item
  Conduct a mixed design ANOVA (omnibus and follow-up) in R.
\item
  Interpret output from the mixed design ANOVA (and follow-up).
\item
  Prepare an APA style results section of the mixed design ANOVA output.
\item
  Conduct a power analysis for mixed design ANOVA.
\end{itemize}

\hypertarget{planning-for-practice-7}{%
\subsection{Planning for Practice}\label{planning-for-practice-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option comes from the research vignette. The Murrar and Brauer \citeyearpar{murrar_entertainment-education_2018} article has three variables (attitudes toward Arabs, attitudes toward Whites, and a difference score) which are suitable for mixed design ANOVAs. I will demonstrate a mixed design ANOVA with the difference score. I'll leave the other two variables for opportunities for practice.

As a third option, you are welcome to use data to which you have access and is suitable for two-way ANOVA. IIn either case the practice options suggest that you:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a mixed design ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    report main and interaction effects
  \item
    conduct follow-up testing of simple main effects
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{readings-resources-7}{%
\subsection{Readings \& Resources}\label{readings-resources-7}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Repeated Measures ANOVA in R: The Ultimate Guide. (n.d.). Datanovia. Retrieved October 19, 2020, from \url{https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/}

  \begin{itemize}
  \tightlist
  \item
    This website is an excellent guide for both one-way repeated measures and mixed design ANOVA. It is a great resource for both the conceptual and procedural. This is the guide I have used for the basis of the lecture. Working through their example would be provide an additional, excellent, opportunity for practice.
  \end{itemize}
\item
  Murrar, S., \& Brauer, M. (2018). Entertainment-education effectively reduces prejudice. \emph{Group Processes \& Intergroup Relations, 21}(7), 1053--1077. \url{https://doi.org/10.1177/1368430216682350}

  \begin{itemize}
  \tightlist
  \item
    This article is the source of our research vignette. Our vignette is simulated from the first of their two experiments. The authors did not conduct mixed design ANOVA. Instead, they ran independent-samples \emph{t} tests to test the differences between the sitcom conditions for each of the three waves. This is comparable to conducting the simple-main effect analysis of condition within wave subsequent to a significant interaction.
  \item
    Full-text of the article is available at the \href{https://www.researchgate.net/publication/312177602_Entertainment-education_effectively_reduces_prejudice}{authors' ResearchGate}.
  \end{itemize}
\end{itemize}

\hypertarget{packages-6}{%
\subsection{Packages}\label{packages-6}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# will install the package if not already installed}
\CommentTok{\# if(!require(knitr))\{install.packages(\textquotesingle{}knitr\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\}}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\}}
\CommentTok{\# if(!require(rstatix))\{install.packages(\textquotesingle{}rstatix\textquotesingle{})\}}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(effectsize))\{install.packages(\textquotesingle{}effectsize\textquotesingle{})\}}
\CommentTok{\# if(!require(WebPower))\{install.packages(\textquotesingle{}WebPower\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-mixed-design-anova}{%
\section{Introducing Mixed Design ANOVA}\label{introducing-mixed-design-anova}}

Mixed design ANOVA is characterized by the following:

\begin{itemize}
\tightlist
\item
  at least two independent variables.
\item
  Termed ``mixed'' because

  \begin{itemize}
  \tightlist
  \item
    one is a between-subjects factor, and
  \item
    one is a repeated-measures (i.e., within-subjects) factor.
  \end{itemize}
\item
  In essence, we are simultaneously conducting

  \begin{itemize}
  \tightlist
  \item
    a one-way independent ANOVA and a
  \item
    a one-way repeated-measures ANOVA.
  \end{itemize}
\end{itemize}

Especially when there is a significant interaction there can be numerous ways to follow up. We will work one set of analyses: simple main effects (condition within wave; wave within condition) and, when needed, conduct posthoc pairwise comparisons as follow-up. Other good options include identifying a priori contrasts and conducting polynomials (not demonstrated in this lecture).

\begin{figure}
\centering
\includegraphics{images/mixed/mx_workflow.jpg}
\caption{Image of a workflow for mixed design ANOVA}
\end{figure}

The steps in working the mixed design generally include,

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Exploring the data/evaluating the assumptions
\item
  Evaluating the omnibus test
\item
  Follow-up to the omnibus

  \begin{itemize}
  \tightlist
  \item
    if significant interaction effect: simple main effects and further follow-up to those
  \item
    if significant main effect (but no significant interaction effect), identify source of significance in the main effect
  \item
    if no significance, stop
  \end{itemize}
\item
  Write it up with tables, figure(s)
\end{enumerate}

Assumptions for the mixed design ANOVA include the following:

\begin{itemize}
\tightlist
\item
  The dependent variable should be continuous with no significant outliers in any cell of the design

  \begin{itemize}
  \tightlist
  \item
    Check by visualizing the data using box plots and by using the \emph{rstatix::identify\_outliers()} function
  \end{itemize}
\item
  The DV should be approximately normally distributed in each cell of the design

  \begin{itemize}
  \tightlist
  \item
    Check with Shapiro-Wilk normality test \emph{rstatix::shapiro\_test()} function and with visual inspection by creating Q-Q plots. The \emph{ggpubr::ggqqplot()} function is a great tool.
  \end{itemize}
\item
  The variances of the differences between groups should be equal. This is termed the \textbf{sphericity assumption.} This can be checked with Mauchly's test of sphericity, which is reported automatically in the \emph{rstatix::anova\_test()} output.
\end{itemize}

The best way to address violations of these assumptions is not always clear. Possible solutions include:

\begin{itemize}
\tightlist
\item
  For 2- and 3- way ANOVAs, violations of the normality assumption might be addressed by removing extreme outliers or considering transformations of the data. Transformations, though, introduce their own compexities regarding interpretation. Kline's text \citeyearpar{kline_principles_2016} provides excellent coverage of options.
\item
  A robust ANOVA option is available in the \emph{WRS2} package
\item
  If there are three or more waves/conditions and the sample is large, it may be possible to run a multi-level, model.
\item
  In the absence of alternatives, it may be necessary to run the mixed design with the violated assumptions, but report them.
\item
  \ldots.and more. Internet searches continue to offer new approaches and alternatives.
\end{itemize}

\hypertarget{research-vignette-7}{%
\section{Research Vignette}\label{research-vignette-7}}

This lesson's research vignette is from Murrar and Brauer's \citeyearpar{murrar_entertainment-education_2018} article that describes the results of two studies that evaluated interventions designed to reduce prejudice against Arabs/Muslims. We are working only a portion of the first study reported in the article. Participants (\emph{N} = 193), all who were White, were randomly assigned to one of two conditions where they watched six episodes of the sitcom \href{http://www.friends-tv.org/}{\emph{Friends}} or \href{https://en.wikipedia.org/wiki/Little_Mosque_on_the_Prairie}{\emph{Little Mosque on the Prairie}}. The sitcoms and specific episodes were selected after significant pilot testing. The selection was based on the tension selecting stimuli that were as similar as possible, yet the intervention-oriented sitcom needed to invoke psychological processes known to reduce prejudice. The authors felt that both series had characters that were likable and relatable and were engaged in regular activities of daily living. The Friends series featured characters who were predominantly White, cis-gendered, and straight. The Little Mosque series portrayed the experience of Western Muslims and Arabs as they lived in a small Canadian town. This study involved assessment across three waves: baseline (before watching the assigned episodes), post1 (immediately after watching the episodes), and post2 (completed 4-6 weeks after watching the episodes).

The study used \emph{feelings and liking thermometers}, rating their feelings and liking toward 10 different groups of people on a 0 to 100 sliding scale (with higher scores reflecting greater liking and positive feelings). For the purpose of this analysis, the ratings of attitudes toward White people and attitudes toward Arabs/Muslims were used. A third metric was introduced by subtracting the attitudes towards Arabs/Muslims from the attitudes toward Whites. Higher scores indicated more positive attitudes toward Whites where as low scores indicated no difference in attitudes. To recap, there were three potential dependent variables, all continuously scaled:

\begin{itemize}
\tightlist
\item
  \emph{AttWhite}: attitudes toward White people; higher scores reflect greater liking
\item
  \emph{AttArab}: attitudes toward Arab people; higher scores reflect greater liking
\item
  \emph{Diff}: the difference between AttWhite and AttArab; higher scores reflect a greater liking for White people
\end{itemize}

With random assignment, nearly equal cell sizes, a condition with two levels (Friends, Little Mosque), and three waves (baseline, post1, post2), this is perfect for mixed design ANOVA.

\begin{figure}
\centering
\includegraphics{images/mixed/Murrar_design.jpg}
\caption{Image of the design for the Murrar and Brauer (2018) study}
\end{figure}

\hypertarget{simulating-the-data-from-the-journal-article}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article}}

Below is the code I have used to simulate the data. The simulation includes two dependent variables (AttWhite, AttArab), Wave (baseline, post1, post2), and COND (condition; Friends, Little\_Mosque). There is also a caseID (repeated three times across the three waves) and rowID (giving each observation within each case an ID). This creates the long-file, where each person has 3 rows of data representing baseline, post1, and post2. You can use this simulation for two of the three practice suggestions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# change this to any different number (and rerun the simulation) to}
\CommentTok{\# rework the chapter problem}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210813}\NormalTok{)}
\NormalTok{AttWhite }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{76.79}\NormalTok{, }\AttributeTok{sd =} \FloatTok{18.55}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.37}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{18.99}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{77.47}\NormalTok{, }\AttributeTok{sd =} \FloatTok{18.95}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.81}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{19.29}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{77.79}\NormalTok{, }\AttributeTok{sd =} \FloatTok{17.25}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.89}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{19.44}\NormalTok{)), }\DecValTok{3}\NormalTok{)  }\CommentTok{\#sample size, M and SD for each cell; this will put it in a long file}
\CommentTok{\# set upper bound for variable}
\NormalTok{AttWhite[AttWhite }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{100}
\CommentTok{\# set lower bound for variable}
\NormalTok{AttWhite[AttWhite }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{AttArab }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.11}\NormalTok{, }\AttributeTok{sd =} \FloatTok{20.97}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.37}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{20.03}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.16}\NormalTok{, }\AttributeTok{sd =} \FloatTok{21.64}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{70.52}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{18.55}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{65.29}\NormalTok{, }\AttributeTok{sd =} \FloatTok{19.76}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{70.3}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{17.98}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\CommentTok{\# set upper bound for variable}
\NormalTok{AttArab[AttArab }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{100}
\CommentTok{\# set lower bound for variable}
\NormalTok{AttArab[AttArab }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{rowID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{579}\NormalTok{))}
\NormalTok{caseID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{((}\DecValTok{1}\SpecialCharTok{:}\DecValTok{193}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\DecValTok{193}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Post1"}\NormalTok{, }\DecValTok{193}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Post2"}\NormalTok{, }\DecValTok{193}\NormalTok{))}
\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{Murrar\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(rowID, caseID, Wave, COND, AttArab, AttWhite)}
\end{Highlighting}
\end{Shaded}

Let's check the structure. We want

\begin{itemize}
\tightlist
\item
  rowID and caseID to be unordered factors
\item
  Wave and COND to be ordered factors
\item
  AttArab and AttWhite to be numerical
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Murrar\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   579 obs. of  6 variables:
 $ rowID   : Factor w/ 579 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ caseID  : int  1 2 3 4 5 6 7 8 9 10 ...
 $ Wave    : chr  "Baseline" "Baseline" "Baseline" "Baseline" ...
 $ COND    : chr  "Friends" "Friends" "Friends" "Friends" ...
 $ AttArab : num  74.3 55.8 33.3 66.3 71 ...
 $ AttWhite: num  100 79 75.9 68.2 100 ...
\end{verbatim}

The script below changes

\begin{itemize}
\tightlist
\item
  caseID from integer to factor
\item
  Wave and COND from factor to ordered factors

  \begin{itemize}
  \tightlist
  \item
    It makes sense to order Friends and LittleMosque, since we believe that LittleMosque contains prejudice-reducing properties
  \end{itemize}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# make caseID a factor}
\NormalTok{Murrar\_df[, }\StringTok{"caseID"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Murrar\_df[, }\StringTok{"caseID"}\NormalTok{])}
\CommentTok{\# make Wave an ordered factor}
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Murrar\_df}\SpecialCharTok{$}\NormalTok{Wave, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\StringTok{"Post1"}\NormalTok{,}
    \StringTok{"Post2"}\NormalTok{))}
\CommentTok{\# make COND an ordered factor}
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Murrar\_df}\SpecialCharTok{$}\NormalTok{COND, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\StringTok{"LittleMosque"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Let's check the structure again.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Murrar\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   579 obs. of  6 variables:
 $ rowID   : Factor w/ 579 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ caseID  : Factor w/ 193 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Wave    : Factor w/ 3 levels "Baseline","Post1",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ COND    : Factor w/ 2 levels "Friends","LittleMosque": 1 1 1 1 1 1 1 1 1 1 ...
 $ AttArab : num  74.3 55.8 33.3 66.3 71 ...
 $ AttWhite: num  100 79 75.9 68.2 100 ...
\end{verbatim}

A key dependent variable in the Murrar and Brauer \citep{murrar_entertainment-education_2018} article is \emph{attitude difference.} Specifically, the attitudes toward Arabs score was subtracted from the attitudes toward Whites scores. Higher attitude difference indicate a greater preference for Whites. Let's create that variable, here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{Diff }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_df}\SpecialCharTok{$}\NormalTok{AttWhite }\SpecialCharTok{{-}}\NormalTok{ Murrar\_df}\SpecialCharTok{$}\NormalTok{AttArab}
\end{Highlighting}
\end{Shaded}

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R. This is what I would do.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Murrar\_df, \textquotesingle{}Murrar\_RDS.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file Murrar\_df \textless{}{-} readRDS(\textquotesingle{}Murrar\_RDS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv (``Excel lite'') or .rds (R object) file. This is not a necessary step.

The code for .csv will likely lose the formatting (i.e., stripping Wave and COND of their ordered factors), but it is easy to view in Excel.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Murrar\_df,}
\CommentTok{\# file=\textquotesingle{}DiffCSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file Murrar\_df \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}DiffCSV.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{working-the-mixed-design-anova-with-r-packages}{%
\section{Working the Mixed Design ANOVA with R packages}\label{working-the-mixed-design-anova-with-r-packages}}

\hypertarget{exploring-data-and-testing-assumptions}{%
\subsection{Exploring data and testing assumptions}\label{exploring-data-and-testing-assumptions}}

We begin the 2x3 mixed design ANOVA with a preliminary exploration of the data and testing of the assumptions. Here's where we are on the workflow:

\begin{figure}
\centering
\includegraphics{images/mixed/mx_Assumptions.jpg}
\caption{Image of the workflow showing that we are on the ``Evaluating assumptions'' portion}
\end{figure}

First, let's examine the overal descriptive statistics.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describe}\NormalTok{(Murrar\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         vars   n   mean     sd median trimmed    mad    min    max  range
rowID*      1 579 290.00 167.29 290.00  290.00 214.98   1.00 579.00 578.00
caseID*     2 579  97.00  55.76  97.00   97.00  71.16   1.00 193.00 192.00
Wave*       3 579   2.00   0.82   2.00    2.00   1.48   1.00   3.00   2.00
COND*       4 579   1.49   0.50   1.00    1.49   0.00   1.00   2.00   1.00
AttArab     5 579  66.84  19.75  68.04   67.64  20.38   6.14 100.00  93.86
AttWhite    6 579  75.31  17.02  76.72   76.19  18.50  23.03 100.00  76.97
Diff        7 579   8.47  26.33   8.65    8.50  25.73 -71.51  90.74 162.25
          skew kurtosis   se
rowID*    0.00    -1.21 6.95
caseID*   0.00    -1.21 2.32
Wave*     0.00    -1.51 0.03
COND*     0.03    -2.00 0.02
AttArab  -0.39    -0.16 0.82
AttWhite -0.37    -0.51 0.71
Diff      0.03     0.00 1.09
\end{verbatim}

Our analysis will use the difference score (Diff) as the dependent variable. Let's look at this variable in its combinations of wave and condition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(Diff }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave }\SpecialCharTok{+}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ Murrar\_df, }\AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item   group1       group2 vars  n       mean       sd median    trimmed
X11    1 Baseline      Friends    1 98  9.3064898 23.90867  8.804  8.9906625
X12    2    Post1      Friends    1 98 15.9261327 26.41789 16.191 16.2309375
X13    3    Post2      Friends    1 98 11.9540102 23.33602 10.882 11.8340000
X14    4 Baseline LittleMosque    1 95  9.7331158 30.51895 10.797 10.5544156
X15    5    Post1 LittleMosque    1 95 -0.1486632 26.96858 -1.280 -0.9402727
X16    6    Post2 LittleMosque    1 95  3.6704737 23.66524  1.860  3.7857403
         mad     min    max   range        skew    kurtosis       se
X11 24.48143 -47.342 72.565 119.907  0.17600603 -0.35619536 2.415140
X12 29.77135 -42.598 82.288 124.886 -0.04613379 -0.57456759 2.668609
X13 24.59189 -46.528 75.014 121.542  0.13240007  0.06485450 2.357294
X14 30.90480 -71.510 90.737 162.247 -0.20135358 -0.03033805 3.131178
X15 23.93213 -65.259 83.367 148.626  0.32819576  0.54919109 2.766918
X16 25.26054 -53.856 55.264 109.120 -0.06475209 -0.42366384 2.428002
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note. Recently my students and I have been having intermittent}
\CommentTok{\# struggles with the describeBy function in the psych package. We}
\CommentTok{\# have noticed that it is problematic when using .rds files and when}
\CommentTok{\# using data directly imported from Qualtrics. If you are having}
\CommentTok{\# similar difficulties, try uploading the .csv file and making the}
\CommentTok{\# appropriate formatting changes.}
\end{Highlighting}
\end{Shaded}

First we inspect the means. We see that the baseline scores for the Friends and Little Mosque conditions are similar. However, the post1 and post2 difference scores (i.e., difference in attitudes toward White and Arab individuals, where higher scores indicate more favorable ratings of White individuals) are higher in the Friends condition than in the Little Mosque condition.

\hypertarget{assumption-of-normality}{%
\subsubsection{Assumption of Normality}\label{assumption-of-normality}}

We can use this output to evaluate the distributional characteristics of the dependent variable. Recall that mixed design ANOVA assumes a normal distribution.

Our values of skew and kurtosis are well within the limits \citep{kline_principles_2016} of a normal distribution.

\begin{itemize}
\tightlist
\item
  skew: \textless{} 3; the highest skew value in our data is 0.32
\item
  kurtosis: extreme values are between 8 and 20; the highest kurtosis value in our data is .55
\end{itemize}

The boxplot is one common way for identifying outliers. The boxplot uses the median and the lower (25th percentile) and upper (75th percentile) quartiles. The difference bewteen Q3 and Q1 is the \emph{interquartile range} (IQR).

You'll notice that as we are creating these boxplors we are saving them as objects. This is not necessary to produce the graph. However, we will combine the object with other data, later, to embed results if the anlaysis in our figures.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CNDwiWV }\OtherTok{\textless{}{-}}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Murrar\_df, }\AttributeTok{x =} \StringTok{"Wave"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diff"}\NormalTok{, }\AttributeTok{color =} \StringTok{"COND"}\NormalTok{,}
    \AttributeTok{palette =} \StringTok{"jco"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Assessment Wave"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Difference in Attitudes towards Whites and Arabs"}\NormalTok{,}
\NormalTok{    )}
\NormalTok{CNDwiWV}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-389-1.pdf}
The distributions look relatively normal with the mean well-centered. Given that we simulated the data from means and standard deviations, this is somewhat expected. This boxplot also provides a glimpse of the patterns in our data. That is, the means are quite similar at baseline; in the post intervention waves we see greater difference scores for the Friends condition.

Let's reconfigure the data by putting the wave on the X axis. Plotting it both ways (i.e.,swapping roles of predictor and moderator) can help us get a sense of what is happening.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WVwiCND }\OtherTok{\textless{}{-}}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{ggboxplot}\NormalTok{(Murrar\_df, }\AttributeTok{x =} \StringTok{"COND"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Diff"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Wave"}\NormalTok{,}
    \AttributeTok{palette =} \StringTok{"jco"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Treatment Condition"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Difference in Attitudes towards Whites and Arabs"}\NormalTok{)}
\NormalTok{WVwiCND}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-390-1.pdf}
Outliers are generally identified when values fall outside these lower and upper boundaries. In the short formulas below, IQR is the \emph{interquartile range} (i.e., the middle 50\%, the distance of the box):

\begin{itemize}
\tightlist
\item
  Q1 - 1.5xIQR
\item
  Q3 + 1.5xIQR
\end{itemize}

Extreme values occur when values fall outside these boundaries:

\begin{itemize}
\tightlist
\item
  Q1 - 3xIQR
\item
  Q3 + 3xIQR
\end{itemize}

Using the \emph{rstatix::identify\_outliers} function we can look for outliers in the dependent variable, doubly grouped by our predictor variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Wave, COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{identify\_outliers}\NormalTok{(Diff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 9
  Wave     COND        rowID caseID AttArab AttWhite  Diff is.outlier is.extreme
  <fct>    <fct>       <fct> <fct>    <dbl>    <dbl> <dbl> <lgl>      <lgl>     
1 Baseline LittleMosq~ 107   107      100       28.5 -71.5 TRUE       FALSE     
2 Post1    LittleMosq~ 297   104       16.6    100    83.4 TRUE       FALSE     
3 Post1    LittleMosq~ 315   122       26.8    100    73.2 TRUE       FALSE     
4 Post1    LittleMosq~ 337   144       97.4     32.2 -65.3 TRUE       FALSE     
\end{verbatim}

While we have some outliers (where ``is.outlier'' = ``TRUE''), none are extreme (where ``is.outlier'' = ``FALSE''). We'll keep these in mind as we continue to evaluate the data.

If I had extreme outliers, I would individually inspect them. Especially if something looked awry (e.g., erratic responding extreme scores across variables) I might consider deleting them.

Next we can use the \emph{rstatix::shapiro\_test()} to see if any of the distributions of the dependent variable (Diff) within each wave-by-condition combinations differs significantly from a normal distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Wave, COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{shapiro\_test}\NormalTok{(Diff)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  Wave     COND         variable statistic     p
  <fct>    <fct>        <chr>        <dbl> <dbl>
1 Baseline Friends      Diff         0.993 0.915
2 Baseline LittleMosque Diff         0.993 0.923
3 Post1    Friends      Diff         0.992 0.798
4 Post1    LittleMosque Diff         0.986 0.437
5 Post2    Friends      Diff         0.990 0.708
6 Post2    LittleMosque Diff         0.991 0.762
\end{verbatim}

The Shapiro Wilks test suggests that distribution in each of our cells is not significantly different than normal. We can further visualize this with QQ plots.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggqqplot}\NormalTok{(Murrar\_df, }\StringTok{"Diff"}\NormalTok{, }\AttributeTok{ggtheme =} \FunctionTok{theme\_bw}\NormalTok{()) }\SpecialCharTok{+} \FunctionTok{facet\_grid}\NormalTok{(Wave }\SpecialCharTok{\textasciitilde{}}
\NormalTok{    COND)}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-393-1.pdf}

\hypertarget{homogeneity-of-variance-assumption}{%
\subsubsection{Homogeneity of variance assumption}\label{homogeneity-of-variance-assumption}}

Because there is a between-subjects variable, we need need to evaluate the homogeneity of variance assumption. As before, we can use the Levene's test with the \emph{rstatix::levene\_test()} function. Considering each of the comparisons of condition within wave, there is no instance where we violate the assumption.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Wave) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{levene\_test}\NormalTok{(Diff }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 5
  Wave       df1   df2 statistic      p
  <fct>    <int> <int>     <dbl>  <dbl>
1 Baseline     1   191     3.97  0.0477
2 Post1        1   191     0.141 0.708 
3 Post2        1   191     0.107 0.744 
\end{verbatim}

Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline (\emph{F} {[}1, 191{]} = 3.973, \emph{p} = .047). However, there was no indication of assumption violation at post1 (\emph{F} {[}1, 191{]} = 0.141, \emph{p} = .708), and post2 (\emph{F} {[}1, 191{]} = 0.107, \emph{p} = .743) waves of the design.

\hypertarget{assumption-of-homogeneity-of-covariance-matrices}{%
\subsubsection{Assumption of homogeneity of covariance matrices}\label{assumption-of-homogeneity-of-covariance-matrices}}

In this multivariate sample, the Box's M test evaluates if two or more covariance matrices are homogeneous. Like other tests of assumptions, we want a non-significant test result (i.e., where \emph{p} \textgreater{} .05). Box's M has some disavantages. Box's M has low power in small sample sizes and is overly sensitive in large sample sizes. We would unlikely make a decision about our data with Box's M alone. Rather, we consider it along with our dashboard of diagnostic screeners.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstatix}\SpecialCharTok{::}\FunctionTok{box\_m}\NormalTok{(Murrar\_df[, }\StringTok{"Diff"}\NormalTok{, }\AttributeTok{drop =} \ConstantTok{FALSE}\NormalTok{], Murrar\_df}\SpecialCharTok{$}\NormalTok{COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
  statistic p.value parameter method                                            
      <dbl>   <dbl>     <dbl> <chr>                                             
1      3.21  0.0732         1 Box's M-test for Homogeneity of Covariance Matric~
\end{verbatim}

None-the-less, Box's M indicated no violation of the homogeneity of covariance matrices assumption (\emph{M} = 3.209, \emph{p} = .073)

\hypertarget{apa-style-writeup-of-assumptions}{%
\subsubsection{APA style writeup of assumptions}\label{apa-style-writeup-of-assumptions}}

At this stage we are ready to draft the portion of the APA style writeup that evaluates the assumptions.

\begin{quote}
Mixed design ANOVA has a number of assumptions related to both the within-subjects and between-subjects elements. Data are expected to be normally distributed at each level of design. Visual inspection of boxplots for each wave of the design, assisted by the \emph{rstatix::identify\_outliers()} function (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated some outliers, but none at the extreme level. There was no evidence of skew (all values were at or below the absolute value of 0.32) or kurtosis (all values were below the absolute value of .57; \citep{kline_principles_2016}). Additionally, the Shapiro-Wilk tests applied at each level of the design were non-significant. Because of the between-subjects aspect of the design, the homogeneity of variance assumption was evaluated. Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline \emph{F} {[}1, 191{]} = 3.973, \emph{p} = .047). However, there was no indication of assumption violation at post1 (\emph{F} {[}1, 191{]} = 0.141, \emph{p} = .708), and post2 (\emph{F} {[}1, 191{]} = 0.107, \emph{p} = .743) waves of the design. Further, Box's M-test (\emph{M} = 3.209, \emph{p} = .073) indicated no violation of the homogeneity of covariance matrices. \emph{LATER WE WILL ADD INFORMATION ABOUT THE SPHERICITY ASSUMPTION.}
\end{quote}

\hypertarget{omnibus-anova}{%
\subsection{Omnibus ANOVA}\label{omnibus-anova}}

Having evaluated the assumptions (excepting sphericity) we are ready to move to the evaluation of the omnibus ANOVA. This next step produces both the omnibus test as well as testing the sphericity assumption. Conceptually, evaluating the sphericity assumption precedes the omnibus; procedurally these are evaluated simultaneously. The figure also reflects that decisions related to follow-up are dependent upon the significance of the main and omnibus effects.

\begin{figure}
\centering
\includegraphics{images/mixed/mx_omnibus.jpg}
\caption{Image of the workflow showing that we at the ``Compute the Omnibus ANOVA'' step}
\end{figure}

The \emph{rstatix} package is a wrapper for the \emph{car} package. Authors of \emph{wrappers} attempt to streamline a more complex program to simplify the input needed and maximize the output produced for the typical use-cases.

If we are ever confused about a function, we can place a question mark in front of it. It will summons information and, if the package is in our library, let us know to which package it belongs and open the instructions that are embedded in R/R Studio.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#?anova\_test}
\end{Highlighting}
\end{Shaded}

In the code below the identification of the data, DV, between, and within variables are likely to be intuitive. The within-subjects identifier (\emph{wid}) is the person-level ID that assists the statistic in controlling for the dependency introduced by the repeated-measures factor.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Murrar\_df is our df, Diff is our df, wid is the caseID between is}
\CommentTok{\# the between{-}subjects variable, within is the within subjects}
\CommentTok{\# variable}
\NormalTok{Diff\_2way }\OtherTok{\textless{}{-}}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Murrar\_df, }\AttributeTok{dv =}\NormalTok{ Diff, }\AttributeTok{wid =}\NormalTok{ caseID,}
    \AttributeTok{between =}\NormalTok{ COND, }\AttributeTok{within =}\NormalTok{ Wave)}
\NormalTok{Diff\_2way}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA Table (type III tests)

$ANOVA
     Effect DFn DFd      F        p p<.05      ges
1      COND   1 191 13.149 0.000369     * 0.023000
2      Wave   2 382  0.273 0.761000       0.000933
3 COND:Wave   2 382  5.008 0.007000     * 0.017000

$`Mauchly's Test for Sphericity`
     Effect    W     p p<.05
1      Wave 0.99 0.369      
2 COND:Wave 0.99 0.369      

$`Sphericity Corrections`
     Effect  GGe       DF[GG] p[GG] p[GG]<.05 HFe DF[HF] p[HF] p[HF]<.05
1      Wave 0.99 1.98, 378.06 0.759             1 2, 382 0.761          
2 COND:Wave 0.99 1.98, 378.06 0.007         *   1 2, 382 0.007         *
\end{verbatim}

\hypertarget{checking-the-sphericity-assumption}{%
\subsubsection{Checking the sphericity assumption}\label{checking-the-sphericity-assumption}}

First, we check Mauchly's test for the main and interaction effects that involve the repeated measures variable.

\begin{itemize}
\tightlist
\item
  main effect for Wave: \emph{W} = .99, \emph{p} = .369
\item
  main effect for Wave: \emph{W} = .99, \emph{p} = .369
\end{itemize}

We will be able to add this statement to our assumptions write-up:

\begin{quote}
Mauchly's test indicated no violation of the sphericity assumption for the main effect (\emph{W} = 0.99, \emph{p} = .369) and interaction effect (\emph{W} = 0.99, \emph{p} = .369).
\end{quote}

If the \emph{p} vaue associated with Mauchly's test had been less than .05, we could have used one of the two options (Greenhouse Geyser/GGe or Huynh-Feldt/HFe). In each of these an epsilon value provides an adjustment to the degrees of freedom used in the estimation of the \emph{p} value. There is also an option to use a multivariate approach when ANOVA designs include a repeated measures factor.

\textbf{Omnibus Results}

\begin{quote}
Results of the omnibus ANOVA indicated a significant main effect for condition (\emph{F}{[}1, 191{]} = 13.149, \emph{p} \textless{} .001, \(\eta^{2}\) = 0.023), a non-significant main effect for wave (\emph{F}{[}2, 382{]} = 0.273, \emph{p} = .761, \(\eta^{2}\) = 0.001), and a significant interaction effect (\emph{F}{[}2, 382{]} = 5.008, \emph{p} = 0.007, \(\eta^{2}\) = 0.017). We note that according to Cohen et al.'s \citep{cohen_applied_2003} guidelines, the effect size for the interaction term is small.
\end{quote}

In the output, the column labeled ``ges'' provides the value for the effect size, \(\eta^{2}\). Recall that \emph{eta-squared} is one of the most commonly used measures of effect. It refers to the proportion of variability in the dependent variable/outcome that can be explained in terms of the independent variable/predictor. Traditional interpretive values are similar to the Pearson's \emph{r}:

\begin{itemize}
\tightlist
\item
  0 = no relationship
\item
  .02 = small
\item
  .13 = medium
\item
  .26 = large
\item
  1 = a perfect (one-to-one) correspondence
\end{itemize}

The effect size for our interaction effect (0.017) is small.

With a significant interaction effect, we would focus on interpreting one or both of the simple main effects. Let's first look at the simple main effect of condition within wave option.

\hypertarget{simple-main-effect-of-condition-within-wave}{%
\subsection{Simple main effect of condition within wave}\label{simple-main-effect-of-condition-within-wave}}

The figure reflects our path in the workflow. In the presence of a significant interaction effect we could choose from a variety of follow-up tests.

\begin{figure}
\centering
\includegraphics{images/mixed/mx_SimpleMainA.jpg}
\caption{Image of the workflow showing that we are at the ``Simple Main Effects for Factor A within all levels of Factor B'' step}
\end{figure}

If we take this option we follow up with 3 one-way ANOVAs. When we look at condition within wave, our ANOVAs will look like this:

\begin{itemize}
\tightlist
\item
  comparison of Friends and Little Mosque within the baseline wave
\item
  comparison of Friends and Little Mosque within the post1 wave
\item
  comparison of Friends and Little Mosque within the post2 wave
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# crate an object to hold the output the group\_by function is what}
\CommentTok{\# results in three, one{-}way ANOVAs for each of the waves, separately}
\CommentTok{\# the between = Cond means that each level of cond will be compared}
\CommentTok{\# method {-} \textquotesingle{}bonferroni\textquotesingle{} gets us both the standard and adjusted p}
\CommentTok{\# values}
\NormalTok{SimpleWave }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(Wave) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(}\AttributeTok{dv =}\NormalTok{ Diff, }\AttributeTok{wid =}\NormalTok{ caseID, }\AttributeTok{between =}\NormalTok{ COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{get\_anova\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{adjust\_pvalue}\NormalTok{(}\AttributeTok{method =} \StringTok{"bonferroni"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coefficient covariances computed by hccm()
Coefficient covariances computed by hccm()
Coefficient covariances computed by hccm()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SimpleWave}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 9
  Wave     Effect   DFn   DFd      F         p `p<.05`       ges    p.adj
* <fct>    <chr>  <dbl> <dbl>  <dbl>     <dbl> <chr>       <dbl>    <dbl>
1 Baseline COND       1   191  0.012 0.914     ""      0.0000614 1       
2 Post1    COND       1   191 17.5   0.0000438 "*"     0.084     0.000131
3 Post2    COND       1   191  5.99  0.015     "*"     0.03      0.045   
\end{verbatim}

In prior lectures we have adjusted the \emph{p} value against which we compare the resulting \emph{p} value. When we specify ``bonferroni'' on the \emph{adjust\_pvalue()} command, the algorithm adjusts the reported \emph{p} value for us. We can see the unadjusted \emph{p} value in the ``p p\textless.05'' column and the Bonferroni adjustment in the ``p.adj'' column.

I think that it will be easiest for us to interpret this simple main effect as the traditional \emph{p} \textless{} .05 and then apply the restrictions to the alpha at the next level of analysis. In this particular instance, we would have statistically significant differences somewhere between the Friends and Little Mosque conditions for both the Post (\emph{p} = .027) and Post2 (\emph{p} = .010) waves.

F strings:

\begin{itemize}
\tightlist
\item
  Pre: \emph{F} (1, 191) = 0.012, \emph{p} = .914, \(\eta^{2}\) = 0.000 (the effect size is zero)
\item
  Post: \emph{F} (1, 191) = 17.497, \emph{p} \textless{} .001, \(\eta^{2}\) = 0.084 (approaching a moderate effect size)
\item
  Post2: \emph{F} (1, 191) = 5.994, \emph{p} = .015, \(\eta^{2}\) = 0.030 (a small effect size)
\end{itemize}

Recall, interpretation for the eta-squared are .02 \textasciitilde{} small, .13 \textasciitilde{} medium, and \textgreater.26 \textasciitilde{} large

Because there are only two levels (Friends, Little Mosque) within each wave (baseline, post1, post2), this simple effects analysis is complete with the three pairwise comparisons.

As always, we have several choices about how to manage Type I error. In a circumstance when the analysis of simple main effects (condition within wave) includes only three pairwise comparisons, we can use the LSD method \citep{green_using_2014}. This means that we can we can leave the alpha at 0.05. If we were to use a traditional Bonferroni, we would use \(\alpha\) = .017 (.05/3). Although the more restrictive Bonferroni criteria comes close, in both cases we would still have one non-significant(baseline) and two significant (post1, post2) simple main effects.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{.}\DecValTok{05}\SpecialCharTok{/}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01666667
\end{verbatim}

If we were to write up this result:

\begin{quote}
We followed the significant interaction effect with an evaluation of simple main effects of condition within wave. Because there were only three comparisons following the omnibus evaluation, we used the LSD method to control for Type I error and left the alpha at .05 \citep{green_using_2014}. There was a non-statistically significant difference between conditions at baseline: \emph{F} (1, 191) = 0.012, \emph{p} = .914, \(\eta^{2}\) = 0.000. However other were statistically significant differences at post1 (\emph{F} {[}1, 191{]} = 17.497, \emph{p} \textless{} .001, \(\eta^{2}\) = 0.084) and post2 (\emph{F}{[}1, 191{]} = 5.994, \emph{p} = .015, \(\eta^{2}\) = 0.030). We note that the effect size at post1 approached a moderate size; the effect size at post2 was small.
\end{quote}

\hypertarget{simple-main-effect-of-wave-within-condition}{%
\subsection{Simple main effect of wave within condition}\label{simple-main-effect-of-wave-within-condition}}

Alternatively, we could evaluate the simple main effect of wave within condition. The figure reflects our path along the workflow.

\begin{figure}
\centering
\includegraphics{images/mixed/mx_SimplemainB.jpg}
\caption{Image of the workflow showing that we at the ``Simple Main Effects for Factor B within all levels of Factor A'' step}
\end{figure}

If we conducted this alternative we would start with three one-way ANOVAs and then follow each of those with pairwise comparisons. First, the one-way repeated measures ANOVAs:

\begin{itemize}
\tightlist
\item
  comparison of baseline, post1, and post2 within the Friends condition
\item
  comparison of baseline, post1, and post2 within the Little Mosque condition
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SimpleCond }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(}\AttributeTok{dv =}\NormalTok{ Diff, }\AttributeTok{wid =}\NormalTok{ caseID, }\AttributeTok{within =}\NormalTok{ Wave) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{get\_anova\_table}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{adjust\_pvalue}\NormalTok{(}\AttributeTok{method =} \StringTok{"bonferroni"}\NormalTok{)}
\NormalTok{SimpleCond}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 9
  COND         Effect   DFn   DFd     F     p `p<.05`   ges p.adj
  <fct>        <chr>  <dbl> <dbl> <dbl> <dbl> <chr>   <dbl> <dbl>
1 Friends      Wave       2   194  1.76 0.175 ""      0.012 0.35 
2 LittleMosque Wave       2   188  3.39 0.036 "*"     0.022 0.072
\end{verbatim}

Below are the \emph{F} strings for the one-way ANOVAs the followed the omnibus, mixed design, ANOVA:

\begin{itemize}
\tightlist
\item
  Friends: \emph{F} (2, 194) = 1.759, \emph{p} = 0.175, \(\eta^{2}\) = 0.012 (effect size indicates no relationship)
\item
  Little Mosque: \emph{F} (2, 188) = 3.392, \emph{p} = 0.036, \(\eta^{2}\) = 0.072 (a small-to-moderate effect size)
\end{itemize}

Because each of these one-way ANOVAs has three levels, we need to follow with pairwise comparisons. However, we only need to conduct them for the Little Mosque condition. As you can see we generally work our way down to comparing chunks to each other to find the source(s) of significant differences.

You will notice that we are saving the results of the pairwise comparisons as an object. This is not necessary, however we can use this object in combination with the boxplot we created earlier to embed results of our analysis in the resulting figure.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwcWVwiGP }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{group\_by}\NormalTok{(COND) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{pairwise\_t\_test}\NormalTok{(Diff }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{,}
        \AttributeTok{p.adjust.method =} \StringTok{"bonferroni"}\NormalTok{)  }\CommentTok{\#\%\textgreater{}\%}
\CommentTok{\# select({-}df, {-}statistic, {-}p) \# Remove details}
\NormalTok{pwcWVwiGP}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 16
  COND   estimate .y.   group1 group2    n1    n2 statistic     p    df conf.low
* <fct>     <dbl> <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl> <dbl>    <dbl>
1 Frien~    -6.62 Diff  Basel~ Post1     98    98    -1.80  0.075    97   -13.9 
2 Frien~    -2.65 Diff  Basel~ Post2     98    98    -0.793 0.43     97    -9.27
3 Frien~     3.97 Diff  Post1  Post2     98    98     1.09  0.276    97    -3.23
4 Littl~     9.88 Diff  Basel~ Post1     95    95     2.45  0.016    94     1.86
5 Littl~     6.06 Diff  Basel~ Post2     95    95     1.62  0.108    94    -1.36
6 Littl~    -3.82 Diff  Post1  Post2     95    95    -1.03  0.304    94   -11.2 
# ... with 5 more variables: conf.high <dbl>, method <chr>, alternative <chr>,
#   p.adj <dbl>, p.adj.signif <chr>
\end{verbatim}

At this point, we likely need to control for Type I error. Why? We have already conducted two one-way ANOVAs after the omnibus. Now we will conduct three more pairwise comparisons in the Little Mosque condition. I would divide .05/3 and interpret these pairwise comparisons with an alpha of .017.

We find a significant difference between baseline and post1 (\emph{t}{[}95{]} = 2.447, \emph{p} = .016), but non-significant differences between baseline and post2 (\emph{t}{[}95{]} = 1.621, \emph{p} = .108) and post1 and post2 (\emph{t}{[}95{]} = -1.034, \emph{p} = .304)

If we were to write up this result:

\begin{quote}
We followed the significant interaction effect with an evaluation of simple main effects of wave within condition. There were non-significant difference within the the Friends condition (\emph{F} {[}2, 194{]} = 1.759, \emph{p} = 0.175, \(\eta^{2}\) = 0.012). There were significant differences with an effect size indicating a small-to-moderate effect in the Little Mosque condition (\emph{F} {[}2, 188{]} = 3.392, \emph{p} = 0.036, \(\eta^{2}\) = 0.072). We followed up the significant simple main effect for the Little Mosque condition with pairwiwse comparisons. At this level we controlled for Type I error by dividing alpha (.05) by the number of paired comparisons (3). We found a significant difference between baseline and post1 (\emph{t}{[}95{]} = 2.447, \emph{p} = .016), but non-significant differences between baseline and post2 (\emph{t}{[}95{]} = 1.621, \emph{p} = .108) and post1 and post2 (\emph{t}{[}95{]} = -1.034, \emph{p} = .304).
\end{quote}

\hypertarget{if-we-only-had-a-main-effect}{%
\subsection{If we only had a main effect}\label{if-we-only-had-a-main-effect}}

When there is an interaction effect, we do not interpret main effects. This is because the solution is more complicated than a main effect could explain. It is important, though, to know how to interpret a main effect. We would do this if we had one or more significant main effects and no interaction effect.

The figure shows our place on the workflow.

\begin{figure}
\centering
\includegraphics{images/mixed/mx_main.jpg}
\caption{Image of a workflow showing that we at the ``Main effects only'' step}
\end{figure}

If we had not had a significant interaction, but did have a significant main effect for wave, we could have conducted pairwise comparisons for pre, post1, and post2 -- collapsing across condition.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_df }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{pairwise\_t\_test}\NormalTok{(Diff }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{paired =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{p.adjust.method =} \StringTok{"bonferroni"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 10
  .y.   group1   group2    n1    n2 statistic    df     p p.adj p.adj.signif
* <chr> <chr>    <chr>  <int> <int>     <dbl> <dbl> <dbl> <dbl> <chr>       
1 Diff  Baseline Post1    193   193    0.539    192 0.59      1 ns          
2 Diff  Baseline Post2    193   193    0.652    192 0.515     1 ns          
3 Diff  Post1    Post2    193   193    0.0528   192 0.958     1 ns          
\end{verbatim}

Ignoring condition (Friends, Little Mosque), we do not see changes across time. This is not surprising since the \emph{F} test for the main effect was also non-significant (\emph{F}{[}2, 382{]} = 0.273, \emph{p} = .761, \(\eta^{2}\) = 0.0014),

If we had had a non-significant interaction effect but a significant main effect for condition, there would have been no need for further follow-up. Why? Because there were only two levels the significant main effect already tells us there were statistically significant differences between Friends and Little Mosque (\emph{F}{[}1, 191{]} = 13.149, \emph{p} \textless{} .001, \(\eta^{2}\) = 0.023).

\hypertarget{apa-style-write-up-of-the-results}{%
\subsection{APA Style Write-up of the Results}\label{apa-style-write-up-of-the-results}}

Recall that earlier in this lesson we save objects for the boxplots (e.g., CNDwiWV) and the pairwise comparisons (e.g., pwcVWwiGP). The script below use the objects created from omnibus ANOVA and the pairwise comparisons to add results to the figure. Depending on where you are presenting your results, these may be useful.

This first figure would pair well if you report the simple main effect of condition within wave.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwcWVwiGP }\OtherTok{\textless{}{-}}\NormalTok{ pwcWVwiGP }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{add\_xy\_position}\NormalTok{(}\AttributeTok{x =} \StringTok{"Wave"}\NormalTok{)}
\NormalTok{CNDwiWV }\SpecialCharTok{+}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_pvalue\_manual}\NormalTok{(pwcWVwiGP, }\AttributeTok{tip.length =} \DecValTok{0}\NormalTok{, }\AttributeTok{hide.ns =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_test\_label}\NormalTok{(Diff\_2way, }\AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{),}
        \AttributeTok{caption =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_pwc\_label}\NormalTok{(pwcWVwiGP))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-403-1.pdf}

This second figure would pair well with the results that reported the simple main effect of wave within condition.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# pwcWVwiGP were my pairwise comparisons for the simple effect}
\CommentTok{\# UE\_2way was my omnibus ANOVA object WVwiCND was the boxplot before}
\CommentTok{\# I did the ANOVA}
\NormalTok{pwcWVwiGP }\OtherTok{\textless{}{-}}\NormalTok{ pwcWVwiGP }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{add\_xy\_position}\NormalTok{(}\AttributeTok{x =} \StringTok{"Wave"}\NormalTok{)}
\NormalTok{WVwiCND }\SpecialCharTok{+}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_pvalue\_manual}\NormalTok{(pwcWVwiGP, }\AttributeTok{tip.length =} \DecValTok{0}\NormalTok{, }\AttributeTok{hide.ns =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_test\_label}\NormalTok{(Diff\_2way, }\AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{),}
        \AttributeTok{caption =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_pwc\_label}\NormalTok{(pwcWVwiGP))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-404-1.pdf}

\hypertarget{results}{%
\subsubsection{Results}\label{results}}

\begin{quote}
We conducted a 2 X 3 mixed design ANOVA to evaluate the combined effects of condition (Friends and Little Mosque) and wave (baseline, post1, post2) on a difference score that compared attitudes toward White and Arab people.
\end{quote}

\begin{quote}
Mixed design ANOVA has a number of assumptions related to both the within-subjects and between-subjects elements. Data are expected to be normally distributed at each level of design. Visual inspection of boxplots for each wave of the design, assisted by the \emph{rstatix::identify\_outliers()} function (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated some outliers, but none at the extreme level. There was no evidence of skew (all values were at or below the absolute value of 0.32) or kurtosis (all values were below the absolute value of .57; \citep{kline_principles_2016}). Additionally, the Shapiro-Wilk tests applied at each level of the design were non-significant. Because of the between-subjects aspect of the design, the homogeneity of variance assumption was evaluated. Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline \emph{F} {[}1, 191{]} = 3.973, \emph{p} = .047). However, there was no indication of assumption violation at post1 (\emph{F} {[}1, 191{]} = 0.141, \emph{p} = .708), and post2 (\emph{F} {[}1, 191{]} = 0.107, \emph{p} = .743) waves of the design. Further, Box's M-test (\emph{M} = 3.209, \emph{p} = .073) indicated no violation of the homogeneity of covariance matrices. Mauchly's test indicated no violation of the sphericity assumption for the main effect (\emph{W} = 0.99, \emph{p} = .369) and interaction effect (\emph{W} = 0.99, \emph{p} = .369).
\end{quote}

\begin{quote}
Results of the omnibus ANOVA indicated a significant main effect for condition (\emph{F}{[}1, 191{]} = 13.149, \emph{p} \textless{} .001, \(\eta^{2}\) = 0.023), a non-significant main effect for wave (\emph{F}{[}2, 382{]} = 0.273, \emph{p} = .761, \(\eta^{2}\) = 0.001), and a significant interaction effect (\emph{F}{[}2, 382{]} = 5.008, \emph{p} = 0.007, \(\eta^{2}\) = 0.017).
\end{quote}

\begin{quote}
We followed the significant interaction effect with an evaluation of simple main effects of wave within condition. There were non-significant difference within the the Friends condition (\emph{F} {[}2, 194{]} = 1.759, \emph{p} = 0.175, \(\eta^{2}\) = 0.012). There were significant differences with an effect size indicating a small-to-moderate effect in the Little Mosque condition (\emph{F} {[}2, 188{]} = 3.392, \emph{p} = 0.036, \(\eta^{2}\) = 0.072). We followed up the significant simple main effect for the Little Mosque condition with pairwiwse comparisons. At this level we controlled for Type I error by dividing alpha (.05) by the number of paired comparisons (3). We found a significant difference between baseline and post1 (\emph{t}{[}95{]} = 2.447, \emph{p} = .016), but non-significant differences between baseline and post2 (\emph{t}{[}95{]} = 1.621, \emph{p} = .108) and post1 and post2 (\emph{t}{[}95{]} = -1.034, \emph{p} = .304).
\end{quote}

\begin{quote}
As illustrated in Figure 1 difference scores were comparable at baseline. After the intervention, difference scores increased for those in the Friends condition -- indicating more favorable attitudes toward White people. In contrast, those exposed to the Little Mosque condition had difference scores that were lower. Means and standard deviations are reported in Table 1.
\end{quote}

The following code can be used to write output to .csv files. From there it is easy(er) to manipulate them into tables for use in an empirical manuscript.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(pwcWVwiGP, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"pwcWVwiGP.csv"}\NormalTok{)}
\CommentTok{\# this command can also be used to export other output}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(Diff\_2way}\SpecialCharTok{$}\NormalTok{ANOVA, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"Diff\_2way.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(SimpleWave, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"SimpleWave.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(SimpleCond, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"SimpleCond.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{comparing-our-findings-to-murrar-and-brauer--murrar_entertainment-education_2018}{%
\subsubsection{\texorpdfstring{Comparing our findings to Murrar and Brauer \citeyearpar{murrar_entertainment-education_2018}}{Comparing our findings to Murrar and Brauer {[}-@murrar\_entertainment-education\_2018{]}}}\label{comparing-our-findings-to-murrar-and-brauer--murrar_entertainment-education_2018}}

In general, the results of our simulation mapped onto the findings. If you have access to the article I encourage you to examine it as you consider my observations.

\begin{itemize}
\tightlist
\item
  The authors started their primary analyses of Experiment 1 with independent \emph{t} tests comparing the Friends and Little Mosque conditions within each of the baseline, post1, and post2 waves. This is equivalent to our simple main effects of condition within wave that we conducted as follow-up to the significant interaction effect. It is not clear to me why they did not precede this with a mixed design ANOVA.

  \begin{itemize}
  \tightlist
  \item
    The results of the article are presented in their Table 1
  \item
    Our results were comparable in that we found no attitude difference at baseline
  \item
    Similar to the results in the article we found statistically significant differences (with comparable \emph{p} values and effect sizes) at post1 and post2
  \end{itemize}
\item
  With two experiments (each with a number of associated hypotheses) in a single paper there were a large number of analyses conducted by the authors. I think they designed tables and figures that provided an efficient and clear review of the study design and their findings.
\item
  This finding is exciting to me. Anti-racism education frequently encourages individuals to expose themselves to content authored/created by individuals from groups with marginalized identities. This finding supports that approach to prejudice reduction.
\end{itemize}

\hypertarget{power-in-mixed-design-anova}{%
\section{Power in Mixed Design ANOVA}\label{power-in-mixed-design-anova}}

The package \href{https://webpower.psychstat.org/wiki/_media/grant/practical_statistica_interior_for_kindle.pdf}{\emph{wp.rmanova}} was designed for power analysis in repeated measures ANOVA.

Power analysis allows us to determine the probability of detecting an effect of a given size with a given level of confidence. Especially when we don't achieve significance, we may want to stop.

In the \emph{WebPower} package, we specify 6 of 7 interrelated elements; the package computes the missing element

\begin{itemize}
\tightlist
\item
  \emph{n} = sample size (number of individuals in the whole study)
\item
  \emph{ng} = number of groups
\item
  \emph{nm} = number of repeated measurements (i.e., waves)
\item
  \emph{f} = Cohen's \emph{f} (an effect size; we can use a conversion calculator); Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively
\item
  \emph{nscor} = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package's default; \textless{} 1 means some correction was applied
\item
  \emph{alpha} = is the probability of Type I error; we traditionally set this at .05
\item
  \emph{power} = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
\item
  \emph{type} = 0 is for between-subjects, 1 is for repeated measures, 2 is for interaction effect; in a mixed design ANOVA we will select ``2''
\end{itemize}

As in the prior lessons, we need to convert our effect size for the \emph{interaction} to \(f\) effect size (this is not the same as the \emph{F} test). The \emph{effectsize} package has a series of converters. We can use the \emph{eta2\_to\_f()} function to translate the \(\eta^{2}\) associated with the interaction effect to Cohen's \emph{f}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#interaction effect}
\NormalTok{effectsize}\SpecialCharTok{::}\FunctionTok{eta2\_to\_f}\NormalTok{(}\FloatTok{0.017}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1315066
\end{verbatim}

We can now retrieve information from our study (including the Cohen's \emph{f} value we just calculated) and insert it into the script for the power analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WebPower}\SpecialCharTok{::}\FunctionTok{wp.rmanova}\NormalTok{(}\AttributeTok{n=}\DecValTok{193}\NormalTok{, }\AttributeTok{ng=}\DecValTok{2}\NormalTok{, }\AttributeTok{nm=}\DecValTok{3}\NormalTok{, }\AttributeTok{f =}\NormalTok{ .}\DecValTok{1315}\NormalTok{, }\AttributeTok{nscor =}\NormalTok{ .}\DecValTok{99}\NormalTok{, }\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{05}\NormalTok{, }\AttributeTok{power =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Repeated-measures ANOVA analysis

      n      f ng nm nscor alpha     power
    193 0.1315  2  3  0.99  0.05 0.3493183

NOTE: Power analysis for interaction-effect test
URL: http://psychstat.org/rmanova
\end{verbatim}

We are powered at .349 (we have a 35\% of rejecting the null hypothesis, if it is true)

In reverse, setting \emph{power} at .80 (the traditional value) and changing \emph{n} to \emph{NULL} yields a recommended sample size.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WebPower}\SpecialCharTok{::}\FunctionTok{wp.rmanova}\NormalTok{(}\AttributeTok{n =} \ConstantTok{NULL}\NormalTok{, }\AttributeTok{ng =} \DecValTok{2}\NormalTok{, }\AttributeTok{nm =} \DecValTok{3}\NormalTok{, }\AttributeTok{f =} \FloatTok{0.1315}\NormalTok{, }\AttributeTok{nscor =} \FloatTok{0.99}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{power =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{type =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Repeated-measures ANOVA analysis

          n      f ng nm nscor alpha power
    562.608 0.1315  2  3  0.99  0.05   0.8

NOTE: Power analysis for interaction-effect test
URL: http://psychstat.org/rmanova
\end{verbatim}

Given our desire for strong power and our weak effect size, this power analysis suggests a sample size of 562 participants to detect a significant interaction effect.

\hypertarget{practice-problems-7}{%
\section{Practice Problems}\label{practice-problems-7}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that offer differing levels of difficulty. Whichever you choose, you will focus on these larger steps in one-way ANOVA, including:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct a two-way (minimally a 2x3), mixed design, ANOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    report main and interaction effects
  \item
    conduct follow-up testing of simple main effects
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-2}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-2}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

\begin{itemize}
\tightlist
\item
  If mixed design ANOVA is new to you, perhaps you just change the number in ``set.seed(210813)'' from 210813 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.
\item
  If you are interested in power, change the sample size to something larger or smaller.
\item
  If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.
\end{itemize}

\hypertarget{problem-2-conduct-a-one-way-anova-with-a-different-dependent-variable.}{%
\subsection{Problem \#2: Conduct a one-way ANOVA with a different dependent variable.}\label{problem-2-conduct-a-one-way-anova-with-a-different-dependent-variable.}}

The Murrar et al. \citeyearpar{murrar_entertainment-education_2018} article has three dependent variables (attitudes toward people who are Arab, attitudes toward people who are White, and the difference score). I analyzed the difference score. Select one of the other dependent variables. If you do not get a significant interaction, play around with the simulation (changing the sample size, standard deviations, or both) until you get a significant interaction effect.

\hypertarget{problem-3-try-something-entirely-new.-2}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-2}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a mixed design ANOVA. Please have at least 3 levels for one predictor and at least 2 levels for the second predictor.

\hypertarget{grading-rubric-4}{%
\subsection{Grading Rubric}\label{grading-rubric-4}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Narrate the research vignette, describing the IV and DV & 5 & \_\_\_\_\_ \\
2. Simulate (or import) and format data & 5 & \_\_\_\_\_ \\
3. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
4. Conduct omnibus ANOVA (w effect size) & 5 & \_\_\_\_\_ \\
5. Conduct one set of follow-up tests; narrate your choice & 5 & \_\_\_\_\_ \\
6. Describe approach for managing Type I error & 5 & \_\_\_\_\_ \\
7. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
8 Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 40 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{ANCOVA}{%
\chapter{Analysis of Covariance}\label{ANCOVA}}

\href{https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=c0a9e50e-2e9d-4769-bd44-ad8c010143df}{Screencasted Lecture Link}

The focus of this lecture is analysis of covariance. Sticking with the same research vignette as we used for the mixed design ANOVA, we rearrange the variables a bit to see how they work in an ANCOVA design. The results help clarify the distinction between \emph{moderator} and \emph{covariate.}

\hypertarget{navigating-this-lesson-9}{%
\section{Navigating this Lesson}\label{navigating-this-lesson-9}}

There is about just about an hour of lecture. If you work through the materials with me, plan for an additional hour or two

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail. All original materials are provided at the \href{https://github.com/lhbikos/ReCenterPsychStats}{Github site} that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's \protect\hyperlink{ReCintro}{introduction}

\hypertarget{learning-objectives-9}{%
\subsection{Learning Objectives}\label{learning-objectives-9}}

Learning objectives from this lecture include the following:

\begin{itemize}
\tightlist
\item
  Define a \emph{covariate} and distinguish it from a \emph{moderator.}
\item
  Recognize the case where ANCOVA is a defensible statistical approach for analyzing the data.
\item
  Name and test the assumptions underlying ANCOVA.
\item
  Analyze, interpret, and write up results for ANCOVA.
\item
  List the conditions that are prerequisite for the appropriate use of a covariate or control variable.
\end{itemize}

\hypertarget{planning-for-practice-8}{%
\subsection{Planning for Practice}\label{planning-for-practice-8}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results \emph{should} map onto the ones obtained in the lecture.

The second option comes from the research vignette. For this ANCOVA article, I take a lot of liberties with the variables and research design. You could further mix and match for a different ANCOVA constellation.

As a third option, you are welcome to use data to which you have access and is suitable for ANCOVA. In either case the practice options suggest that you:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct an ANCOVA, including

  \begin{itemize}
  \tightlist
  \item
    omnibus test and effect size
  \item
    report main effects and engage in any follow-up testing
  \item
    interpret results in light of the role of the second predictor variable as a \emph{covariate} (as opposed to the moderating role in the prior lessons)
  \end{itemize}
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{readings-resources-8}{%
\subsection{Readings \& Resources}\label{readings-resources-8}}

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

\begin{itemize}
\tightlist
\item
  Green, S. B., \& Salkind, N. J. (2014). One-Way Analysis of Covariance (Lesson 27). In \emph{Using SPSS for Windows and Macintosh: Analyzing and understanding data} (Seventh edition., pp.~151--160). Boston: Pearson. OR

  \begin{itemize}
  \tightlist
  \item
    This lesson provides an excellent review of ANCOVA with examples of APA style write-ups. The downside is that it is written for use in SPSS.
  \end{itemize}
\item
  ANCOVA in R: The Ultimate Practical Guide. (n.d.). Retrieved from \url{https://www.datanovia.com/en/lessons/ancova-in-r/}

  \begin{itemize}
  \tightlist
  \item
    This is the workflow we are using for the lecture and written specifically for R.
  \end{itemize}
\item
  Bernerth, J. B., \& Aguinis, H. (2016). A critical review and best‐practice recommendations for control variable usage. \emph{Personnel Psychology, 69}(1), 229--283. \url{https://doi.org/10.1111/peps.12103}

  \begin{itemize}
  \tightlist
  \item
    An article from the industrial-organizational psychology world. Especially relevant for this lesson is the flowchart on page 273 and the discussion (pp.~270 to the end).
  \end{itemize}
\item
  Murrar, S., \& Brauer, M. (2018). Entertainment-education effectively reduces prejudice. \emph{Group Processes \& Intergroup Relations, 21}(7), 1053--1077. \url{https://doi.org/10.1177/1368430216682350}
\item
  This article is the source of our research vignette. I used this same article in the lesson on \protect\hyperlink{Mixed}{mixed design ANOVA}. Swapping variable roles can be useful in demonstrating how ANCOVA is different than mixed design ANOVA.
\end{itemize}

\hypertarget{packages-7}{%
\subsection{Packages}\label{packages-7}}

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# used to convert data from long to wide}
\CommentTok{\# if(!require(reshape2))\{install.packages(\textquotesingle{}reshape2\textquotesingle{})\}}
\CommentTok{\# if(!require(broom))\{install.packages(\textquotesingle{}broom\textquotesingle{})\}}
\CommentTok{\# if(!require(tidyverse))\{install.packages(\textquotesingle{}tidyverse\textquotesingle{})\}}
\CommentTok{\# if(!require(psych))\{install.packages(\textquotesingle{}psych\textquotesingle{})\} easy plots}
\CommentTok{\# if(!require(ggpubr))\{install.packages(\textquotesingle{}ggpubr\textquotesingle{})\} pipe{-}friendly R}
\CommentTok{\# functions if(!require(rstatix))\{install.packages(\textquotesingle{}rstatix\textquotesingle{})\} export}
\CommentTok{\# objects for table making}
\CommentTok{\# if(!require(MASS))\{install.packages(\textquotesingle{}MASS\textquotesingle{})\}}
\CommentTok{\# if(!require(knitr))\{install.packages(\textquotesingle{}knitr\textquotesingle{})\}}
\CommentTok{\# if(!require(dplyr))\{install.packages(\textquotesingle{}dplyr\textquotesingle{})\}}
\CommentTok{\# if(!require(apaTables))\{install.packages(\textquotesingle{}apaTables\textquotesingle{})\}}
\end{Highlighting}
\end{Shaded}

\hypertarget{introducing-analysis-of-covariance-ancova}{%
\section{Introducing Analysis of Covariance (ANCOVA)}\label{introducing-analysis-of-covariance-ancova}}

Analysis of covariance (ANCOVA) evaluates the null hypothesis that

\begin{itemize}
\tightlist
\item
  population means on a dependent variable are equal across levels of a factor(s) adjusting for differences on a covariate(s); stated differently -
\item
  the population adjusted means are equal across groups
\end{itemize}

This lecture introduces a distinction between \textbf{moderators} and \textbf{covariates}.

\textbf{Moderator}: a variable that changes the strength or direction of an effect between two variables X (predictor, independent variable) and Y (criterion, dependent variable).

\textbf{Covariate}: an observed, continuous variable, that (when used properly) has a relationship with the dependent variable. It is included in the analysis, as a predictor, so that the predictive relationship between the independent (IV) and dependent (DV) are adjusted.

Understanding this difference may be facilitated by understanding one of the assumptions of ANCOVA -- that the slopes relating the covariate to the dependent variable are the same for all groups (i.e., the homogeneity-of-slopes assumption). If this assumption is violated then the between-group differences in adjusted means are not interpretable and the covariate should be treated as a moderator and analyses that assess the simple main effects (i.e., follow-up to a significant interaction) should be conducted.

A one-way ANCOVA requires three variables:

\begin{itemize}
\tightlist
\item
  IV/factor -- categorical (2 or more)
\item
  DV -- continuous
\item
  covariate -- continuous
\end{itemize}

Green and Salkind \citep{green_one-way_2014} identified common uses of ANCOVA:

\begin{itemize}
\tightlist
\item
  Studies with a pretest and random assignment of subjects to factor levels. Variations on this research design include:

  \begin{itemize}
  \tightlist
  \item
    assignment to factor levels based on that pretest,
  \item
    matching based on the pretest, and random assignment to factor levels,
  \item
    simply using the pretest as a covariate for the posttest DV.
  \end{itemize}
\item
  Studies with a potentially confounding variable (best when there is theoretical justification and prior empirical evidence for such) over which the researcher wants ``control''
\end{itemize}

Although it is possible to have multi-way (e.g., 2-way, 3-way) ANCOVA, in this lecture we will only work two, one-way ANCOVAs representing these common use cases.

ANCOVA has four primary assumptions:

\textbf{Linearity}: The covariate is linearly related to the dependent variable within all levels of the factor (IV).

\textbf{Homogeneity of regression slopes}: The weights or slopes relating the covariate to the DV are equal across all levels of the factor.

\textbf{Normally distributed}: The DV is normally distributed in the population for any specific value of the covariate and for any one level of a factor. This assumption applies to every combination of the values of the covariate and levels ohttps://www.datanovia.com/en/lessons/ancova-in-r/f the factor and requires them all to be normally distributed. To the degree that population distributions are not normal and sample sizes are small, \emph{p} values may not be trustworthy and power reduced. Evaluating this is frequently operationalized by inspecting the residuals and identifying outliers.

\textbf{Homogeneity of variances}: The variances of the DV for the conditional distributions (i.e., every combination of the values of the covariate and levels of the factor) are equal.

We are following the approach to analyzing ANCOVA identifed in the Datanovia lesson on ANCOVA \citep{datanovia_ancova_nodate}.

Our analytic process will be similar to others in the ANOVA series:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Prepare the data
\item
  Evaluate potential violation of the assumptions
\item
  Compute the omnibus ANCOVA, and follow-up accordingly

  \begin{itemize}
  \tightlist
  \item
    If significant: follow-up with post-hoc comparisons, planned contrasts, and/or polynomial
  \item
    If non-significant: stopping.
  \end{itemize}
\end{enumerate}

An ANCOVA workflow maps this in further detail.

\begin{figure}
\centering
\includegraphics{images/ANCOVA/wf_ANCOVA.jpg}
\caption{Image of the ANCOVA workflow}
\end{figure}

\hypertarget{research-vignette-8}{%
\section{Research Vignette}\label{research-vignette-8}}

We will continue with the example used in the \protect\hyperlink{Mixed}{mixed design ANOVA lesson} The article does not contain any ANCOVA analyses, but there is enough data that I can demonstrate the two general ways (i.e., controlling for the pretest, controlling for a potentially confounding variable) that ANCOVA is used.

Here is a quick reminder of the research vignette.

Murrar and Brauer's \citeyearpar{murrar_entertainment-education_2018} article described the results of two studies designed to reduce prejudice against Arabs/Muslims. In the lesson on mixed design ANOVA, we only worked the first of two experiments reported in the study. Participants (\emph{N} = 193), all who were White, were randomly assigned to one of two conditions where they watched six episodes of the sitcom \href{http://www.friends-tv.org/}{\emph{Friends}} or \href{https://en.wikipedia.org/wiki/Little_Mosque_on_the_Prairie}{\emph{Little Mosque on the Prairie}}. The sitcoms and specific episodes were selected after significant pilot testing. The selection was based on the tension selecting stimuli that were as similar as possible, yet the intervention-oriented sitcom needed to invoke psychological processes known to reduce prejudice. The authors felt that both series had characters that were likable and relateble who were engaged in activities of daily living. The Friends series featured characters who were predominantly White, cis-gendered, and straight. The Little Mosque series portrays the experience Western Muslims and Arabs as they live in a small Canadian town. This study involved assessment across three waves: baseline (before watching the assigned episodes), post1 (immediately after watching the episodes), and post2 (completed 4-6 weeks after watching the episodes).

The study used \emph{feelings and liking thermometers}, rating their feelings and liking toward 10 different groups of people on a 0 to 100 sliding scale (with higher scores reflecting greater liking and positive feelings). For the purpose of this analysis, the ratings of attitudes toward White people and attitudes toward Arabs/Muslims were used. A third metric was introduced by subtracting the attitudes towards Arabs/Muslims from the attitudes toward Whites. Higher scores indicated more positive attitudes toward Whites where as low scores indicated no difference in attitudes. To recap, there were three potential dependent variables, all continuously scaled:

\begin{itemize}
\tightlist
\item
  AttWhite: attitudes toward White people; higher scores reflect greater liking
\item
  AttArab: attitudes toward Arab people; higher scores reflect greater liking
\item
  Diff: the difference between AttWhite and AttArab; higher scores reflect a greater liking for White people
\end{itemize}

With random assignment, nearly equal cell sizes, a condition with two levels (Friends, Little Mosque), and three waves (baseline, post1, post2), this is perfect for mixed design ANOVA but suitable for an ANCOVA demonstration.

\begin{figure}
\centering
\includegraphics{images/mixed/Murrar_design.jpg}
\caption{Image of the design for the Murrar and Brauer (2018) study}
\end{figure}

\hypertarget{simulating-the-data-from-the-journal-article-1}{%
\subsection{Simulating the data from the journal article}\label{simulating-the-data-from-the-journal-article-1}}

Below is the code I have used to simulate the data. The simulation includes two dependent variables (AttWhite, AttArab), Wave (baseline, post1, post2), and COND (condition; Friends, Little\_Mosque). There is also a caseID (repeated three times across the three waves) and rowID (giving each observation within each case an ID). You can use this simulation for two of the three practice suggestions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\CommentTok{\# change this to any different number (and rerun the simulation) to}
\CommentTok{\# rework the chapter problem}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{210813}\NormalTok{)}
\CommentTok{\# sample size, M and SD for each cell; this will put it in a long}
\CommentTok{\# file}
\NormalTok{AttWhite }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{76.79}\NormalTok{, }\AttributeTok{sd =} \FloatTok{18.55}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.37}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{18.99}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{77.47}\NormalTok{, }\AttributeTok{sd =} \FloatTok{18.95}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.81}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{19.29}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{77.79}\NormalTok{, }\AttributeTok{sd =} \FloatTok{17.25}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{75.89}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{19.44}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\CommentTok{\# set upper bound for variable}
\NormalTok{AttWhite[AttWhite }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{100}
\CommentTok{\# set lower bound for variable}
\NormalTok{AttWhite[AttWhite }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{AttArab }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.11}\NormalTok{, }\AttributeTok{sd =} \FloatTok{20.97}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.37}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{20.03}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{64.16}\NormalTok{, }\AttributeTok{sd =} \FloatTok{21.64}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{70.52}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{18.55}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{98}\NormalTok{, }\AttributeTok{mean =} \FloatTok{65.29}\NormalTok{, }\AttributeTok{sd =} \FloatTok{19.76}\NormalTok{), }\FunctionTok{rnorm}\NormalTok{(}\DecValTok{95}\NormalTok{, }\AttributeTok{mean =} \FloatTok{70.3}\NormalTok{,}
    \AttributeTok{sd =} \FloatTok{17.98}\NormalTok{)), }\DecValTok{3}\NormalTok{)}
\CommentTok{\# set upper bound for variable}
\NormalTok{AttArab[AttArab }\SpecialCharTok{\textgreater{}} \DecValTok{100}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{100}
\CommentTok{\# set lower bound for variable}
\NormalTok{AttArab[AttArab }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{rowID }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{579}\NormalTok{))}
\NormalTok{caseID }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{((}\DecValTok{1}\SpecialCharTok{:}\DecValTok{193}\NormalTok{), }\DecValTok{3}\NormalTok{)}
\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\DecValTok{193}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Post1"}\NormalTok{, }\DecValTok{193}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Post2"}\NormalTok{, }\DecValTok{193}\NormalTok{))}
\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{),}
    \FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\DecValTok{98}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\StringTok{"LittleMosque"}\NormalTok{, }\DecValTok{95}\NormalTok{))}
\CommentTok{\# groups the 3 variables into a single df: ID\#, DV, condition}
\NormalTok{Murrar\_df }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(rowID, caseID, Wave, COND, AttArab, AttWhite)}
\CommentTok{\# make caseID a factor}
\NormalTok{Murrar\_df[, }\StringTok{"caseID"}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(Murrar\_df[, }\StringTok{"caseID"}\NormalTok{])}
\CommentTok{\# make Wave an ordered factor}
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{Wave }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Murrar\_df}\SpecialCharTok{$}\NormalTok{Wave, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Baseline"}\NormalTok{, }\StringTok{"Post1"}\NormalTok{,}
    \StringTok{"Post2"}\NormalTok{))}
\CommentTok{\# make COND an ordered factor}
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{COND }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(Murrar\_df}\SpecialCharTok{$}\NormalTok{COND, }\AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Friends"}\NormalTok{, }\StringTok{"LittleMosque"}\NormalTok{))}
\CommentTok{\# creates the difference score}
\NormalTok{Murrar\_df}\SpecialCharTok{$}\NormalTok{Diff }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_df}\SpecialCharTok{$}\NormalTok{AttWhite }\SpecialCharTok{{-}}\NormalTok{ Murrar\_df}\SpecialCharTok{$}\NormalTok{AttArab}
\end{Highlighting}
\end{Shaded}

Let's check the structure. We want

\begin{itemize}
\tightlist
\item
  rowID and caseID to be unordered factors,
\item
  Wave and COND to be ordered factors,
\item
  AttArab, AttWhite, and Diff to be numerical
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(Murrar\_df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   579 obs. of  7 variables:
 $ rowID   : Factor w/ 579 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ caseID  : Factor w/ 193 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ Wave    : Factor w/ 3 levels "Baseline","Post1",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ COND    : Factor w/ 2 levels "Friends","LittleMosque": 1 1 1 1 1 1 1 1 1 1 ...
 $ AttArab : num  74.3 55.8 33.3 66.3 71 ...
 $ AttWhite: num  100 79 75.9 68.2 100 ...
 $ Diff    : num  25.71 23.18 42.67 1.92 29.01 ...
\end{verbatim}

The structure looks satisfactory. R will automatically ``order'' factors alphabetically or numerically. In this lesson's example the alphabettical ordering (i.e., Baseline, Post1, Post2; Friends, LittleMosque) is consistent with the logic in our study.

If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv (``Excel lite'') or .rds (R object) file. This is not a necessary step.

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R. This is what I would do. \emph{Note: My students and I have discovered that the the psych::describeBy() function seems to not work with files in the .rds format, but does work when the data are imported with .csv.}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Murrar\_df, \textquotesingle{}Murrar\_RDS.rds\textquotesingle{}) bring back the simulated}
\CommentTok{\# dat from an .rds file Murrar\_df \textless{}{-} readRDS(\textquotesingle{}Murrar\_RDS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

The code for .csv will likely lose the formatting (i.e., stripping Wave and COND of their ordered factors), but it is easy to view in Excel.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Murrar\_df,}
\CommentTok{\# file=\textquotesingle{}DiffCSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE) bring}
\CommentTok{\# back the simulated dat from a .csv file Murrar\_df \textless{}{-} read.csv}
\CommentTok{\# (\textquotesingle{}DiffCSV.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{scenario-1-controlling-for-the-pretest}{%
\section{Scenario \#1: Controlling for the pretest}\label{scenario-1-controlling-for-the-pretest}}

So that we can begin to understand how the covariate operates, we are going to predict attitudes towards Arabs at post-test (AttArabP1) by condition (COND), controlling for attitudes toward Arabs at baseline (AttArabB). You may notice that in this analysis we are ignoring the second post-test. This is because I am simply demonstrating ANCOVA. To ignore the second post test would be a significant loss of information.

\hypertarget{preparing-the-data-1}{%
\subsection{Preparing the data}\label{preparing-the-data-1}}

When the covariate in ANCOVA is a pretest, we need three variables:

\begin{itemize}
\tightlist
\item
  IV that has two or more levels; in our case it is the Friends and Little Mosque conditions
\item
  DV that is continuous; in our case it is the attitudes toward Arabs at post1
\item
  Covariate that is continuous; in our case it is the attitudes toward Arabs at baseline
\end{itemize}

The form of our data matters. The simulation created a \emph{long} form (formally called the \emph{person-period} form) of data. That is, each observation for each person is listed in its own row. In this dataset where we have 193 people with 3 observation (baseline, post1, post2) each, we have 579 rows. In ANCOVA where we use the pre-test as a covariate, we need all the data to be on a single row.This is termed the \emph{person level} form of data. We can restructure the data with the \emph{data.table} and \emph{reshape2}()* packages.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a new df (Murrar\_wide) Identify the original df In the}
\CommentTok{\# transition from long{-}to{-}wide it seems like you can only do one}
\CommentTok{\# time{-}varying variable at a time When there are multiple}
\CommentTok{\# time{-}varying and time{-}static variables, put all the time{-}static}
\CommentTok{\# variables on the left side of the tilde Put the name of the single}
\CommentTok{\# time{-}varying variable in the concatonated list}
\NormalTok{Murrar1 }\OtherTok{\textless{}{-}}\NormalTok{ reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Murrar\_df, }\AttributeTok{formula =}\NormalTok{ caseID }\SpecialCharTok{+}\NormalTok{ COND }\SpecialCharTok{\textasciitilde{}}
\NormalTok{    Wave, }\AttributeTok{value.var =} \StringTok{"AttArab"}\NormalTok{)}
\CommentTok{\# before restructuring a second variable, rename the first variable}
\NormalTok{Murrar1 }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(Murrar1, }\AttributeTok{AttArabB =} \StringTok{"Baseline"}\NormalTok{, }\AttributeTok{AttArabP1 =} \StringTok{"Post1"}\NormalTok{,}
    \AttributeTok{AttArabP2 =} \StringTok{"Post2"}\NormalTok{)}
\CommentTok{\# repeat the process for additional variables; but give the new df}
\CommentTok{\# new names {-}{-} otherwise you\textquotesingle{}ll overwrite your work}
\NormalTok{Murrar2 }\OtherTok{\textless{}{-}}\NormalTok{ reshape2}\SpecialCharTok{::}\FunctionTok{dcast}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Murrar\_df, }\AttributeTok{formula =}\NormalTok{ caseID }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Wave, }\AttributeTok{value.var =} \StringTok{"AttWhite"}\NormalTok{)}
\NormalTok{Murrar2 }\OtherTok{\textless{}{-}} \FunctionTok{rename}\NormalTok{(Murrar2, }\AttributeTok{AttWhiteB =} \StringTok{"Baseline"}\NormalTok{, }\AttributeTok{AttWhiteP1 =} \StringTok{"Post1"}\NormalTok{,}
    \AttributeTok{AttWhiteP2 =} \StringTok{"Post2"}\NormalTok{)}
\CommentTok{\# Now we join them}
\NormalTok{Murrar\_wide }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{full\_join}\NormalTok{(Murrar1, Murrar2, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"caseID"}\NormalTok{))}

\FunctionTok{str}\NormalTok{(Murrar\_wide)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
'data.frame':   193 obs. of  8 variables:
 $ caseID    : Factor w/ 193 levels "1","2","3","4",..: 1 2 3 4 5 6 7 8 9 10 ...
 $ COND      : Factor w/ 2 levels "Friends","LittleMosque": 1 1 1 1 1 1 1 1 1 1 ...
 $ AttArabB  : num  74.3 55.8 33.3 66.3 71 ...
 $ AttArabP1 : num  80.3 76.6 92 96.5 59.1 ...
 $ AttArabP2 : num  64.8 43.3 40.3 69.1 74.9 ...
 $ AttWhiteB : num  100 79 75.9 68.2 100 ...
 $ AttWhiteP1: num  95.6 51 91.9 86.7 75.8 ...
 $ AttWhiteP2: num  100 89.7 49.5 99.4 83.1 ...
\end{verbatim}

If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv (``Excel lite'') or .rds (R object) file. This is not a necessary step.

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R. This is what I would do.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# to save the df as an .rds (think \textquotesingle{}R object\textquotesingle{}) file on your computer;}
\CommentTok{\# it should save in the same file as the .rmd file you are working}
\CommentTok{\# with saveRDS(Murrar\_wide, \textquotesingle{}MurrarW\_RDS.rds\textquotesingle{}) bring back the}
\CommentTok{\# simulated dat from an .rds file Murrar\_wide \textless{}{-}}
\CommentTok{\# readRDS(\textquotesingle{}MurrarW\_RDS.rds\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

The code for .csv will likely lose the formatting (i.e., stripping Wave and COND of their ordered factors), but it is easy to view in Excel.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# write the simulated data as a .csv write.table(Murrar\_wide,}
\CommentTok{\# file=\textquotesingle{}MurrarW\_CSV.csv\textquotesingle{}, sep=\textquotesingle{},\textquotesingle{}, col.names=TRUE, row.names=FALSE)}
\CommentTok{\# bring back the simulated dat from a .csv file Murrar\_wide \textless{}{-}}
\CommentTok{\# read.csv (\textquotesingle{}MurrarW\_CSV.csv\textquotesingle{}, header = TRUE)}
\end{Highlighting}
\end{Shaded}

\hypertarget{checking-the-assumptions}{%
\subsection{Checking the assumptions}\label{checking-the-assumptions}}

There are a number of assumptions in ANCOVA. These include:

\begin{itemize}
\tightlist
\item
  random sampling
\item
  independence in the scores representing the dependent variable

  \begin{itemize}
  \tightlist
  \item
    there is, of course, intentional dependence in any repeated measures or within-subjects variable
  \end{itemize}
\item
  linearity of the relationship between the covariate and DV within all levels of the independent variable
\item
  homogeneity of the regression slopes
\item
  a normally distributed DV for any specific value of the covariate and for any one level of a factor
\item
  homogeneity of variance
\end{itemize}

These are depicted in the flowchart, below.

\begin{figure}
\centering
\includegraphics{images/ANCOVA/wf_ANCOVA_assumptions.jpg}
\caption{Image of the ANCOVA workflow, showing our current place in the process}
\end{figure}

\hypertarget{linearity-assumption}{%
\subsubsection{Linearity assumption}\label{linearity-assumption}}

ANCOVA assumes that there is linearity between the covariate and outcome variable at each level of the grouping variable. In our case this means that there is linearity between the pre-test (covariate) and post-test (outcome variable) at each level of the intervention (Friends, Little Mosque).

We can create a scatterplot (with regression lines) between covariate (our pretest) and the outcome (post-test1).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggscatter}\NormalTok{(Murrar\_wide, }\AttributeTok{x =} \StringTok{"AttArabB"}\NormalTok{, }\AttributeTok{y =} \StringTok{"AttArabP1"}\NormalTok{, }\AttributeTok{color =} \StringTok{"COND"}\NormalTok{,}
    \AttributeTok{add =} \StringTok{"reg.line"}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_regline\_equation}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{paste}\NormalTok{(..eq.label..,}
\NormalTok{    ..rr.label.., }\AttributeTok{sep =} \StringTok{"\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}"}\NormalTok{), }\AttributeTok{color =}\NormalTok{ COND))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-419-1.pdf}
As in not surprising (because we tested a similar set of variables in the mixed design chapter), this relationship look like an interaction effect. Let's continue our exploration.

\hypertarget{homogeneity-of-regression-slopes}{%
\subsubsection{Homogeneity of regression slopes}\label{homogeneity-of-regression-slopes}}

This assumption requires that the slopes of the regression lines formed by the covariate and the outcome variable are the same for each group. The assumption evaluates that there is no interaction between the outcome and covariate. The plotted regression lines should be parallel.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND }\SpecialCharTok{*}\NormalTok{ AttArabB)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coefficient covariances computed by hccm()
\end{verbatim}

\begin{verbatim}
ANOVA Table (type II tests)

         Effect DFn DFd      F           p p<.05   ges
1          COND   1 189 26.819 0.000000569     * 0.124
2      AttArabB   1 189  0.676 0.412000000       0.004
3 COND:AttArabB   1 189  4.297 0.040000000     * 0.022
\end{verbatim}

Because the statistically significant interaction term is violation of homogeneity of regression slopes (\emph{F} {[}1, 189{]} = 4.297, \emph{p} = .040, \(\eta^2\) = 0.022) we should not proceed with ANCOVA as a statistical option. However, for the sake of demonstration, I will continue. One of the reasons I wanted to work this example as ANCOVA is to demonstrate that covariates and moderators each have their role. We can already see how this data is best analyzed with mixed design ANOVA.

\hypertarget{normality-of-residuals}{%
\subsubsection{Normality of residuals}\label{normality-of-residuals}}

Our goal here is to specify a model and extract \emph{residuals}: the difference between the observed value of the DV and its predicted value. Each data point has one residual. The sum and mean of residuals are equal to 0.

Once we have saved the residuals, we can treat them as data and evaluate the shape of their distribution. We hope that the distribution is not statistically significantly different from a normal one. We first compute the model with \emph{lm()} (lm stands for ``linear model''). This is a linear regression.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a linear regression model predicting DV from COV \& IV}
\NormalTok{AttArabB\_Mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttArabB }\SpecialCharTok{+}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ Murrar\_wide)}
\NormalTok{AttArabB\_Mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = AttArabP1 ~ AttArabB + COND, data = Murrar_wide)

Coefficients:
     (Intercept)          AttArabB  CONDLittleMosque  
        63.01428          -0.06042          14.92165  
\end{verbatim}

With the \emph{broom::augment()} function we can augment our \emph{lm()} model object to add fitted values and residuals.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# new model by augmenting the lm model}
\NormalTok{AttArabB\_Mod.metrics }\OtherTok{\textless{}{-}}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{augment}\NormalTok{(AttArabB\_Mod)}
\CommentTok{\# shows the first three rows of the UEmodel.metrics}
\FunctionTok{head}\NormalTok{(AttArabB\_Mod.metrics, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 9
  AttArabP1 AttArabB COND    .fitted .resid   .hat .sigma .cooksd .std.resid
      <dbl>    <dbl> <fct>     <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>
1      80.3     74.3 Friends    58.5   21.7 0.0111   20.2 0.00440      1.08 
2      76.6     55.8 Friends    59.6   17.0 0.0116   20.2 0.00280      0.845
3      92.0     33.3 Friends    61.0   31.0 0.0247   20.1 0.0204       1.56 
\end{verbatim}

From this, we can assess the normality of the residuals using the Shapiro Wilk test

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# apply shapiro\_test to that augmented model}
\NormalTok{rstatix}\SpecialCharTok{::}\FunctionTok{shapiro\_test}\NormalTok{(AttArabB\_Mod.metrics}\SpecialCharTok{$}\NormalTok{.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  variable                    statistic p.value
  <chr>                           <dbl>   <dbl>
1 AttArabB_Mod.metrics$.resid     0.984  0.0261
\end{verbatim}

The statistically significant Shapiro Wilk test has indicated a violation of the normality assumption (\emph{W} = 0.984, \emph{p} = .026).

\hypertarget{homogeneity-of-variances}{%
\subsubsection{Homogeneity of variances}\label{homogeneity-of-variances}}

ANCOVA presumes that the variance of the residuals is equal for all groups. We can check this with the Levene's test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AttArabB\_Mod.metrics }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{levene\_test}\NormalTok{(.resid }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
    df1   df2 statistic      p
  <int> <int>     <dbl>  <dbl>
1     1   191      3.52 0.0623
\end{verbatim}

A non-significant Levene's test indicated no violation of the homogeneity of the residual variances for all groups (\emph{F}{[}1, 191{]} = 3.515 \emph{p} = .062).

\hypertarget{outliers}{%
\subsubsection{Outliers}\label{outliers}}

We can identify outliers by examining the standardized (or studentized) residuals. This is the residual divided by its estimated standard error. Standardized residuals are interpreted as the number of standard errors away from the regression line.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# from our model metrics show us any standardized residuals that are}
\CommentTok{\# \textgreater{}3}
\NormalTok{AttArabB\_Mod.metrics }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  AttArabP1 AttArabB         COND  .fitted    .resid       .hat   .sigma
1     6.137   68.085 LittleMosque 73.82234 -67.68534 0.01056251 19.62279
     .cooksd .std.resid
1 0.04044273  -3.371254
\end{verbatim}

We do have one outlier with a standardized residual that has an absolute value greater than 3. At this point I am making a mental note of this. If this were ``for real'' I might more closely inspect these data. I would look at the whole response. If any response seemed invalid (e.g., random, extreme, or erratic responding) I would delete it. If the responses seemed valid, I \emph{could} truncate them to exactly 3 SEs or. I could also ignore it. Kline \citeyearpar{kline_principles_2016} has a great section on some of these options.

As noted by the suggestion of an interaction effect, our preliminary analyses suggests that ANCOVA is not the best option. We know from the prior lesson that a mixed design ANOVA worked well. In the spirit of an example, here's a preliminary write-up so far:

\hypertarget{write-up-of-assumptions}{%
\subsubsection{Write-up of Assumptions}\label{write-up-of-assumptions}}

\begin{quote}
A one-way analysis of covariance (ANCOVA) was conducted. The independent variable, condition, had two levels: Friends, Little Mosque. The dependent variable was attitudes towards Arabs expressed by the participant at post-test and covariate was the pre-test assessment of the same variable. A preliminary analysis evaluating the homogeneity-of-slopes assumption indicated that the relationship between the covariate and the dependent variable differed significantly as a function of the independent variable, \emph{F} (1, 189) = 4.297, \emph{p} = .040, \(\eta^2\) = 0.022. Regarding the assumption that the dependent variable is normally distributed in the population for any specific value of the covariate and for any one level of a factor, results of the Shapiro-Wilk test of normality on the model residuals was also significant,\emph{W} = 0.984, \emph{p} = .026. Only one datapoint (in the Little Mosque condition) had a standardized residual (-3.37) that exceeded an absolute value of 3.0. A non-significant Levene's test indicated no violation of the homogeneity of the residual variances for all groups, \emph{F}(1, 191) = 3.515, \emph{p} = .062.
\end{quote}

\hypertarget{calculating-the-omnibus-anova}{%
\subsection{Calculating the Omnibus ANOVA}\label{calculating-the-omnibus-anova}}

We are ready to conduct the omnibus ANOVA.

\begin{figure}
\centering
\includegraphics{images/ANCOVA/wf_ANCOVA_omnibus.jpg}
\caption{Image of the ANCOVA workflow, showing our current place in the process.}
\end{figure}

\emph{Order of variable entry} matters in ANCOVA. Thinking of the \emph{controlling for} language associate with covariates, we want to remove the effect of the covariate before we run the one-way ANOVA. With this ANCOVA we are asking the question, ``Does the condition (Friends or Little Mosque) contribute to more positive attitudes toward Arabs, when controlling for the pre-test score?''

In repeated measures projects, we expect there to be dependency in the data. That is, in most cases prior waves will have significant prediction on later waves. When ANCOVA uses a prior asssessment or wave as a covariate, that variable ``claims'' as much variance as possible and the subsequent variable can capture what is left over.

In the code below, we are predicting attitudes toward Arabs at post1 from the condition (Friends or Little Mosque), controlling for attitudes toward Arabs at baseline.

The \emph{ges} column provides the effect size, \(\eta^2\) where a general rule-of-thumb for interpretation is .02 (small), .13 (medium), and .26 (large) \citep{lakens_calculating_2013}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MurrarB\_ANCOVA }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttArabB }\SpecialCharTok{+}\NormalTok{ COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coefficient covariances computed by hccm()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstatix}\SpecialCharTok{::}\FunctionTok{get\_anova\_table}\NormalTok{(MurrarB\_ANCOVA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA Table (type II tests)

    Effect DFn DFd      F           p p<.05   ges
1 AttArabB   1 190  0.665 0.416000000       0.003
2     COND   1 190 26.361 0.000000698     * 0.122
\end{verbatim}

There was a non-significant effect of the baseline covariate on the post-test (\emph{F}{[}1, 190{]} = 0.665, \emph{p} = .416, \(\eta^2\) = 0.003). After controlling for the baseline attitudes toward Arabs, there was a statistically significant effect of condition on post-test attitudes toward Arabs, \emph{F}(1,190) = 26.361, \emph{p} \textless{} .001, \(\eta^2\) = 0.122. This appears to be a moderately sized effect.

\hypertarget{post-hoc-pairwise-comparisons-controlling-for-the-covariate}{%
\subsection{Post-hoc pairwise comparisons (controlling for the covariate)}\label{post-hoc-pairwise-comparisons-controlling-for-the-covariate}}

Just like in one-way ANOVA, we follow-up the significant effect of condition. We'll use all-possible pairwise comparisons. In our case, we only have two levels of the categorical factor, so this run wouldn't be necessary. I included it to provide the code for doing so. If there were three or more variables, we would see all possible comparisons.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc\_B }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{emmeans\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{covariate =}\NormalTok{ AttArabB, }\AttributeTok{p.adjust.method =} \StringTok{"none"}\NormalTok{)}
\NormalTok{pwc\_B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 9
  term          .y.   group1 group2    df statistic       p   p.adj p.adj.signif
* <chr>         <chr> <chr>  <chr>  <dbl>     <dbl>   <dbl>   <dbl> <chr>       
1 AttArabB*COND AttA~ Frien~ Littl~   190     -5.13 6.98e-7 6.98e-7 ****        
\end{verbatim}

Not surprisingly (since this single pairwise comparison is redundant with the omnibus ANCOVA), results suggest a statistically significant difference between Friends and Little Mosque at Post1.

With the script below we can obtain the covariate-adjusted marginal means. These are termed \emph{estimated marginal means.} Take a look at these and compare them to what we would see in the regular descriptives. It is helpful to see the grand mean (AttArabB) and then the marginal means (emmean).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emmeans\_B }\OtherTok{\textless{}{-}}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_emmeans}\NormalTok{(pwc\_B)}
\NormalTok{emmeans\_B}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 8
  AttArabB COND         emmean    se    df conf.low conf.high method      
     <dbl> <fct>         <dbl> <dbl> <dbl>    <dbl>     <dbl> <chr>       
1     66.2 Friends        59.0  2.04   190     55.0      63.0 Emmeans test
2     66.2 LittleMosque   73.9  2.07   190     69.8      78.0 Emmeans test
\end{verbatim}

Note that the \emph{emmeans} process produces slightly different means than the raw means produced with the \emph{psych} package's \emph{describeBy()} function. Why? Because the \emph{get\_emmeans()} function uses the model that included the covariate. That is, the \emph{estimated} means are covariate-adjusted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{descripts\_P1 }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ Murrar\_wide,}
    \AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{descripts\_P1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item       group1 vars  n     mean       sd  median  trimmed      mad   min
X11    1      Friends    1 98 59.02351 21.65024 57.9955 59.31306 23.67045 8.297
X12    2 LittleMosque    1 95 73.92134 18.51082 74.4600 75.52858 15.98984 6.137
    max  range       skew   kurtosis       se
X11 100 91.703 -0.0518848 -0.6252126 2.187005
X12 100 93.863 -0.9798189  1.6335325 1.899170
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Note. Recently my students and I have been having intermittent}
\CommentTok{\# struggles with the describeBy function in the psych package. We}
\CommentTok{\# have noticed that it is problematic when using .rds files and when}
\CommentTok{\# using data directly imported from Qualtrics. If you are having}
\CommentTok{\# similar difficulties, try uploading the .csv file and making the}
\CommentTok{\# appropriate formatting changes.}
\end{Highlighting}
\end{Shaded}

(\emph{M} = 59.02, \emph{SD} = 21.65)
(\emph{M} = 73.92, \emph{SD} = 18.51)

In our case the adjustments are very minor. Why? The effect of the attitudes toward Arabs baseline test on the attitudes toward Arabs post test was nonsignificant. We can see this in the bivariate correlations, below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MurP1\_Rmat }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{corr.test}\NormalTok{(Murrar\_wide[}\FunctionTok{c}\NormalTok{(}\StringTok{"AttArabB"}\NormalTok{, }\StringTok{"AttArabP1"}\NormalTok{)])}
\NormalTok{MurP1\_Rmat}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:psych::corr.test(x = Murrar_wide[c("AttArabB", "AttArabP1")])
Correlation matrix 
          AttArabB AttArabP1
AttArabB      1.00     -0.05
AttArabP1    -0.05      1.00
Sample Size 
[1] 193
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
          AttArabB AttArabP1
AttArabB      0.00      0.47
AttArabP1     0.47      0.00

 To see confidence intervals of the correlations, print with the short=FALSE option
\end{verbatim}

The correlation between attitudes toward Arabs at baseline and post test are nearly negligible (\emph{r} = -0.05, \emph{p} = .47)

\hypertarget{toward-an-apa-style-results-section}{%
\subsection{Toward an APA style results section}\label{toward-an-apa-style-results-section}}

As we assemble the elements for an APA style result sections, a table with the means, adjusted means, and correlations may be helpful.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Murrar\_wide[}\FunctionTok{c}\NormalTok{(}\StringTok{"AttArabB"}\NormalTok{, }\StringTok{"AttArabP1"}\NormalTok{)], }\AttributeTok{table.number =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 1 

Means, standard deviations, and correlations with confidence intervals
 

  Variable     M     SD    1          
  1. AttArabB  66.25 19.66            
                                      
  2. AttArabP1 66.36 21.46 -.05       
                           [-.19, .09]
                                      

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# You can save this as a Microsoft word document by adding this}
\CommentTok{\# statement into the command: filename = \textquotesingle{}your\_filename.doc\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Additionally, writing this output to excel files helped create the two tables that follow. The \emph{MASS} package is useful to export the model objects into .csv files. They are easily opened in Excel where they can be manipulated into tables for presentations and manuscripts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(pwc\_B, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"pwc\_B.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(emmeans\_B, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"emmeans\_B.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(descripts\_P1, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"descripts\_P1.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ultimately, I would want a table that included this information. Please refer to the APA style manual for more proper formatting for a manuscript that requires APA style.

\begin{longtable}[]{@{}l@{}}
\toprule()
Table 1 \\
\midrule()
\endhead
Unadjusted and Covariate-Adjusted Descriptive Statistics \\
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}lcc@{}}
\toprule()
Condition & Unadjusted & Covariate-Adjusted \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}lcccc@{}}
\toprule()
& \emph{M} & \emph{SD} & \emph{EMM} & \emph{SE} \\
\midrule()
\endhead
Friends & 59.02 & 21.65 & 59.01 & 2.04 \\
Little Mosque & 73.92 & 18.51 & 73.93 & 2.07 \\
\bottomrule()
\end{longtable}

Unlike the figure we created when we were testing assumptions, this script creates a plot from the model (which identifies AttArabB in its role as covariate). Thus, the relationship between condition and AttArabP1 controls for the effect of the AttArabB covariate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc\_B }\OtherTok{\textless{}{-}}\NormalTok{ pwc\_B }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{add\_xy\_position}\NormalTok{(}\AttributeTok{x =} \StringTok{"COND"}\NormalTok{, }\AttributeTok{fun =} \StringTok{"mean\_se"}\NormalTok{)}
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggline}\NormalTok{(rstatix}\SpecialCharTok{::}\FunctionTok{get\_emmeans}\NormalTok{(pwc\_B), }\AttributeTok{x =} \StringTok{"COND"}\NormalTok{, }\AttributeTok{y =} \StringTok{"emmean"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ conf.low, }\AttributeTok{ymax =}\NormalTok{ conf.high), }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_pvalue\_manual}\NormalTok{(pwc\_B, }\AttributeTok{hide.ns =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{tip.length =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_test\_label}\NormalTok{(MurrarB\_ANCOVA, }\AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{),}
        \AttributeTok{caption =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_pwc\_label}\NormalTok{(pwc\_B))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-433-1.pdf}
\textbf{Results}

\begin{quote}
A one-way analysis of covariance (ANCOVA) was conducted. The independent variable, condition, had two levels: Friends, Little Mosque. The dependent variable was attitudes towards Arabs expressed by the participant at post-test and covariate was the baseline assessment of the same variable. Descriptive statistics are presented in Table 1. A preliminary analysis evaluating the homogeneity-of-slopes assumption indicated that the relationship between the covariate and the dependent variable differed significantly as a function of the independent variable, \emph{F} (1, 189) = 4.297, \emph{p} = .040, \(\eta^2\) = 0.022. Regarding the assumption that the dependent variable is normally distributed in the population for any specific value of the covariate and for any one level of a factor, results of the Shapiro-Wilk test of normality on the model residuals was also significant,\emph{W} = 0.984, \emph{p} = .026. Only one datapoint (in the Little Mosque condition) had a standardized residual (-3.37) that exceeded an absolute value of 3.0. A non-significant Levene's test indicated no violation of the homogeneity of the residual variances for all groups, \emph{F}(1, 191) = 3.515 \emph{p} = .062.
\end{quote}

\begin{quote}
There was a non-significant effect of the baseline covariate on the post-test (\emph{F}{[}1, 190{]} = 0.665, \emph{p} = .416, \(\eta^2\) = 0.003). After controlling for the baseline attitudes toward Arabs, there was a statistically significant effect of condition on post-test attitudes toward Arabs, \emph{F}(1,190) = 26.361, \emph{p} \textless{} .001, \(\eta^2\) = 0.122. This appears to be moderate in size. Given there were only two conditions, no further follow-up was required. As illustrated in Figure 1, results suggest that those in the Little Mosque condition (\emph{M} = 73.92, \emph{SD} = 18.51) had more favorable attitudes toward Arabs than those in the Friends condition (\emph{M} = 59.02, \emph{SD} = 21.65). Means and covariate-adjusted means are presented in Table 1b.
\end{quote}

\hypertarget{scenario-2-controlling-for-a-confounding-or-covarying-variable}{%
\section{Scenario \#2: Controlling for a confounding or covarying variable}\label{scenario-2-controlling-for-a-confounding-or-covarying-variable}}

In the scenario below, I am simulating a one-way ANCOVA, predicting attitudes toward Arabs at post1 as a function of sitcom condition (Friends, Little Mosque), controlling for the participants' attitudes toward Whites. That is, the ANCOVA will compare the the means of the two groups (at post1, only), adjusted for level of attitudes toward Whites

TO BE CLEAR: This is not the best way to analyze this data. With such a strong, balanced design, the multi-way, mixed design ANOVAs were an excellent choice that provided much fuller information than this demonstration, below. The purpose of this over-simplified demonstration is merely to give another example of using a variable as a \emph{covariate} rather than a \emph{moderator.}

\hypertarget{preparing-the-data-2}{%
\subsection{Preparing the data}\label{preparing-the-data-2}}

When the covariate in ANCOVA is a potentially confounding variable, we need three variables:

\begin{itemize}
\tightlist
\item
  IV that has two or more levels; in our case it is the Friends and Littls Mosque sitcom conditions.
\item
  DV that is continuous; in our case it attitudes toward Arabs at post1 (AttArabP1).
\item
  Covariate that is continuous; in our case it attitudes toward Whites at post1 (AttWhiteP1). \emph{Note} We could have also chosen attitudes toward Whites at baseline.
\end{itemize}

We can continue using the Murrar\_wide df.

\hypertarget{checking-the-assumptions-1}{%
\subsection{Checking the assumptions}\label{checking-the-assumptions-1}}

There are a number of assumptions in ANCOVA. These include:

\begin{itemize}
\tightlist
\item
  random sampling
\item
  independence in the scores representing the dependent variable
\item
  linearity of the relationship between the covariate and DV within all levels of the independent variable
\item
  homogeneity of the regression slopes
\item
  a normally distributed DV for any specific value of the covariate and for any one level of a factor
\item
  homogeneity of variance
\end{itemize}

These are depicted in the flowchart, below.

\begin{figure}
\centering
\includegraphics{images/ANCOVA/wf_ANCOVA_assumptions.jpg}
\caption{Image of the ANCOVA workflow, showing our current place in the process}
\end{figure}

\hypertarget{linearity-assumption-1}{%
\subsubsection{Linearity assumption}\label{linearity-assumption-1}}

ANCOVA assumes that there is linearity between the covariate and outcome variable at each level of the grouping variable. In our case this means that there is linearity between the attitudes toward Whites (covariate) and attitudes toward Arabs (outcome variable) at each level of the intervention (Friends, Little Mosque).

We can create a scatterplot (with regression lines) between the covariate (attitudes toward Whites) and the outcome (attitudes toward Arabs).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggscatter}\NormalTok{(Murrar\_wide, }\AttributeTok{x =} \StringTok{"AttWhiteP1"}\NormalTok{, }\AttributeTok{y =} \StringTok{"AttArabP1"}\NormalTok{, }\AttributeTok{color =} \StringTok{"COND"}\NormalTok{,}
    \AttributeTok{add =} \StringTok{"reg.line"}\NormalTok{) }\SpecialCharTok{+}\NormalTok{ ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_regline\_equation}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =} \FunctionTok{paste}\NormalTok{(..eq.label..,}
\NormalTok{    ..rr.label.., }\AttributeTok{sep =} \StringTok{"\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}"}\NormalTok{), }\AttributeTok{color =}\NormalTok{ COND))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-434-1.pdf}
As we look at this scatterplot, we are trying to determine if there is an interaction effect (rather than a covarying effect). The linearity here, looks reasonable and not terribly ``interacting'' (to help us decide whether empathy should be a covariate or a moderator). More testing can help us make this distinction.

\hypertarget{homogeneity-of-regression-slopes-1}{%
\subsubsection{Homogeneity of regression slopes}\label{homogeneity-of-regression-slopes-1}}

This assumption requires that the slopes of the regression lines formed by the covariate and the outcome variable are the same for each group. The assumption evaluates that there is no interaction between the outcome and covariate. The plotted regression lines should be parallel.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND }\SpecialCharTok{*}\NormalTok{ AttWhiteP1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coefficient covariances computed by hccm()
\end{verbatim}

\begin{verbatim}
ANOVA Table (type II tests)

           Effect DFn DFd      F          p p<.05       ges
1            COND   1 189 26.240 0.00000074     * 0.1220000
2      AttWhiteP1   1 189  0.014 0.90700000       0.0000729
3 COND:AttWhiteP1   1 189  1.886 0.17100000       0.0100000
\end{verbatim}

Preliminary analysis supported ANCOVA as a statistical option in that there was no violation of the homogeneity of regression slopes as the interaction term was not statistically significant, \emph{F} (1, 189) = 1.886, \emph{p} = .171, \(\eta^2\) = 0.010.

\hypertarget{normality-of-residuals-1}{%
\subsubsection{Normality of residuals}\label{normality-of-residuals-1}}

Assessing the normality of residuals means running the model, capturing the unexplained portion of the model (i.e., the \emph{residuals}), and then seeing if they are normally distributed. Proper use of ANCOVA is predicated on normally distributed residuals.

We first compute the model with \emph{lm()}. The \emph{lm()} function is actually testing what we want to test. However, at this early stage, we are just doing a ``quick run and interpretation'' to see if we are within the assumptions of ANCOVA.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create a linear regression model predicting DV from COV \& IV}
\NormalTok{WhCov\_mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttWhiteP1 }\SpecialCharTok{+}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ Murrar\_wide)}
\NormalTok{WhCov\_mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = AttArabP1 ~ AttWhiteP1 + COND, data = Murrar_wide)

Coefficients:
     (Intercept)        AttWhiteP1  CONDLittleMosque  
       59.765300         -0.009897         14.886178  
\end{verbatim}

We can use the \emph{augment(model)} function rom the \emph{broom} package to add fitted values and residuals.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WhCov\_mod.metrics }\OtherTok{\textless{}{-}}\NormalTok{ broom}\SpecialCharTok{::}\FunctionTok{augment}\NormalTok{(WhCov\_mod)}
\CommentTok{\# shows the first three rows of the UEcon\_model.metrics}
\FunctionTok{head}\NormalTok{(WhCov\_mod.metrics, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 9
  AttArabP1 AttWhiteP1 COND    .fitted .resid   .hat .sigma .cooksd .std.resid
      <dbl>      <dbl> <fct>     <dbl>  <dbl>  <dbl>  <dbl>   <dbl>      <dbl>
1      80.3       95.6 Friends    58.8   21.4 0.0176   20.2 0.00685      1.07 
2      76.6       51.0 Friends    59.3   17.3 0.0203   20.2 0.00518      0.867
3      92.0       91.9 Friends    58.9   33.2 0.0152   20.1 0.0140       1.65 
\end{verbatim}

Now we assess the normality of residuals using the Shapiro Wilk test. The script below captures the ``.resid'' column from the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstatix}\SpecialCharTok{::}\FunctionTok{shapiro\_test}\NormalTok{(WhCov\_mod.metrics}\SpecialCharTok{$}\NormalTok{.resid)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  variable                 statistic p.value
  <chr>                        <dbl>   <dbl>
1 WhCov_mod.metrics$.resid     0.984  0.0294
\end{verbatim}

The statistically significant Shapiro Wilk test indicate a violation of the normality assumption (\emph{W} = 0.984, \emph{p} = .029). As I mentioned before, there are better ways to analyze this research vignette. None-the-less, we will continue with this demonstration so that you will have the procedural and conceptual framework for conducting ANCOVA.

\hypertarget{homogeneity-of-variances-1}{%
\subsubsection{Homogeneity of variances}\label{homogeneity-of-variances-1}}

ANCOVA presumes that the variance of the residuals is equal for all groups. We can check this with the Levene's test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WhCov\_mod.metrics }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{levene\_test}\NormalTok{(.resid }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
    df1   df2 statistic      p
  <int> <int>     <dbl>  <dbl>
1     1   191      4.54 0.0344
\end{verbatim}

Contributing more evidence that ANCOVA is not the best way to analyze this data, a statistically significant Levene's test indicates a violation of the homogeneity of the residual variances (\emph{F}{[}1, 191{]} = 4.539, \emph{p} = .034).

\hypertarget{outliers-1}{%
\subsubsection{Outliers}\label{outliers-1}}

We can identify outliers by examining the standardized (or studentized) residual. This is the residual divided by its estimated standard error. Standardized residuals are interpreted as the number of standard errors away from the regression line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WhCov\_mod.metrics }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(}\FunctionTok{abs}\NormalTok{(.std.resid) }\SpecialCharTok{\textgreater{}} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{as.data.frame}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  AttArabP1 AttWhiteP1         COND  .fitted    .resid       .hat   .sigma
1     6.137     59.518 LittleMosque 74.06242 -67.92542 0.01407535 19.65185
     .cooksd .std.resid
1 0.05447684  -3.383443
\end{verbatim}

There is one outlier with a standardized residual with an absolute value greater than 3. At this point I am making a mental note of this. If this were ``for real'' I might more closely inspect these data. I would look at the whole response. If any response seems invalid (e.g., random, erratic, or extreme responding) I would delete it. If the response seem valid, I \emph{could} truncate them to within 3 SEs. I could also ignore it. Kline \citeyearpar{kline_principles_2016} has a great section on some of these options.

\hypertarget{write-up-of-assumptions-1}{%
\subsubsection{Write-up of Assumptions}\label{write-up-of-assumptions-1}}

\begin{quote}
A one-way analysis of covariance (ANCOVA) was conducted. The independent variable, sitcom condition, had two levels: Friends, Little Mosque. The dependent variable was attitudes towards Arabs at pre-test. Preliminary anlayses which tested the assumptions of ANCOVA were mixed. Results suggesting that the relationship between the covariate and the dependent variable did not differ significantly as a function of the independent variable (\emph{F} {[}1, 189{]} = 1.886, \emph{p} = .171, \(\eta^2\) = 0.010) provided evidence that we did not violate the homogeneity-of-slopes assumption. In contrast, the Shapiro-Wilk test of normality on the model residuals was statistically significant (\emph{W} = 0.984, \emph{p} = .029). This means that we likely violated the assumption that the dependent variable is normally distributed in the population for any specific value of the covariate and for any one level of a factor. Regarding outliers, one datapoint (-3.38) had a standardized residual that exceeded an absolute value of 3.0. Further, a statistically significant Levene's test indicated a violation of the homogeneity of the residual variances for all groups, (\emph{F}{[}1, 191{]} = 4.539, \emph{p} = .034).
\end{quote}

Because the intent of this analysis was to demonstrate how ANCOVA differs from mixed design ANOVA we proceeded with the analysis. Were this for ``real research'' we would have chosen a different analysis.

\hypertarget{calculating-the-omnibus-anova-1}{%
\subsection{Calculating the Omnibus ANOVA}\label{calculating-the-omnibus-anova-1}}

We are ready to conduct the omnibus ANOVA.

\begin{figure}
\centering
\includegraphics{images/ANCOVA/wf_ANCOVA_omnibus.jpg}
\caption{Image of the ANCOVA workflow, showing our current place in the process.}
\end{figure}

\emph{Order of variable entry} matters in ANCOVA. Thinking of the \emph{controlling for} language associated with covariates, we firstly want to remove the effect of the covariate.

In the code below we are predicting attitudes toward Arabs at post1 from attitudes toward Whites at post1 (the covariate) and sitcom condition (Friends, Little Mosque).

The \emph{ges} column provides the effect size, \(\eta^2\) where a general rule-of-thumb for interpretation is .01 (small), .06 (medium), and .14 (large) \citep{lakens_calculating_2013}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{WhCov\_ANCOVA }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{anova\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ AttWhiteP1 }\SpecialCharTok{+}\NormalTok{ COND)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Coefficient covariances computed by hccm()
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rstatix}\SpecialCharTok{::}\FunctionTok{get\_anova\_table}\NormalTok{(WhCov\_ANCOVA)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ANOVA Table (type II tests)

      Effect DFn DFd      F           p p<.05       ges
1 AttWhiteP1   1 190  0.014 0.907000000       0.0000722
2       COND   1 190 26.119 0.000000779     * 0.1210000
\end{verbatim}

There was a non-significant effect of the attitudes toward Whites covariate on the attitudes toward Arabs at post-test, \emph{F} (1, 190) = 0.014, \emph{p} = .907, \(\eta^2\) \textless{} .001. After controlling for attitudes toward Whites, there was a statistically significant effect in attitudes toward Arabs at post-test between the conditions, \emph{F}(1, 190) = 26.119, \emph{p} \textless{} .001, \(\eta^2\) = 0.121. The effect size was moderate.

\hypertarget{post-hoc-pairwise-comparisons-controlling-for-the-covariate-1}{%
\subsection{Post-hoc pairwise comparisons (controlling for the covariate)}\label{post-hoc-pairwise-comparisons-controlling-for-the-covariate-1}}

With only two levels of sitcom condition (Friends, Little Mosque), we do not need to conduct post-hoc pairwise comparisons. However, because many research designs involve three or more levels, I will use code that would evaluates them here.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc\_cond }\OtherTok{\textless{}{-}}\NormalTok{ Murrar\_wide }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{emmeans\_test}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{covariate =}\NormalTok{ AttWhiteP1, }\AttributeTok{p.adjust.method =} \StringTok{"none"}\NormalTok{)}
\NormalTok{pwc\_cond}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 9
  term          .y.   group1 group2    df statistic       p   p.adj p.adj.signif
* <chr>         <chr> <chr>  <chr>  <dbl>     <dbl>   <dbl>   <dbl> <chr>       
1 AttWhiteP1*C~ AttA~ Frien~ Littl~   190     -5.11 7.79e-7 7.79e-7 ****        
\end{verbatim}

Results suggest a statistically significant post-test difference between the Friends and Little Mosque sitcom conditions.
With the script below we can obtain the covariate-adjusted marginal means. These are termed \emph{estimated marginal means.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{emmeans\_cond }\OtherTok{\textless{}{-}}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_emmeans}\NormalTok{(pwc\_cond)}
\NormalTok{emmeans\_cond}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 8
  AttWhiteP1 COND         emmean    se    df conf.low conf.high method      
       <dbl> <fct>         <dbl> <dbl> <dbl>    <dbl>     <dbl> <chr>       
1       74.4 Friends        59.0  2.04   190     55.0      63.1 Emmeans test
2       74.4 LittleMosque   73.9  2.08   190     69.8      78.0 Emmeans test
\end{verbatim}

As before, these means are usually different (even if only ever-so-slightly) than the raw means you would obtain from the descriptives.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{descripts\_cond }\OtherTok{\textless{}{-}}\NormalTok{ psych}\SpecialCharTok{::}\FunctionTok{describeBy}\NormalTok{(AttArabP1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ COND, }\AttributeTok{data =}\NormalTok{ Murrar\_wide,}
    \AttributeTok{mat =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{descripts\_cond}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    item       group1 vars  n     mean       sd  median  trimmed      mad   min
X11    1      Friends    1 98 59.02351 21.65024 57.9955 59.31306 23.67045 8.297
X12    2 LittleMosque    1 95 73.92134 18.51082 74.4600 75.52858 15.98984 6.137
    max  range       skew   kurtosis       se
X11 100 91.703 -0.0518848 -0.6252126 2.187005
X12 100 93.863 -0.9798189  1.6335325 1.899170
\end{verbatim}

\hypertarget{toward-an-apa-style-results-section-1}{%
\subsection{Toward an APA style results section}\label{toward-an-apa-style-results-section-1}}

Tables with the means, adjusted means, and pairwise comparison output may be helpful. The \emph{apa.cor.table()} function in the \emph{apaTables} package is helpful for providing means, standarddeviations, and correlations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{apaTables}\SpecialCharTok{::}\FunctionTok{apa.cor.table}\NormalTok{(Murrar\_wide[}\FunctionTok{c}\NormalTok{(}\StringTok{"AttArabP1"}\NormalTok{, }\StringTok{"AttWhiteP1"}\NormalTok{)], }\AttributeTok{table.number =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}


Table 2 

Means, standard deviations, and correlations with confidence intervals
 

  Variable      M     SD    1          
  1. AttArabP1  66.36 21.46            
                                       
  2. AttWhiteP1 74.37 17.28 -.02       
                            [-.16, .12]
                                       

Note. M and SD are used to represent mean and standard deviation, respectively.
Values in square brackets indicate the 95% confidence interval.
The confidence interval is a plausible range of population correlations 
that could have caused the sample correlation (Cumming, 2014).
 * indicates p < .05. ** indicates p < .01.
 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# You can save this as a Microsoft word document by adding this}
\CommentTok{\# statement into the command: filename = \textquotesingle{}your\_filename.doc\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

Writing this output to excel files helped create the two tables that follow.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(pwc\_cond, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"pwc\_con.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(emmeans\_cond, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"emmeans\_con.csv"}\NormalTok{)}
\NormalTok{MASS}\SpecialCharTok{::}\FunctionTok{write.matrix}\NormalTok{(descripts\_cond, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{file =} \StringTok{"descripts\_con.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Ultimately, I would want a table that included this information. Please refer to the APA style manual for more proper formatting for a manuscript that requires APA style.

\begin{longtable}[]{@{}l@{}}
\toprule()
Table 1b \\
\midrule()
\endhead
Unadjusted and Covariate-Adjusted Descriptive Statistics \\
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}lcc@{}}
\toprule()
Condition & Unadjusted & Covariate-Adjusted \\
\midrule()
\endhead
\bottomrule()
\end{longtable}

\begin{longtable}[]{@{}lcccc@{}}
\toprule()
& \emph{M} & \emph{SD} & \emph{EMM} & \emph{SE} \\
\midrule()
\endhead
Friends & 59.02 & 21.65 & 59.03 & 2.04 \\
Little Mosque & 73.92 & 18.51 & 73.92 & 2.08 \\
\bottomrule()
\end{longtable}

Unlike the figure we created when we were testing assumptions, this script creates a plot from the model (which identifies AttWhiteP1 in its role as covariate). Thus, the relationship between condition and AttArabP1 controls for the effect of the AttArabB covariate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pwc\_cond }\OtherTok{\textless{}{-}}\NormalTok{ pwc\_cond }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{    rstatix}\SpecialCharTok{::}\FunctionTok{add\_xy\_position}\NormalTok{(}\AttributeTok{x =} \StringTok{"COND"}\NormalTok{, }\AttributeTok{fun =} \StringTok{"mean\_se"}\NormalTok{)}
\NormalTok{ggpubr}\SpecialCharTok{::}\FunctionTok{ggline}\NormalTok{(rstatix}\SpecialCharTok{::}\FunctionTok{get\_emmeans}\NormalTok{(pwc\_B), }\AttributeTok{x =} \StringTok{"COND"}\NormalTok{, }\AttributeTok{y =} \StringTok{"emmean"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ conf.low, }\AttributeTok{ymax =}\NormalTok{ conf.high), }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    ggpubr}\SpecialCharTok{::}\FunctionTok{stat\_pvalue\_manual}\NormalTok{(pwc\_B, }\AttributeTok{hide.ns =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{tip.length =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{subtitle =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_test\_label}\NormalTok{(WhCov\_ANCOVA, }\AttributeTok{detailed =} \ConstantTok{TRUE}\NormalTok{),}
        \AttributeTok{caption =}\NormalTok{ rstatix}\SpecialCharTok{::}\FunctionTok{get\_pwc\_label}\NormalTok{(pwc\_cond))}
\end{Highlighting}
\end{Shaded}

\includegraphics{ReCenterPsychStats_files/figure-latex/unnamed-chunk-447-1.pdf}
\textbf{Results}

\begin{quote}
A one-way analysis of covariance (ANCOVA) was conducted. The independent variable, sitcom condition, had two levels: Friends, Little Mosque. The dependent variable was attitudes towards Arabs at pre-test. Preliminary analyses which tested the assumptions of ANCOVA were mixed. Results suggesting that the relationship between the covariate and the dependent variable did not differ significantly as a function of the independent variable (\emph{F} {[}1, 189{]} = 1.886, \emph{p} = .171, \(\eta^2\) = 0.010) provided evidence that we did not violate the homogeneity-of-slopes assumption. In contrast, the Shapiro-Wilk test of normality on the model residuals was statistically significant (\emph{W} = 0.984, \emph{p} = .029). This means that we likely violated the assumption that the dependent variable is normally distributed in the population for any specific value of the covariate and for any one level of a factor. Regarding outliers, one datapoint (-3.38) had a standardized residual that exceeded an absolute value of 3.0. Further, a statistically significant Levene's test indicated a violation of the homogeneity of the residual variances for all groups, (\emph{F}{[}1, 191{]} = 4.539, \emph{p} = .034).
\end{quote}

Because the intent of this analysis was to demonstrate how ANCOVA differs from mixed design ANOVA we proceeded with the analysis. Were this for ``real research'' we would have chosen a different analysis.

\begin{quote}
There was a non-significant effect of the attitudes toward Whites covariate on the attitudes toward Arabs post-test, \emph{F} (1,190) = 0.014, \emph{p} = .907, \(\eta^2\) \textless{} .001. After controlling for attitudes toward Whites, there was a statistically significant effect in attitudes toward Arabs at post-test between the conditions, \emph{F}(1. 190) = 26.119, \emph{p} \textless{} .001, \(\eta^2\) = 0.121. The effect size was moderately large. Means and covariate-adjusted means are presented in Table 1b.
\end{quote}

\hypertarget{more-and-a-recap-on-covariates}{%
\section{More (and a recap) on covariates}\label{more-and-a-recap-on-covariates}}

Covariates, sometimes termed \emph{controls} are often used to gain statistical control over variables that are difficult to control in a research design. That is, it may be impractical for polychotomize an otherwise continuous variable and/or it is impractical to have multiple factors and so a covariate is a more manageable approach. Common reasons for including covariates include \citep{bernerth_critical_2016}:

\begin{itemize}
\tightlist
\item
  they mathematically remove variance associated with nonfocal variables,
\item
  the \emph{purification principle} -- removing unwanted or confusing variance,
\item
  they remove the \emph{noise} in the analysis to clear up the clear up the relationship between IV and DVs.
\end{itemize}

Perhaps it is an oversimplification, but we can think of three categories of variables: moderators, covariates, and mediators. Through ANOVA and ANCOVA, we distinguish between moderator and covariate.

\textbf{Moderator}: a variable that changes the strength or direction of an effect between two variables X (predictor, independent variable) and Y (criterion, dependent variable).

\textbf{Covariate}: an observed, continuous variable, that (when used properly) has a relationship with the dependent variable. It is included in the analysis, as a predictor, so that the predictive relationship between the independent (IV) and dependent (DV) are adjusted.

Bernerth and Aguinis \citeyearpar{bernerth_critical_2016} conducted a review of how and when control variables were used in nearly 600 articles published between 2003 and 2012. Concurrently with their analysis, they provided guidance for when to use control variables (covariates). The flowchart that accompanies their article is quite helpful. Control variables (covariates) should only be used when:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Theory suggests that the potential covariate(s) relate(s) to variable(s) in the currrent study.
\item
  There is empirical justification for including the covariate in the study.
\item
  The covariate can be measured reliably.
\end{enumerate}

Want more? Instructions for calculating a two-way ANCOVA are here: \url{https://www.datanovia.com/en/lessons/ancova-in-r/}

\hypertarget{practice-problems-8}{%
\section{Practice Problems}\label{practice-problems-8}}

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that offer differing levels of difficulty. Whichever you choose, you will focus on these larger steps in one-way ANCOVA, including:

\begin{itemize}
\tightlist
\item
  test the statistical assumptions
\item
  conduct an ANCOVA
\item
  if the predictor variable has more three or more levels, conduct follow-up testing
\item
  present both means and coviarate-adjusted means
\item
  write a results section to include a figure and tables
\end{itemize}

\hypertarget{problem-1-play-around-with-this-simulation.-3}{%
\subsection{Problem \#1: Play around with this simulation.}\label{problem-1-play-around-with-this-simulation.-3}}

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.

\begin{itemize}
\tightlist
\item
  If ANCOVA is new to you, perhaps you just change the number in ``set.seed(210813)'' from 210813 to something else. Then rework Scenario\#1, Scenario\#2, or both. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.
\item
  If you are interested in power, change the sample size to something larger or smaller.
\item
  If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.
\end{itemize}

\hypertarget{problem-2-conduct-a-one-way-ancova-with-the-dv-and-covariate-at-post2.}{%
\subsection{Problem \#2: Conduct a one-way ANCOVA with the DV and covariate at post2.}\label{problem-2-conduct-a-one-way-ancova-with-the-dv-and-covariate-at-post2.}}

The Murrar et al. \citeyearpar{murrar_entertainment-education_2018}article has three waves: baseline, post1, post2. In this lesson, I focused on the post1 waves. Rerun this analysis using the post2 wave data.

\hypertarget{problem-3-try-something-entirely-new.-3}{%
\subsection{Problem \#3: Try something entirely new.}\label{problem-3-try-something-entirely-new.-3}}

Using data for which you have permission and access (e.g., IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete an ANCOVA.

\hypertarget{grading-rubric-5}{%
\subsection{Grading Rubric}\label{grading-rubric-5}}

Regardless which option(s) you chose, use the elements in the grading rubric to guide you through the practice.
Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5493}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2535}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1972}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Assignment Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Possible
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Points Earned
\end{minipage} \\
\midrule()
\endhead
1. Check and, if needed, format data & 5 & \_\_\_\_\_ \\
2. Evaluate statistical assumptions & 5 & \_\_\_\_\_ \\
3. Conduct omnibus ANCOVA (w effect size) & 5 & \_\_\_\_\_ \\
4. If the IV has three or more levels, conduct follow-up tests & 5 & \_\_\_\_\_ \\
5. Present means and covariate-adjusted means; interpret them & 5 & \_\_\_\_\_ \\
6. APA style results with table(s) and figure & 5 & \_\_\_\_\_ \\
7. Explanation to grader & 5 & \_\_\_\_\_ \\
\textbf{Totals} & 35 & \_\_\_\_\_ \\
\bottomrule()
\end{longtable}

\hypertarget{refs}{%
\chapter*{References}\label{refs}}
\addcontentsline{toc}{chapter}{References}

  \bibliography{STATSnMETH.bib}

\end{document}
