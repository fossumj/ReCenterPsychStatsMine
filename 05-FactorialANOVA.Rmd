# Factorial (Between-Subjects) ANOVA {#between}

```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

[Screencasted Lecture Link](https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=3bb1bee1-c2ac-4cda-95f2-ad8b0152132c) 

In this lesson we conduct a 3X2 ANOVA. The lesson is long and somewhat complicated.  We will

* Work an actual example from the literature.
  - "by hand", and
  - with R packages
* I will also demonstrate 
  - several options for exploring interaction effects, and
  - several options for exploring main effects.
* Exploring these options will allow us to:
  - Gain familiarity with the concepts central to multi-factor ANOVAs.
  - Explore tools for analyzing the complexity in designs.

The complexity is that not all of these things need to be conducted for every analysis. The Two-Way ANOVA Decision-Tree is provided to help you map a way through your own analyses. I will periodically indicate this map in the lesson so that we can more easily keep track of where we are in the process.
  

## Navigating this Lesson

There is about 1 hour and 30 minutes hours of lecture.  If you work through the materials with me plan for another two hours of study.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReCenterPsychStats) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Learning objectives from this lecture include the following:

* Define, locate, and interpret all the effects associated with two-way ANOVA:
  - main
  - interaction (introducing the concept, *moderator*)
  - simple main effects
* Identify which means belong with which effects.  Then compare and interpret them.
  - marginal means
  - individual cell means 
  - comparing them
* Map a process/workflow for investigating a factorial ANOVA
* Manage Type I error
* Conduct a power analysis to determine sample size 


### Planning for Practice

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results *should* map onto the ones obtained in the lecture. 

The second option comes from the research vignette. The Ramdhani et al. [-@ramdhani_affective_2018] article has two dependent variables (negative and positive evaluation) which are suitable for two-way ANOVAs.  I will demonstrate a simulation of one of their 2X3 ANOVAs (negative) in this lecturette. The second dependent variable (positive) is suggested for the moderate level of difficulty. 

As a third option, you are welcome to use data to which you have access and is suitable for two-way ANOVA. In either case, you will be expected to:

* test the statistical assumptions
* conduct a two-way ANOVA, including
  - omnibus test and effect size
  - report main and interaction effects
  - conduct follow-up testing of simple main effects
* write a results section to include a figure and tables

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s) that are freely available on the internet.  Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Navarro, D. (2020). Chapter 16: Factorial ANOVA. In *Learning Statistics with R - A tutorial for Psychology Students and other Beginners.* Retrieved from https://stats.libretexts.org/Bookshelves/Applied_Statistics/Book%3A_Learning_Statistics_with_R_-_A_tutorial_for_Psychology_Students_and_other_Beginners_(Navarro)
  - Navarro's OER includes a good mix of conceptual information about factorial ANOVA as well as R code.  My code/approach is a mix of Field's, Navarro's, and other techniques I have found on the internet and learned from my students.
* Ramdhani, N., Thontowi, H. B., & Ancok, D. (2018). Affective Reactions Among Students Belonging to Ethnic Groups Engaged in Prior Conflict. *Journal of Pacific Rim Psychology, 1*2, e2. https://doi.org/10.1017/prp.2017.22

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r }
#will install the package if not already installed
#if(!require(psych)){install.packages("psych")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(lsr)){install.packages("lsr")}
#if(!require(car)){install.packages("car")}
#if(!require(ggpubr)){install.packages("ggpubr")}
#if(!require(pwr2)){install.packages("pwr2")}
#if(!require(apaTables)){install.packages("apaTables")}
```

## Introducing Factorial ANOVA

My approach to teaching is to address the conceptual as we work problems.  That said, there are some critical ideas we should address first.

**ANOVA is for experiments** (or arguably closely related designs).  As we get into the assumptions you'll see that it has some interesting ones (e.g., there should be an equal/equivalent number of cases per cell).  To the degree that we violate these assumptions, we should locate alternative statistical approaches where these assumptions are relaxed.

**Factorial**: the term used when there are two or more independent variables (the factors).  The factors could be between-groups, within-groups, repeated measures, or a combination of between and within.  

* **Independent factorial design**: several IVs (predictors/factors) and each has been measured using different participants (between groups).
* **Related factorial design**: several IVs (factors/predictors) have been measured, but the same participants have been used in all conditions (repeated measures).
* **Mixed design**: several IVs (factors/predictors) have been measured; some with different participants whereas others use the same participants. A combo of independent (between) and related (within) designs.
*  Factor naming follows a number/levels convention.  
  - Today's example is a 3X2 ANOVA.  We know there are two factors that have three and two levels, respectively: *Ethnicity*  has three levels representing the two ethnic groups that were in prior conflict (Marudese, Dayaknese) and a third group who was uninvolved in the conflict (Javanese); *Photo stimulus* has two levels representing members of the two ethnic groups that were in prior conflict (Madurese, Dayaknese);  

 **Moderator** is what creates an interaction.  Below are traditional representations of the *statistical* and *conceptual* figures of interaction effects. We will say that Factor B, *moderates* the relationship between Factor A/IV and the DV.
 
In a later lesson we work an ANCOVA -- where we will distinguish between a *moderator* and a *covariate.*  In regression models, you will likely be introduced to the *mediator.*  

![Graphic representations of a moderated relationship?](images/factorial/modfigs.jpg){#id .class width=1000 height=400px}

### Workflow for Two-Way ANOVA

The following is a proposed workflow for conducting a two-way ANOVA. 

![An image of a workflow for the two-way ANOVA](images/factorial/TwoWayWrkFlo.jpg) 


Steps of the workflow include:

1. Enter data 
    + seems straightforward; understanding the format of data can often provide clues as to which ANOVA/statistic to use
    + predictors should formatted as as factors (ordered or unordered); the dependent variable should be continuously scaled
2. Explore data 
    + graphing the data 
    + computing descriptive statistics
    + check distributional assumptions 
    + use Levene's test to check for homogeneity of variance 
    + use Shapiro Wilks to check for normality
3. Construct or choose contrasts
    + select contrasts & specify for all of the independent variables in your analysis
    + if you want to use Type III sums of squares, contrasts must be orthogonal
4. Compute the omnibus ANOVA
    + *Depending on what you found in the data exploration phase, you may need to run a robust version of the test*
5. Follow-up testing based on significant main or interaction effects
    + significant interactions require test of simple main effects which could be urther explored with contrasts, posthoc acomparisons, and/or polynomials
    + *the exact methods you choose will depend upon the tests of assumptions during data exploration*
6. Managing Type I error

## Research Vignette

The research vignette for this example was located in Kalimantan, Indonesia and focused on bias in young people from three ethnic groups. The Madurese and Dayaknese groups were engaged in ethnic conflict that spanned 1996 to 2001. The last incidence of mass violence was in 2001 where approximately 500 people (mostly from the Madurese ethnic group)were expelled from the province. Ramdhani et al.'s [-@ramdhani_affective_2018] research hypotheses were based on the roles of the three ethnic groups in the study.  The Madurese appear to be viewed as the transgressors when they occupied lands and took employment and business opportunities from the Dayaknese. Ramdhani et al. also included a third group who were not involved in the conflict (Javanese). The research participants were students studying in Yogyakara who were not involved in the conflict.  They included 39 Madurese, 35 Dyaknese, and 37 Javanese; 83 were male and 28 were female.

In the study [@ramdhani_affective_2018], participants viewed facial pictures of three men and three women (in traditional dress) from each ethnic group (6 photos per ethnic group). Participant were asked, "How do you feel when you see this photo?  Please indicate your answers based on your actual feelings."  Participants responded on a 7-point Likert scale ranging from 1 (*strongly disagree*) to 7 (*strongly agree*. Higher scores indicated ratings of higher intensity on that scale.  The two scales included the following words:

* Positive:  friendly, kind, helpful, happy
* Negative:  disgusting, suspicious, hateful, angry

Below is script to simulate data for the negative reactions variable from the information available from the manuscript [@ramdhani_affective_2018].


```{r}
library(tidyverse)
set.seed(210731)
Negative<-round(c(rnorm(17,mean=1.91,sd=0.73),rnorm(18,mean=3.16,sd=0.19),rnorm(19, mean=3.3, sd=1.05), rnorm(20, mean=3.00, sd=1.07), rnorm(18, mean=2.64, sd=0.95), rnorm(19, mean=2.99, sd=0.80)),3) #sample size, M and SD for each cell; this will put it in a long file
Positive<-round(c(rnorm(17,mean=4.99,sd=1.38),rnorm(18,mean=3.83,sd=1.13),rnorm(19, mean=4.2, sd=0.82), rnorm(20, mean=4.19, sd=0.91), rnorm(18, mean=4.17, sd=0.60), rnorm(19, mean=3.26, sd=0.94)),3) #sample size, M and SD for each cell; this will put it in a long file
ID <- factor(seq(1,111))
Rater <- c(rep("Dayaknese",35), rep("Madurese", 39), rep ("Javanese", 37))
Photo <- c(rep("Dayaknese", 17), rep("Madurese", 18), rep("Dayaknese", 19), rep("Madurese", 20), rep("Dayaknese", 18), rep("Madurese", 19))
Ramdhani_df<- data.frame(ID, Negative, Positive, Rater, Photo) #groups the 3 variables into a single df:  ID#, DV, condition
#ANOVAresults<-aov(Negative~Rater*Photo, Ramdhani_df) #runs the ANOVA -- I used this in the simulation to check my work
#summary(ANOVAresults) #ANOVA output
#model.tables(ANOVAresults,"means") #extracts the means for the 3 groups
```

For two-way ANOVA our variables need to be properly formatted. In our case:

* Negative is a continuously scaled DV and should be *num*
* Positive is a continuously scaled DV and should be *num*
* Rater should be an unordered factor
* Photo should be an unordered facor

```{r}
str(Ramdhani_df)
```
Our Negative variable is correctly formatted. Let's reformat Numeric and Photo to be factors. And ask for the structure again. In the absence of instruction, R will order the factors alphabetically. In this case this is fine.  If we had ordered factors such as dosage (placebo, lo, hi) we would want to specify the order.

```{r}
Ramdhani_df[,'Rater'] <- as.factor(Ramdhani_df[,'Rater'])
Ramdhani_df[,'Photo'] <- as.factor(Ramdhani_df[,'Photo'])
str(Ramdhani_df)
```
If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv ("Excel lite") or .rds (R object) file. This is not a necessary step.

The code for .csv will likely lose the formatting (i.e., making the Rater and Photo variables factors), but it is easy to view in Excel.
```{r}
#write the simulated data  as a .csv
#write.table(Ramdhani_df, file="RamdhaniCSV.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#Ramdhani_df <- read.csv ("RamdhaniCSV.csv", header = TRUE)
#str(Ramdhani_df)
```

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R.
```{r}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(Ramdhani_df, "Ramdhani_RDS.rds")
#bring back the simulated dat from an .rds file
#Ramdhani_RDS <- readRDS("Ramdhani_RDS.rds")
#str(Ramdhani_RDS)
```

### Preliminary exploration of our research vignette

Let's first examine the descriptive statistic (e.g., means of Negative) by group. We can use the *describeBy()* function from the *psych* package.

```{r }
library(psych)
negative.descripts <- psych::describeBy(Negative ~ Rater + Photo, mat = TRUE, data = Ramdhani_df, digits = 3) #digits allows us to round the output
negative.descripts
```

The *write.table()* function can be a helpful way to export output to .csv files so that you can manipulate it into tables. 

```{r }
write.table(negative.descripts, file="NegativeDescripts.csv", sep=",", col.names=TRUE, row.names=FALSE)
```

At this stage, it would be useful to plot our data. Figures can really help us conceptualize our analysis.  

```{r }
library(ggpubr)
ggboxplot(Ramdhani_df, x = "Rater", y = "Negative", color = "Photo",xlab = "Ethnicity of Rater", ylab = "Negative Reaction")
```
Narrating results is sometimes made easier if variables are switched. There is usually not a right or wrong answer. Here is another view, switching the Rater and Photo predictors.

```{r}
ggboxplot(Ramdhani_df, x = "Photo", y = "Negative", color = "Rater", xlab = "Photo Stimulus",
             ylab = "Negative Reaction")
```

```{r }
ggline(Ramdhani_df, x = "Rater", y = "Negative", color = "Photo", xlab = "Ethnicity of Rater",
             ylab = "Negative Reaction", add = c("mean_se", "dotplot"))

#add this for a different color palette:  palette = c("#00AFBB", "#E7B800")
```


```{r }
ggline(Ramdhani_df, x = "Photo", y = "Negative", color = "Rater", xlab = "Photo Stimulus",
             ylab = "Negative Reaction", add = c("mean_se", "dotplot"))
```


## Working the Factorial ANOVA (by hand)

Before we work an ANOVA let's take a moment to consider what we are doing and how it informs our decision-making. This figure (which already contains "the answers") may help conceptualize how variance becomes partitioned.

![Image of a flowchart that partitions variance from sums of squares totals to its component pieces](images/factorial/partition.png)

As in one-way ANOVA, we partition variance into **total**, **model**, and **residual**.  However, we now further divide the $SS_M$ into its respective factors A(column), B(row) and their a x b product.


In this, we begin to talk about main effects and interactions.

### Sums of Squares Total

Our formula is the same as before:

$$SS_{T}= \sum (x_{i}-\bar{x}_{grand})^{2}$$
Let's calculate it for the Ramdhani et al. [-@ramdhani_affective_2018] data.
Our grand (i.e., overall) mean is 

```{r }
mean(Ramdhani_df$Negative)
```

Subtracting the grand mean from each Accurate score yields a mean difference.
```{r }
library(tidyverse)
Ramdhani_df <- Ramdhani_df %>% 
  mutate(m_dev = Negative-mean(Negative))
```
Pop quiz:  What's the sum of our new *m_dev* variable?

Let's find out!
```{r }
sum(Ramdhani_df$m_dev)
```

If we square those mean deviations:
```{r }
Ramdhani_df <- Ramdhani_df %>% 
  mutate(m_devSQ = m_dev^2)
```

If we sum the squared mean deviations:

```{r }
sum(Ramdhani_df$m_devSQ)
```
This value, 114.775, the sum of squared deviations around the grand mean, is our $SS_T$; the associated *degrees of freedom* is $N$ - 1.  
In factorial ANOVA, we divide $SS_T$ into **model/between sums of squares** and **residual/within sums of squares**.

### Sums of Squares for the Model 

$$SS_{M}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$$

The *model* generally represents the notion that the means are different than each other.  We want the variation between our means to be greater than the variation within each of the groups from which our means are calculated.

In factorial, we need to obtain means for each of the combinations of the factors. We have a 3 x 2:

* Rater with three levels:  Dayaknese, Madurese, Javanese
* Photo with two levels:  Dayaknese, Madurese

Let's repeat some code we used before to get the cell-level means and sample size.

```{r}
psych::describeBy(Negative ~ Rater + Photo, mat = TRUE, data = Ramdhani_df, digits = 3)
```

We also need the grand mean (i.e., the mean that disregards the factors).
```{r }
mean(Ramdhani_df$Negative)
```
This formula occurs in six chunks, representing the six cells of our designed.  In each of the chunks we have the $n$, group mean, and grand mean.

```{r }
17*(1.818 - 2.947)^2 + 18*(2.524 - 2.947)^2 + 19*(3.301 - 2.947)^2 + 18*(3.129 - 2.947)^2 + 19*(3.465 - 2.947)^2 + 20*(3.297 - 2.947)^2
```
This value, 35.415, $SS_M$ is the value accounted for by the model -- the proportion of variance accounted for by the grouping variables/factors, Rater and Photo.

### Sums of Squares Residual (or within)

$SS_R$ is error associated with within group variability – If people are randomly assigned to treatment group there should be no other covariate (confounding variable) so that all SSR variability is *uninteresting* for the research and treated as noise.

$$SS_{R}= \sum(x_{ik}-\bar{x}_{k})^{^{2}}$$
Here's another configuration of the same:

$$SS_{R}= s_{group1}^{2}(n-1) + s_{group2}^{2}(n-1) + s_{group3}^{2}(n-1) + s_{group4}^{2}(n-1) + s_{group5}^{2}(n-1) + s_{group6}^{2}(n-1))$$

Again, the formula is in six chunks -- but this time the calculations are *within-group*.  We need the variance (the standard deviation squared) for the calculation. Let's take another look at our descriptives.

```{r}
psych::describeBy(Negative ~ Rater + Photo, mat = TRUE, data = Ramdhani_df, digits = 3)
```

Calculating $SS_R$
```{r}
((.768^2)*(17-1))+ ((.742^2)*(18-1)) + ((1.030^2)*(19-1)) + ((.156^2)*(18-1)) + ((.637^2)*(19-1)) + ((1.332^2)*(20-1))
```

The value for our $SS_R$ is 79.321.  It's degrees of freedom is $N - k$, meaning the total $N$ minus the number of groups:  
```{r}
111 - 6
```

### Relationship between $SS_T$, $SS_M$, and $SS_R$.  In case it's not clear:

$SS_T = SS_M + SS_R$
In our case:

* $SS_T$ was 114.775
* $SS_M$ was 35.415
* $SS_R$ was 79.321

Considering rounding error, we were successful!
```{r }
35.415 + 79.321
```

### But we need to understand the SS for each of the factors and their product.

#### Rater Main Effect

$SS_a:Rater$ is calculated the same way as $SS_M$ for one-way ANOVA, simply collapsing across Photo and calculating the *marginal means* for Negative as a function of ethnicity of the Rater:

Reminder of the formula:  $SS_{a:Rater}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$

With Rater, we have three cells:

```{r}
psych::describeBy(Negative ~ Rater, mat = TRUE, data = Ramdhani_df, digits = 3)
```
Again, we need the grand mean.
```{r }
mean(Ramdhani_df$Negative)
```
Now to calculate the Rater main effect
```{r }
35*(2.491 - 2.947)^2 + 37*(3.007 - 2.947)^2 +39*(3.299 - 2.947)^2  
```
#### Photo  Main Effect

$SS_b:Photo$ is calculated the same way as $SS_M$ for one-way ANOVA, simply collapsing across Rater and calculating the *marginal means* for Negative as a function of ethnicity reflected in the Photo stimulus:

Reminder of the formula:  $SS_{a:Photo}= \sum n_{k}(\bar{x}_{k}-\bar{x}_{grand})^{2}$

With Photo, we have only two cells:

```{r}
psych::describeBy(Negative ~ Photo, mat = TRUE, data = Ramdhani_df, digits = 3)
```

Again, we need the grand mean
```{r }
mean(Ramdhani_df$Negative)
```

```{r }
54*(2.575 - 2.947)^2 + 57*(3.300 - 2.947)^2 
```
#### Interaction effect

The interaction term is simply the $SS_M$ remaining after subtracting the SS from the main effects.

$SS_{axb} = SS_M - (SS_a + SS_b)$

```{r}
35.415 - (12.243 + 14.575)

```
Let's revisit the figure I showed at the beginning of this section to see, again, how variance is partitioned.

![Image of a flowchart that partitions variance from sums of squares totals to its component pieces](images/factorial/partition.png)

### Source Table Games!

As in the lesson for one-way ANOVA, we can use the hints in this source table to determine if we have statistically significance in the model.  The formulas in the table provide some hints. 

|Summary ANOVA for Negative Reaction
|:-------------------------------------------------------|

|Source    |SS       |df         |$MS = \frac{SS}{df}$ |$F = \frac{MS_{source}}{MS_{resid}}$ |$F_{CV}$|
|:---------|:--------|:----------|:------|:------|:------|
|Model     |         |$k-1$      |       |       |       |
|a         |         |$k_{a}-1$  |       |       |       |
|b         |         |$k_{b}-1$  |       |       |       |
|aXb       |         |$(df_{a})(df_{b})$||       |       |
|Residual  |         |$n-k$      |       |       |       |
|Total     |         |           |       |       |       |



|Summary ANOVA for Negative Reaction
|:-------------------------------------------------------|

|Source    |SS       |df         |$MS = \frac{SS}{df}$ |$F = \frac{MS_{source}}{MS_{resid}}$ |$F_{CV}$|
|:---------|:--------|:----------|:------|:------|:------|
|Model     |35.415   |5          |7.083  |9.381  |2.301  |
|a         |12.243   |2          |6.122  |8.109  |3.083  |
|b         |14.575   |1          |14.575 |19.305 |3.932  |
|aXb       |8.597    |2          |4.299  |5.694  |3.083  |
|Residual  |79.321   |105        |0.755  |       |       |
|Total     |114.775  |           |       |       |       |

```{r}
#hand-calculating the MS values
35.415/5   #Model
12.243/2   #a: Rater
14.575/1   #b:  Photo
8.597/2    #axb interaction term
79.321/105 #residual
#hand-calculating the F values
7.083/.755  #Model
6.122/.755  #a: Rater
14.575/.755 #b:  Photo
4.299/.755  #axb interaction term
```

To find the $F_{CV}$ we can use an [F distribution table](https://www.statology.org/f-distribution-table/).

Or use a look-up function, which follows this general form:  qf(p, df1, df2. lower.tail=FALSE)
```{r}
#looking up the F critical values
qf(.05, 5, 105, lower.tail=FALSE)#Model F critical value
qf(.05, 2, 105, lower.tail=FALSE)#a and axb F critical value
qf(.05, 1, 105, lower.tail=FALSE)#b F critical value

```
When the $F$ value exceeds the $F_{CV}$, the effect is statistically significant.


### Interpreting the results

What have we learned?

* there is a main effect for Rater
* there is a main effect for Photo
* there is a significant interaction effect

In the face of this significant interaction effect, we would follow-up by investigating the interaction effect.  Why?  The significant interaction effect means that findings (e.g., the story of the results) are more complex than group identity or photo stimulus, alone, can explain.

## Working the Factorial ANOVA with R packages

### Evaluating the statistical assumptions

All statistical tests have some assumptions about the data. This particular ANOVA has three:

Assumptions

* Cases represent random samples from the populations 
  - This is an issue of research design
  - ANOVA was really designed for the RCT; while we see it applied elsewhere...argHHHHHHHH
* Scores on the DV are independent of each other.
  - With correlated observations, a dramatic increase of Type I error
  - There are better options for analyzing data that has dependencies (i.e., repeated measures ANOVA...but also dyadic, multilevel modeling)
* DV is normally distributed for each of the populations
  - that is, each cell (representing the combinations of each factor) is normally distributed
* Population variances of the DV are the same for all cells
  - When cell sizes ≠, ANOVA not robust to this violation and cannot trust F ratio

Even though we position the evaluation of assumptions first -- some of the best tests of the assumptions use the resulting ANOVA model.  So, I'm going to quickly run the model now -- but not explain the results -- but use it to evaluate the assumptions.

I have marked our Two-Way ANOVA Decision-Tree with a yellow box outlined in red to let us know that we are just beginning the process of analyzing our data.

![Image of a flowchart showing that we are on the "Evaluating assumptions" portion of the decision-tree](images/factorial/WrkFlw_Assumptions.jpg)

```{r }
TwoWay_neg<-aov(Negative~Rater*Photo, Ramdhani_df)
summary(TwoWay_neg)
model.tables(TwoWay_neg,"means")
```

#### DV is normally distributed

We can start easy with **skew** and **kurtosis**.

```{r}
library(psych)
psych::describeBy(Negative ~ Rater + Photo, mat = TRUE, data = Ramdhani_df, digits = 3)
```

Using guidelines from Kline [-@kline_principles_2016]
* All skew is below 3; 
* All kurtosis is below 8 (8 to 20 is considered to be *extreme*)

In a factorial design, the Shapiro-Wilk test is applied to residuals from the model itself.  Examination of those residuals can give us a good indication of normality.

First, we extract the residuals (i.e., that which is left-over/unexplained) from the model.

```{r }
resid_neg<- residuals (TwoWay_neg) #creates object of residuals
hist(resid_neg)
```
So far so good -- these look normal. Let's examine a QQ plot.  We want the dots (the *residuals*), or what is leftover after the model is applied to line up on the diagonal line (within reason). This would indicate a normal distribution.

```{r  }
qqnorm(resid_neg)
```
We can formally test the distribution of the residuals with a Shapiro test. We want the associated *p* value to be less than 0.05.
```{r }
shapiro.test(resid_neg)
```
Whooo hoo!  $p > 0.05$

Here's how I would summarize our data in terms of normality:

Factorial ANOVA assumes that the dependent variable is normally is distributed for all cells in the design.  Our analysis suggested skew and kurtosis were within the bounds considered to be normally distributed.  Further, the Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) suggested that the plotting of the residuals did not differ significantly from a normal distribution ($W$ = 0.9846, $p$ = 0.234).

#### Homogeneity of variance

Levene's test is also applied to the ANOVA model; in this case I just wrote it out.  Levene's requires a *fully saturated model.*  This means that the prediction model requires an interaction effect (not just two, non-interacting predictors).

```{r }
library(car)
leveneTest(Negative ~ Rater*Photo, data = Ramdhani_df)
```
Levene's test has indicated a violation of the homogeneity of variance assumption ($F$[5, 105] = 8.634, $p$ < .001). This is not surprising. The boxplots shows some widely varying variances. 

### Evaluating the Omnibus ANOVA

The *F*-tests associated with the two-way ANOVA are the *omnibus* -- giving the result for the main and interaction effects.  

Here's where we are in the decision tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WrkFlw_Omnibus.jpg)

When we run the two-way ANOVA we will be looking for several effects:

* main effects for each predictor, and
* the interaction effect.

It is possible that all of them will be significant, none will be significant, or some will be significant. The interaction effect always takes precedence over the main effect because it let's us know there is a more nuanced/complex story.

In specifying the ANOVA, order of entry matters.  If you have a distinction between IV and moderator, put the IV first.
```{r }
TwoWay_neg<-aov(Negative~Rater*Photo, Ramdhani_df)
summary(TwoWay_neg)
model.tables(TwoWay_neg,"means")
```

Let’s gather the *F strings*”* from the above table.
Rater main effect: significant (*F*[2, 105] = 8.077, *p* < .001). Photo stimulus main effect: significant (*F*[1, 105] = 19.346, *p* < .001). Interaction effect: significant (*F*[2, 105] = 5.696, *p* = .004).

```{r }
plot(TwoWay_neg) 
```

#### Effect sizes

**Eta squared** is one of the most commonly used measures of effect. It refers to the proportion of variability in the DV/outcome variable that can be explained in terms of the IV/predictor.  Traditional interpretive values are similar to the Pearson's *r*:

0 = no relationship
.02 = small
.13 = medium
.26 = large
1 = a perfect (one-to-one) correspondence

More interpretive info can be found here:  https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize

The formula is straightforward:

$$\eta ^{2}=\frac{SS_{M}}{SS_{T}}$$
```{r }
library(lsr)
etaSquared(TwoWay_neg)
```
We can update our *F* string:
Rater main effect: significant (*F*[2, 105] = 8.077, *p* < .001, $\eta ^{2}$ = 0.107). Photo stimulus main effect: significant (*F*[1, 105] = 19.346, *p* < .001, $\eta ^{2}$ = 0.127). Interaction effect: significant (*F*[2, 105] = 5.696, *p* = .004, $\eta ^{2}$ = 0.075).

#### APA Write-up of the omnibus results

A 3 X 2 ANOVA was conducted to evaluate the effects of rater ethnicity (3 levels, Dayaknese, Madurese, Javanese) and photo stimulus (2 levels, Dayaknese on Madurese,) on negative reactions to the photo stimuli. Results of Levene’s Test for Equality of Error Variances indicated violation of the assumption, ($F$[5, 105] = 8.834, $p$ < .001). Our analysis of the individual cell means (see Table 1 for means and standard deviations) suggested skew and kurtosis were within the bounds considered to be normally distributed [@kline_principles_2016]. A non-significant Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) provided further evidence that the assumption of normality was not violated ($W$ = 0.9846, $p$ = 0.234).

Computing sums of squares with a Type II approach, the results for the ANOVA indicated a significant main effect for ethnicity of the rater (*F*[2, 105] = 8.077, *p* < .001, $\eta ^{2}$ = 0.107), a significant main effect for photo stimulus, (*F*[1, 105] = 19.346, *p* < .001, $\eta ^{2}$ = 0.127), and a significant interaction effect (*F*[2, 105] = 5.696, *p* = .004, $\eta ^{2}$ = 0.075).

*...The next paragraph will have one of the follow-up options. We will add it later in the lesson*

### Follow-up a significant interaction effect

The aspirational ideal of a factorial ANOVA is a significant interaction effect.  Interpretation is more complex than a main effect (later) of either factor.

There are a mazillion ways to follow-up a significant interaction effect.  I will demo the four I believe to be the most useful in the context of psychologists operating within the scientist-practitioner-advocacy context.

When an interaction effect is significant (irrespective of the significance of one or more main effects), examination of **simple main effects** is a common statistical/explanatory approached that is used. The Two-Way ANOVA Decision Tree shows where we are in this process.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WrkFlo_IntSmp.jpg)

We're going to subset the data to see only the people in one group (e.g., low income neighborhood) and the do the other group(s) separately. 

Remember the custom orthogonal contrasts in one-way ANOVA? We will do the same for these super, simple main effects. In our 3x2 ANOVA, Rater has three levels: Dayaknese, Madurese, and Javanese.

#### Option #1 the simple main effect of photo stimulus within ethnicity of the rater

Here we subset each of the three ethnic groups and then compare their ratings of the two photos.
```{r message=FALSE, warning=FALSE}
Dayaknese <- subset(Ramdhani_df, Rater == "Dayaknese") #subset data
Dayaknese_simple <- aov(Negative ~ Photo, data = Dayaknese) #change df to subset, new model name 
summary(Dayaknese_simple) #output for simple main effect
etaSquared(Dayaknese_simple, anova = FALSE ) #effect size for simple main effect can add "type = 1,2,3,4" to correspond with the ANOVA that was run
```
Within the Dayaknese ethnic group, there is a statistically significant difference in negative reactions to Dayaknese and Madurese photos: *F* (1, 33) = 50.4, *p* < .001, $\eta ^{2}$ = 0.60.

Next we evaluate photo rating within the Madurese ethnic group.
```{r  message=FALSE, warning=FALSE}
Madurese <- subset(Ramdhani_df, Rater == "Madurese") #subset data
Madurese_simple <- aov(Negative ~ Photo, data = Madurese) #change df to subset, new model name 
summary(Madurese_simple) #output for simple main effect
etaSquared(Madurese_simple, anova = FALSE ) #effect size for simple main effect can add "type = 1,2,3,4" to correspond with the ANOVA that was run
```
Within the Madurese ethnic group, there was a nonsignificant difference in negative reactions to Dayaknese and Madurese photos:  *F* (1, 37) = 0.00, *p* = .993, $\eta ^{2}$ < .001.

```{r  message=FALSE, warning=FALSE}
Javanese <- subset(Ramdhani_df, Rater == "Javanese") #subset data
Javanese_simple <- aov(Negative ~ Photo, data = Javanese) #change df to subset, new model name 
summary(Javanese_simple) #output for simple main effect
etaSquared(Javanese_simple, anova = FALSE ) #effect size for simple main effect can add "type = 1,2,3,4" to correspond with the ANOVA that was run
```
Within the Javanese ethnic group, there was a significant difference in negative reactions to Dayaknese and Madurese photos: *F* (1, 35) = 17.18, *p* < .001, $\eta ^{2}$ = 0.33.

If I were using this approach in a 3 X 2 ANOVA, I would probably not control for Type I error.  Why?  I only conducted follow-up comparisons to evaluate the simple main effect of photo stimulus within rater ethnicity; that is, I would hold it at alpha = 0.05.  

* Photo stimulus (Dayaknese or Madurese) within the Dayaknese ethnic group.
* Photo stimulus (Dayaknese or Madurese) within the Madurese ethnic group.
* Photo stimulus (Dayaknese or Madurese) within the Javanese ethnic group.  

However, because it's good for instruction, it would be equally fine to use a traditional Bonferroni, dividing .05/3 = 0.017 and testing each at 0.017.  I will use this approach in the write-up.

FAQ:  Could we do the reverse simple effect, ethnicity of rater within the photo stimulus?  Absolutely!  The choice is yours (and sometimes the results will differ).  I usually run both and then report ONE -- the one that conveys the story the data has to tell.  You *could* report both sets, but then you would really want to control Type I error and your repetitive contrasts are faaaaaar from independent/orthogonal.

**APA Style Results for Option #1 follow-up.**  *This would be added to the results of the omnibus two-way ANOVA.* 

To explore the interaction effect, we followed with a test of the simple main effect of photo stimulus within the ethnicity of the rater. That is, we looked at the effect of the photo stimulus within the Dayaknese, Madurese, and Javanese groups, separately. To control for Type I error across the three simple main effects, we set alpha at .017 (.05/3). Results indicated significant differences for Dayaknese (*F* [1, 33] = 50.4, *p* < .001, $\eta ^{2}$ = 0.60.) and Javanese ethnic groups (*F* [1, 35]= 17.18, *p* < .001, $\eta ^{2}$ = 0.33), but not for the Madurese ethnic group (*F* [1, 37] = 0.000, *p* = .993, $\eta ^{2}$ < .001).  As illustrated in Figure 1, the Dayaknese and Javanese rathers both reported stronger negative reactions to the Madurese. The differences in ratings for the Madurese were not statistically significantly different.  In this way, the rater's ethnic group moderated the relationship between the photo stimulus and negative reactions.

```{r }
library(ggpubr)
ggboxplot(Ramdhani_df, x = "Rater", y = "Negative", color = "Photo",xlab = "Ethnicity of Rater", ylab = "Negative Reaction")
```

#### Option #2 the simple main effect of ethnicity of rater within photo stimulus.

In this simple main effect of ethnicity of rater (3 levels) within photo stimulus (2 levels), we will conduct two one-way ANOVAs for the Dayaknese and Madurese photos, separately.  However, we will want to do orthogonal contrast-coding for rater ethnicity for the follow-up (to the follow-up).

It helps to know what the default contrast codes are: 

```{r }
contrasts(Ramdhani_df$Rater)
```

Let's create custom contrasts. Recall that an orthogonal contrast requires that there be one less contrast than the number of groups and that once a group is singled out, it cannot be compared again.

Thus, I want to compare the 

* Javanese to the Dayaknese and Madurese combined, then
* Dayaknese to Madurese

```{r  message=FALSE, warning=FALSE}
# tell R which groups to compare
c1 <- c(1, -2, 1) 
c2 <- c(-1, 0, 1) 
mat <- cbind(c1,c2) #combine the above bits
contrasts(Ramdhani_df$Rater) <- mat # attach the contrasts to the variable
```

This allows us to recheck the contrasts.
```{r }
contrasts (Ramdhani_df$Rater)
```
Yes, in contrast 1 we are comparing the Javanese to the combined Dayaknese and Madurese.  In contrast 2 we are comparing the Dayaknese to the Madureses.

```{r message=FALSE, warning=FALSE}
Dayaknese_Ph <- subset(Ramdhani_df, Photo == "Dayaknese") #subset data
Dykn_simple <- aov(Negative ~ Rater, data = Dayaknese_Ph) #change df to subset, new model name 
summary(Dykn_simple) #output for simple main effect
etaSquared(Dykn_simple, anova = FALSE ) #effect size for simple main effect can add "type = 1,2,3,4" to correspond with the ANOVA that was run
```
*F* [2, 51]) = 13.32, *p* < .001, $\eta ^{2}$ = 0.343.

```{r message=FALSE, warning=FALSE}
summary.aov(Dykn_simple, split=list(Rater=list("Javanese v Dayaknese and Madurese"=1, "Dayaknese  Madurese" = 2)))
```
The simple main effect of ethnicity of the rater within the reaction to the photos of members of the Dayaknese ethnic group was  statistically significant:  *F* [2, 51] = 13.32, *p* < .001, $\eta ^{2}$ = 0.343. Follow-up testing indicated non-significant differences when the ratings from members of the Javanese ethnic group were compared to the Dayaknese and Madurese, combined (*F* [1, 51] = 0.095, *p* = .759).  There was a statistically significant difference when Dayaknese and Madurese raters were compared (*F* [1, 51] =26.554, *p* < .001)

```{r message=FALSE, warning=FALSE}
Madurese_Ph <- subset(Ramdhani_df, Photo == "Madurese") #subset data
Mdrs_simple <- aov(Negative ~ Rater, data = Madurese_Ph) #change df to subset, new model name 
summary(Mdrs_simple) #output for simple main effect
etaSquared(Mdrs_simple, anova = FALSE ) #effect size for simple main effect can add "type = 1,2,3,4" to correspond with the ANOVA that was run
```

*F* [2, 54] = 0.679, *p* = .512, $\eta ^{2}$ = 0.024.

```{r  message=FALSE, warning=FALSE}
summary.aov(Mdrs_simple, split=list(Rater=list("Javanese v Dayaknese and Madurese"=1, "Dayaknese  Madurese" = 2)))
```

The simple main effect of ethnicity of the rater within  rating the photos of Madurese people was not statistically significant:  (*F* [2, 54] = 0.679, *p* = .512, $\eta ^{2}$ = 0.024). Correspondingly, follow-up testing indicated non-significant differences when the ratings of the Javanese were compared to Dayaknese and Madurese, combined (*F* [1, 54] = 1.008, *p* = .320) and when the ratings of the Dayaknese and Madurese were compared (*F* [1, 54] = 0.349, *p* = .557)

To control for Type I error, we have 4 follow-up contrasts (2 for Dayaknese, 2 for Madurese).  We'll control Type I error with .05/4 = .0125

```{r }
.05/4
```

**APA Write-up of the simple main effect of photo stimulus within rater ethnicity.** *This would be added to the write-up of the omnibus two-way ANOVA test.

*Option #2*: To explore the interaction effect, we followed with tests of simple effect of rater ethnicity within the photo stimulus. That is, we looked at the effect of each each rater's ethnicity within the Madurese and Dayaknese photo stimulus, separately. Our first analysis evaluated the effect of the rater's ethnicity when evaluating the Dayaknese photo; our second analysis evaluated effect of the rater's ethnicity when evaluating the Madurese photo. To control for Type I error across the two simple main effects, we set alpha at .0125 (.05/4). The simple main effect of ethnicity of the rater within the reaction to the photos of members of the Dayaknese ethnic group was  statistically significant:  *F* [2, 51] = 13.32, *p* < .001, $\eta ^{2}$ = 0.343. Follow-up testing indicated non-significant differences when the ratings from members of the Javanese ethnic group were compared to the Dayaknese and Madurese, combined (*F* [1, 51] = 0.095, *p* = .759).  There was a statistically significant difference when Dayaknese and Madurese raters were compared (*F* [1, 51] =26.554, *p* < .001).  The simple main effect of ethnicity of the rater within when rating the photos of Madurese people was not statistically significant:  (*F* [2, 54] = 0.679, *p* = .512, $\eta ^{2}$ = 0.024). Correspondingly, follow-up testing indicated non-significant differences when the ratings of the Javanese were compared to Dayaknese and Madurese, combined (*F* [1, 54] = 1.008, *p* = .320) and when the ratings of the Dayaknese and Madurese were compared (*F* [1, 54] = 0.349, *p* = .557).  This moderating effect of ethnicity of the rater on the negative reaction to the photo stimulus is illustrated in Figure 1. 

```{r}
ggboxplot(Ramdhani_df, x = "Photo", y = "Negative", color = "Rater", xlab = "Photo Stimulus",
             ylab = "Negative Reaction")
```

#### Post hoc comparisons

Another option is compare all possible cells. These are termed *post hoc comparisons.*  They are an alternative to simple main effects; you would not report both. The figure shows our place on the Two-Way ANOVA Decision Tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WrkFlw_IntPH.jpg)


As the numbers of levels increase, post hoc comparisons become somewhat unwieldly.  Even though this procedure produces them all, you can select which sensible number you want to prepare and control for Type I error in that way.

The total number of paired comparisons is:  k(k-1)*2
k is the number of groups.  With race (2 levels) and neighborhood income level (3 levels), we have 6 groups

```{r }
6*(6-1)/2
```
Sure enough -- look below:

```{r }
posthocs <- TukeyHSD(TwoWay_neg, ordered = TRUE)
posthocs
```

If we want to consider all 15 pairwise comparisons and also control for Type I error, a Holm's sequential Bonerroni [@green_using_2014] will help us take a middle-of-the-road approach (not as strict as .05/15 with the traditional Bonferroni; not as lenient as "none") to managing Type I error. 

With the Holms, we rank order the *p* values associated with the 15 comparisons in order from lowest (e.g., .0000018) to highest (e.g., 1.000).  The first *p* value is evaluated with the most strict criterion (.05/15) according to the traditional Bonferonni approach. Then, each successive comparison calculates the *p* value by using the number of *remaining* comparisons as the denominator (e.g., .05/14, .05/13, .05/12). As the *p* values rise and the alpha levels relax, there will be a cut-point where remaining comparisons are not statistically significant.

```{r }
.05/15
.05/14
```

To facilitate this contrast, let's extract the 15 TukeyHSD tests and work with them in Excel.

First, obtain the structure of the *posthoc* object

```{r }
str(posthocs)
```

```{r }
write.csv(posthocs$'Rater:Photo', 'posthocsOUT.csv')
```

In Excel, I would sort my results by their *p* values (low to high) and consider my thresshold (*p* < .003) to determine which effects were statistically significant. Using the strictest criteria of *p* < .0033, we would have four statistically significant values.

![Image of the results of the Holms sequential Bonferroni.](images/factorial/Holmsequential.jpg)

I would ask, "Is this what we want?" Similar to the simple main effects we just tested, I am interested in two sets of comparisons:

First, how are the two sets of photos (Madurese and Dayaknese) rated within each set of raters.

* Javanese:Madurese - Javanese:Dayaknese
* Dayaknese:Madurese - Dayaknese:Dayaknese
* Madurese:Madurese - Madurese:Dayaknese

Second, focused on each photo, what are the relative ratings.

* Javanese:Madurese - Dayaknese:Madurese
* Madurese: Madurese - Dayaknese:Madurese

* Javanese:Dayaknese - Dayaknese:Dayaknese
* Madurese: Dayaknese - Dayaknese:Dayaknese

This is only seven sets of comparisons and would considerably reduce the alpha:
```{r}
.05/7
```
Below I have greyed-out the comparisons that are less interesting to me and left the seven that are my focal interest. I have highlighted in green the two comparisons that are statistically significant based on the Holms' sequential criteria. In this case, it does not make any difference in our interpretation of these focal predictors.  

![Image of the results of the Holms sequential Bonferroni.](images/factorial/HolmsSelect.jpg)

#### Polynomial Trends 

In the context of the significant interaction effect, we might also be interested in polynomial trends for any simple main effects where 3 or more cells are compared.

Why?  If there are only two cells being compared, then the significance of that has already been tested and if significant, it is also a significant linear effect (because the shape between any two points is a line, hence it's *linear*). Below is a figure of where the polynomial test of an interaction effect may fall in the process.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WrkFlw_Poly.jpg)

In our example, Rater has three groups  Thus, we could evaluate a polynomial for the simple main effect of ethnicity of the rater within photo stimulus (separately for the photos of the Dayaknese and Madurese). We conduct these separately for Dayaknese, Madurese, and Javanese groups.

In the event that more than one polynomial trend select the higher one.  For example, if both linear and quadratic are selected, interpret the quadratic trend

```{r }
contrasts(Dayaknese_Ph$Rater)<-contr.poly(3)
poly_Dy<-aov(Negative ~ Rater, data = Dayaknese_Ph)
summary.lm(poly_Dy)
```

Results of a polynomial trend analysis indicated a statistically significant linear trend for evaluation of the Dayaknese photos across the three raters *t* (51) = 5.153, *p* < .001.

```{r }
contrasts(Madurese_Ph$Rater)<-contr.poly(3)
poly_Md<-aov(Negative ~ Rater, data = Madurese_Ph)
summary.lm(poly_Md)
```
Results of a polynomial trend analysis were non-significant when ethnicity of the rater was evaluated when rating Madurese photos.

## In the event of a non-significant interaction, but one or more significant main effects

We now focus on the possibility that there might be significant main effects, but a non-significant interaction effect.  We **only** interpret main effects when there is a non-significant interaction effect.  Why? Because in the presence of a significant interaction effect, the main effect will not tell a complete story. *(And, if we didn't specify a correct model, we still might have an incomplete story. But that's another issue.)* Here's where we are on the Decision-Tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WayWrkFlw_Main.jpg)

Recall that main effects are the *marginal means* -- that is the effects of Factor A *collapsed across* all the levels of Factor B.

If the main effect has only two levels (e.g., the ratings of the Dayaknese and Madurese photos):

* the comparison was already ignoring/including all levels of the rater ethnicity factor (Dayaknese, Madurese, Javanese),
* it was only a comparison of two cells (Dayaknese rater, Madurese rater), therefore
* there is no need for further follow-up.  

If the main effect has three or more levels (e.g,. ethnicity of rater with Dayaknese, Madurese, Javanese levels), then you would follow-up with one or more of the myriad of options.  In this class we have focused on three:

* planned contrasts
* posthoc comparisons (all possible cells)
* polynomial

I will demo how to do each as follow-up to a *pretend* scenario where the main effect (but not the interaction) had been significant. I will write up the portion that would be inserted in an APA style results section.

Essentially, we treat these main effect analyses as the follow-up to a significant one-way ANOVA evaluating, in our case, the ethnicity of the Rater.

```{r }
RaterMain <- aov(Negative ~ Rater, data = Ramdhani_df) #DV ~ IV  I say, "DV by IV"
model.tables (RaterMain) #ANOVA output
summary(RaterMain)
etaSquared(RaterMain)
```
A boxplot representing this main effect may help convey how the main effect of Rater (collapsed across Photo) is different than an interaction effect.
```{r}
ggboxplot(Ramdhani_df, x = "Rater", y = "Negative", xlab = "Ethnicity of Rater", ylab = "Negative Reaction")
```


### Follow-up with all post-hocs

An easy possibility is to follow-up with all possible post-hocs. In the main effect case, these are far simpler than where we conducted all possile posthocs for the interaction effect (remember the Holms sequential Bonferroni?).

Here is a reminder of our location on the Decision-Tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/wfMain_PH.jpg)

The *TukeyHSD()* function produces posthoc comparisons by providing the mean difference, a 95% confidence interval of those differences, and the associated *p* value.
```{r }
TukeyHSD(RaterMain, ordered = TRUE)
```
Results suggest there were statistically significant differences (*p* < .05) between the Madurese and Dayaknese. These differences, though, would have been when rating *all* photos.  This analysis disregards the ethnicity portrayed in the photo.

### Follow-up with planned contrasts

We generally try for *orthogonal* contrasts so that the partitioning of variance is independent (clean, not overlapping). Planned contrasts are a great way to do this.  Here's where we are in the Decision-Tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/wfMain_Plnd.jpg)

If you aren't SUPER DUPER careful about your order-of-operations in R, it can confuse objects, so I have named these contrasts *main_c1* and *main_c2* to remind myself that they refer to the main effect of ethnicity of the rater.

In this hypothetical scenario (remember we are pretending we are in the circumstance of a non-significant interaction effect but a significant main effect), I am:

* Constrast #1:  comparing the DV for the Javanese rater to the combined Dayaknese and Madurese raters.
* Contrast #2:  comparing the DV for the Dayaknese and Madurese raters.

These are orthogonal because:

* there are *k* - 1 comparisons, and 
* once a contrast is isolated (i.e., the Javanese rater in contrast #1) it cannot be used again
  -  piece of cake analogy...once you take out a piece of the cake, you really can't put it back in

```{r }
#Contrast1  compares Control against the combined effects of Low and High.
main_c1 <- c(1,-2,1)

#Contrast2 excludes Control; compares Low to High.
main_c2 <- c(-1,0,1)
contrasts(Ramdhani_df$Rater)<-cbind(main_c1, main_c2)
contrasts(Ramdhani_df$Rater)
```
Then we run the contrast

```{r }
mainPlanned <- aov(Negative ~ Rater, data = Ramdhani_df)
summary.lm(mainPlanned)
contrasts(Ramdhani_df$Rater)<-cbind(c(1,-2,1), c(-1,0,1))
```

These planned contrasts show that when the Javanese raters are compared to the combined Dayaknese and Madurese raters, there was a non significant difference, *t*(108) = -0.567, *p* = .572. However, there were significant differences between Dayaknese and Javanese raters, *t*(108) = 3.556, *p* < .001. 


### Polynomial Trends

Polynomial contrasts let us see if there is a linear (or curvilinear) pattern to the data. To detect a trend, the data must be coded in an ascending order...and it needs to be a sensible comparison. Here's where this would fall in our Decision-Tree.

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/wfMain_Poly.jpg)
Because these three ethnic groups are not *ordered* in the same way as would an experiment involving dosage (e.g,. placebo, lo dose, hi dose), evaluation of the polynomial trend is not really justified (even though it is statistically possible). None-the-less, I will demonstrate how it is conducted.

```{r }
contrasts(Ramdhani_df$Rater)<-contr.poly(3)
mainTrend<-aov(Negative ~ Rater, data = Ramdhani_df)
summary.lm(mainTrend)
```

**Rater.L** tests the data to see if there is a significant linear trend.  There is:  *t* = 3.556, *p* < .001.

**Rater.!** tests to see if there is a significant quadratic (curvilinear, one hump) trend.  There is not:  *t* = -0.567, *p* = .572.

Results supported a significant linear trend (*t* = 3.556, *p* < .001) such that negative reactions increased in a linear reaction across the three rating groups.

```{r}
ggboxplot(Ramdhani_df, x = "Rater", y = "Negative", xlab = "Ethnicity of Rater", ylab = "Negative Reaction")
```


## My *final* APA Style Results Section

First, I am reluctant to term anything "final." It seems like there is always the possibility or revision.  Given that I demonstrated a number of options, let me first show the Decision-Tree with the particular path I took:

![Image our place in the Two-Way ANOVA Decision-Tree.](images/factorial/WrkFlw_mypath.jpg)

In light of that, here's the final write-up:

**Results**

A 3 X 2 ANOVA was conducted to evaluate the effects of rater ethnicity (3 levels, Dayaknese, Madurese, Javanese) and photo stimulus (2 levels, Dayaknese on Madurese,) on negative reactions to the photo stimuli. Results of Levene’s Test for Equality of Error Variances indicated violation of the assumption, ($F$[5, 105] = 8.834, $p$ < .001). Our analysis of the individual cell means (see Table 1 for means and standard deviations) suggested skew and kurtosis were within the bounds considered to be normally distributed [@kline_principles_2016]. A non-significant Shapiro-Wilk normality test (applied to the residuals from the factorial ANOVA model) provided further evidence that the assumption of normality was not violated ($W$ = 0.9846, $p$ = 0.234).

Computing sums of squares with a Type II approach, the results for the ANOVA indicated a significant main effect for ethnicity of the rater (*F*[2, 105] = 8.077, *p* < .001, $\eta ^{2}$ = 0.107), a significant main effect for photo stimulus, (*F*[1, 105] = 19.346, *p* < .001, $\eta ^{2}$ = 0.127), and a significant interaction effect (*F*[2, 105] = 5.696, *p* = .004, $\eta ^{2}$ = 0.075).

To explore the interaction effect, we followed with a test of the simple main effect of photo stimulus within the ethnicity of the rater. That is, we looked at the effect of the photo stimulus within the Dayaknese, Madurese, and Javanese groups, separately. To control for Type I error across the three simple main effects, we set alpha at .017 (.05/3). Results indicated significant differences for Dayaknese (*F* [1, 33] = 50.4, *p* < .001, $\eta ^{2}$ = 0.60.) and Javanese ethnic groups (*F* [1, 35]= 17.18, *p* < .001, $\eta ^{2}$ = 0.33), but not for the Madurese ethnic group (*F* [1, 37] = 0.000, *p* = .993, $\eta ^{2}$ < .001).  As illustrated in Figure 1, the Dayaknese and Javanese rathers both reported stronger negative reactions to the Madurese. The differences in ratings for the Madurese were not statistically significantly different.  In this way, the rater's ethnic group moderated the relationship between the photo stimulus and negative reactions.

```{r }
library(ggpubr)
ggboxplot(Ramdhani_df, x = "Rater", y = "Negative", color = "Photo",xlab = "Ethnicity of Rater", ylab = "Negative Reaction")
```


```{r }
library(apaTables)
apa.2way.table(iv1 = Rater, iv2 = Photo, dv = Negative, data = Ramdhani_df, landscape=TRUE, table.number = 1, filename="Table_1_MeansSDs.doc")

```

```{r }
apa.aov.table(TwoWay_neg, filename = "Table_2_effects.doc", table.number = 2, type = "II")
```

### Comparing Our Results to Rhamdani et al. [-@ramdhani_affective_2018]

As is common in simulations, our data gets close to the findings, but does not replicate them exactly. Our main and interaction effects map on very closely. However, in the follow-up tests, while our findings that Dayaknese rated the Madurese photos more negatively, was consistent across our simulated data and the article, the findings related to the Javanese' and Madurese' ratings wiggled around some. A close look at the figures can explain that with varying variability and close means, this is probable.  I find it to be a useful lesson in "what it takes" to get stable, meaningful results. 

## Options for Assumption Violations

In one-way ANOVA we could simply apply the Welch's alternative.  It's not so easy in factorial.  One alternative, though, is to change the Sums of Squares type used in the ANOVA calculations.  

In ANOVA models sums of squares can be calculated four different ways: Type I, II, III, and IV. This matters. 

SS Type II is the *aov()* default. It may be a best practice to go ahead and specify the SS Type in both the *aov()*, eta-squared, and apaTables script so that they are  consistent.

**Type I** sums of squares is similar to hierarchical linear regression in that the first predictor in the model claims as much variance as it can and the leftovers are claimed by the variable entered next – each claiming as much as possible leaving the leftovers for what follows. Unless the variables are completely independent of each other (unlikely), Type I sums of squares cannot evaluate the true main effect of each variable. Type I should not be used to evaluate main effects and interactions because the order of predictors will affect the results.

**Type II** is the R default appropriate if you are interested main effects because it ignores the effect of any interactions involving the main effect. Thus, variance from a main effect is not “lost” to any interaction terms containing that effect. Type II is appropriate for main effects analyses only, but should not be used when evaluating interaction effects. Type II sums of squares is not affected by the type of contrast coding used to specify the predictor variables.

**Type III** is the default in many stats packages – but not R. In Type III all effects (main effects and interactions) are evaluated (simultaneously) taking into consideration all other effects in the model (not just the ones entered before). Type III is more robust to unequal samples sizes (e.g., unbalanced designs). Type III is best when predictors are encoded with orthogonal contrasts.

***Type IV** is identical to Type III except it requires no missing cells. 

Field [-@field_discovering_2012] suggests that it is safest to stick with Type III sums of squares .  We apply the type to the model we create in the initial run. For more information, check out this explanation on [r-bloggers](https://www.r-bloggers.com/2011/03/anova-%E2%80%93-type-iiiiii-ss-explained/).

Many researchers automatically use Type III as the SS type.  Today I went with the R default because

* Type II sums of squares was used in hand-calculations,
* Our example was reasonably balanced (equal cell sizes), and 
* We had only violated the homogeneity of variance assumption.  

For demonstration purposes, let's run the Type III alternative to see the differences:  

```{r }
#this is what we did
Anova(TwoWay_neg)
```

```{r }
#We change the SS type by applying it to our model.
Anova(TwoWay_neg, type ="III")
```
Note that the sums of squares are somewhat different between models -- and that the Type III tests includes an intercept. In today's example, the  statistical significance remains the same across the models.

Now let's compare the effect sizes across models.

```{r}
etaSquared(TwoWay_neg)
```

```{r}
etaSquared(TwoWay_neg, type=3)
```
The Type III effect size for Rater is higher; the others are quite similar.


## Power 

*Power* is often our euphemistic way of asking, "How large should my sample size be?"

Power defined is the ability of the test to detect statistical significance when there is such.  It's represented formulaically as 1 - *P*)(Type II error).  Power is traditionally set at 80% (or .8)

We'll do both -- evaluate the power of our current example and then work backwards to estimate the sample size needed (which is our usual question for MRPs and dissertations).

We'll use the *pwr.2way()* function from the *pwr2* package.
Helpful resources are found here: 

*  https://cran.r-project.org/web/packages/pwr2/pwr2.pdf
*  https://rdrr.io/cran/pwr2/man/ss.2way.html

The *pwr.2way()* and *ss.2way()* functions require the following:

* **a** number of groups in Factor A
* **b** number of groups in Factor B
* **alpha**  significant level (Type I error probaility)
* **beta** Type II error probability (Power = 1 - beta; traditionally set at .1 or .2)
* **f.A** the *f* effect size of Factor A 
* **f.B** the *f* effect size of Factor B
* **B** Iteration times, default is 100 

Hints for calculating the *f.A* and *f.B* values:

* In this case, we will rerun the statistic, grab both, and convert them to the *f* (not the $f^2$)
  - calculation can be straightforward, either use an online calculator, a hand-calculated formula, or the *eta2_to_f* function from the *effectsize* 
* In unknown case, you can substitute what you expect using Cohen's guidelines of .10, .25, and .40 as small, medium, and large (for the *f*, not $F^2$)
* Helpful resource:  https://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/effectSize 

Let's quickly rerun our model to get both the df and calculate the *f* effect value
```{r  message=FALSE, warning=FALSE}
etaSquared(TwoWay_neg, anova = TRUE)
# get the partial eta-square (second number)
# and dfs 
```
If we want to understand power in our analysis, we need to convert our effect size for the *interaction* to $f$ effect size (this is not the same as the *F* test). The *effectsize* package has a series of converters.  We can use the *eta2_to_f()* function. 

```{r}
library(effectsize)
eta2_to_f(0.1066) #FactorA -- Rater
eta2_to_f(0.1274) #Factor B -- Photo
```

Now, to calculate power for our existing model. We'll use the package *pwr2* and the function *pwr.2way()*. To specify this we identify:

a:  number of groups for Factor A (Rater)
b:  number of groups for Factor B (Photo)
size.A:  sample size per group in Factor A (because ours differ slightly, I divided the N by the number of groups)
size.B:  sample size per group in Factor B (because ours differ slightly, I divided the N by the number of groups)
f.A:  Effect size of Factor A
f.A.:  Effect size of Factor B
```{r}
library(pwr2)
pwr.2way(a=3, b=2, alpha = 0.05, size.A = 37, size.B = 55, f.A = .345, f.B = .382)
```
Our power to detect a significant effect for Factor A/Rater and Factor B/Photo was huge!  


**Calculating sample size requirements**

Say we wanted to replicate this study.  We could use the estimates from this study to estimate what would be needed for the replication.

In this specification:

a:  number of groups for Factor A (Rater)
b:  number of groups for Factor B (Photo)
alpha: significance level  (Type I error probability); usually .05
beta:  Type II error probability (Power = 1-beta); usually .80
f.A:  Effect size (*f*) of Factor A (this time we know; other times we can guess from previously published literature)
f.A.:  Effect size (*f*) of Factor B
B: iteration times, default number is 100

```{r}
ss.2way(a = 3, b = 2, alpha = .05, beta = .8, f.A = .345, f.B = .382, B= 100)
```
Curiously, that's just about the number that was in each of the six cells!

Often times researchers will play around with the *f* values.  Remember Cohen's indication of small (.10), medium (.25), and large (.40).  We know we have a small effect for race and a larger effect for neighborhood income.  Let's see what happens when we enter different values. Specifically, what if we only had a medium effect.

```{r}
ss.2way(a = 3, b = 2, alpha = .05, beta = .80, f.A = .25, f.B = .25, B= 100) #if we expected a medium effect
ss.2way(a = 3, b = 2, alpha = .05, beta = .80, f.A = .10, f.B = .10, B= 100) #if we expected a small effect
```


## Practice Problems

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. In any case, you will be expected to:

* test the statistical assumptions
* conduct a two-way ANOVA, including
  - omnibus test and effect size
  - report main and interaction effects
  - conduct follow-up testing of simple main effects
* write a results section to include a figure and tables

### Problem #1:  Play around with this simulation.

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.  

* If two-way ANOVA is new to you, perhaps you just change the number in "set.seed(210731)" from 210731 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.
* If you are interested in power, change the sample size to something larger or smaller.
* If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|5. Describe approach for managing Type I error|    5        |_____  |   
|6. APA style results with table(s) and figure|    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          


### Problem #2:  Conduct a one-way ANOVA with the *positive evaluation* dependent variable.

The Ramdhani et al. [-@ramdhani_affective_2018] article has two dependent variables (negative and positive evaluation) which are suitable for two-way ANOVAs. I used *negative evaluation* as the dependent variable; you are welcome to conduct the analysis with *positive evaluation* as the dependent variable.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|5. Describe approach for managing Type I error |    5        |_____  |   
|6. APA style results with table(s) and figure  |    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |             


### Problem #3:  Try something entirely new.

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a two-way, factorial ANOVA. Please have at least 3 levels for one predictor and at least 2 levels for the second predictor.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Narrate the research vignette, describing the IV and DV | 5 |_____  |
|2. Simulate (or import) and format data               |      5            |_____  |           
|3. Evaluate statistical assumptions     |      5            |_____  |
|4. Conduct omnibus ANOVA (w effect size) |      5           | _____  |  
|5. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|6. Describe approach for managing Type I error |    5        |_____  |   
|7. APA style results with table(s) and figure  |    5        |_____  |       
|8 Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          


```{r include=FALSE}
sessionInfo()
```

